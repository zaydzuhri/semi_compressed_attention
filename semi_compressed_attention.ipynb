{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sdtDsu1Y0EqL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "# os.environ[\"TRITON_INTERPRET\"] = \"1\"\n",
    "torch.manual_seed(69)\n",
    "torch.set_printoptions(profile=\"short\", sci_mode=False, linewidth=100000)\n",
    "# this script is configured to run on a RTX 3060 12GB GPU. you'll want to adjust the model sizes and batch sizes for other devices\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "plt.rcParams['figure.dpi'] = 50\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['xtick.minor.visible'] = True\n",
    "plt.rcParams['ytick.minor.visible'] = True\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANr5dn7W0EqR",
    "outputId": "a00cbe8d-e1c6-454b-b552-4ffd17ce17e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  39526018\n"
     ]
    }
   ],
   "source": [
    "# we use this 40mb file of concatenated anime subtitles as our dataset\n",
    "# just the right size for toy experiments like this I think\n",
    "with open('animesubs.txt', 'r', encoding='latin') as f:\n",
    "    text = f.read()\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pANiObIZ0EqU",
    "outputId": "98f70469-1c89-4341-89e8-c142a14331b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open your mind. Open your mind.\n",
      "\n",
      "Far beyond the deep blue Earth, you and I shall meet...\n",
      "\n",
      "AH! MY GODDESS\n",
      "\n",
      "A snow-white feather comes fluttering down, swaying gently in the air.\n",
      "\n",
      "Without holding back, I want to envelope you, my one and only love.\n",
      "\n",
      "I know I have the power to protect the one I love, right here in my hands.\n",
      "\n",
      "Open your mind. Just as I've always dreamed.\n",
      "\n",
      "Let the wind carry off your hopes, faraway.\n",
      "\n",
      "I have wings nobody can see. Look, you have them, too.\n",
      "\n",
      "They'll take us to where we ca\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WvM5h6_i3KsM"
   },
   "outputs": [],
   "source": [
    "# remove japanese characters\n",
    "text = ''.join(filter(lambda character:ord(character) < 0x3000, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7SOcWJM0EqW",
    "outputId": "a82a4da8-a8ed-47bf-cfb5-2e4c59dee2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters: 86 \n",
      " !'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|Â”\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"unique characters:\", vocab_size, ''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yobmmaeK0EqX",
    "outputId": "26669f38-4b26-4b53-f642-ee7543e7c578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, '*': 6, '+': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '<': 24, '=': 25, '>': 26, '?': 27, '@': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, '[': 55, ']': 56, '_': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63, 'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70, 'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77, 'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, '|': 84, '\\x94': 85, '': 86}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: '*', 7: '+', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '<', 25: '=', 26: '>', 27: '?', 28: '@', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '|', 85: '\\x94', 86: ''}\n",
      "encoded: [43, 73, 62, 71, 1, 82, 72, 78, 75, 1, 70, 66, 71, 61, 10, 1, 43, 73, 62, 71]\n",
      "decoded: Open your mind. Open\n",
      "vocab size: 87\n"
     ]
    }
   ],
   "source": [
    "# yes, all language models will be character level, which isn't ideal but it's good for simplicity\n",
    "# very simple tokenizer\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "# add special token for padding\n",
    "stoi[''] = len(stoi)\n",
    "itos[len(itos)] = ''\n",
    "print(stoi)\n",
    "print(itos)\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "print(\"encoded:\", encode(text[:20]))\n",
    "print(\"decoded:\", decode(encode(text[:20])))\n",
    "vocab_size = len(itos)\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Pnf9KfP0EqY",
    "outputId": "ff581168-335a-4f0f-efeb-0d4bd5b225ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39526018])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZ2fY1pR0EqY",
    "outputId": "5eb599e0-fb79-4cdb-c4b9-52a781d67151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  1, 43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  0,  0, 34, 58, 75,  1, 59, 62, 82, 72, 71, 61,  1, 77, 65, 62,  1, 61, 62, 62, 73,  1, 59, 69, 78, 62,  1, 33, 58, 75, 77, 65,  8,  1, 82, 72, 78,  1, 58, 71, 61,  1, 37,  1, 76, 65, 58, 69, 69,  1, 70, 62, 62, 77, 10, 10, 10,  0,  0, 29, 36,  2,  1, 41, 53,  1, 35, 43, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIaYesPh0Eqa",
    "outputId": "caa27cbb-5ae3-4406-8194-646264d4ba99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39130757]) torch.Size([395261])\n"
     ]
    }
   ],
   "source": [
    "n = int(len(data) * 0.99)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bFhizcI0Eqa",
    "outputId": "c93a4d43-6fb2-4de7-aefc-8fed47e4a861"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 8\n",
    "train_data[:seq_len+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skFCPvQC0Eqc",
    "outputId": "08990c5b-88af-4a51-ce37-3c59989d6d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([2, 64])\n",
      "tensor([[64,  1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71],\n",
      "        [62, 70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([2, 64])\n",
      "tensor([[ 1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71, 58],\n",
      "        [70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61, 66]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, seq_len, batch_size=4):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    # targets are just inputs shifted by 1\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = get_batch('train', 64, 2)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81920000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make all steps, sequence lengths, and batch size the same\n",
    "total_steps = 5000\n",
    "seq_len = 2048\n",
    "batch_size = 8 # these are small models so we can use large batch sizes to fully utilize the GPU\n",
    "# should cover around 2x the dataset\n",
    "total_steps * seq_len * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, seq_len, batch_size, total_steps, val_steps=10, val_interval=50):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    # live plot\n",
    "    fig, ax = plt.subplots()\n",
    "    dh = display.display(fig, display_id=True)\n",
    "    for steps in (bar := tqdm(range(total_steps))):  # increase number of steps for good results...\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train', seq_len=seq_len, batch_size=batch_size)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bar.set_description(f\"loss: {loss.item():.2f}, val loss: {val_losses[-1] if val_losses else 0:.2f}\")\n",
    "        losses.append(loss.item())\n",
    "        if steps % val_interval == 0:\n",
    "            # Calculate validation loss\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for _ in range(val_steps):\n",
    "                    xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "                    _, loss = model(xb, yb)\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= val_steps\n",
    "                val_losses.append(val_loss)\n",
    "            ax.clear()\n",
    "            ax.plot(losses, color='blue', label='train loss', alpha=0.7)\n",
    "            ax.plot(range(0, len(losses), val_interval), val_losses, color='red', label='val loss', alpha=0.7)\n",
    "            ax.set_ylim(0, 4)\n",
    "            ax.legend()\n",
    "            dh.update(fig)\n",
    "    print('final loss:', loss.item(), 'final val loss:', val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure post training perplexity on validation set\n",
    "# Create function that receives a model, context length, and PPL sequence length, and returns the perplexity\n",
    "# The PPL sequence length is the number of characters the function uses to calculate the perplexity\n",
    "# We take the logits and calculate the cross entropy loss from scratch, then exponentiate it to get the perplexity\n",
    "# not only that, but we want the models to do this in actual inference\n",
    "def perplexity(model, seq_len, ppl_seq_len, batch_size=128):\n",
    "    with torch.no_grad():\n",
    "        val_steps = 10\n",
    "        val_loss = 0\n",
    "        for _ in range(val_steps):\n",
    "            xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "            logits, _ = model(xb, yb)\n",
    "            logits = logits.reshape(batch_size, seq_len, vocab_size)\n",
    "            logits = logits[:, :ppl_seq_len]\n",
    "            yb = yb[:, :ppl_seq_len]\n",
    "            # flatten logits and targets\n",
    "            logits = logits.reshape(batch_size*ppl_seq_len, vocab_size)\n",
    "            yb = yb.reshape(batch_size*ppl_seq_len)\n",
    "            # calculate cross entropy loss from scratch\n",
    "            loss = F.cross_entropy(logits, yb)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= val_steps\n",
    "        ppl = torch.exp(torch.tensor(val_loss))\n",
    "        return ppl.item(), val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Compressed AttentioN (SCAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attend Folded Keys Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_fold(x, window_len):\n",
    "    # x is a (B, T, C) tensor\n",
    "    # output should be a (B, T, window_len, C) tensor that slides over the T dimension\n",
    "    # first window_len-1 elements will have to be padded with zeros in the beginning\n",
    "    # example: x = torch.tensor([[[1,2],[3,4],[5,6],[7,8],[9,10]]])\n",
    "    # sliding_window_fold(x, 2) -> torch.tensor([[[[0,1],[1,2]],[[1,2],[3,4]],[[3,4],[5,6]],[[5,6],[7,8]],[[7,8],[9,10]]]])\n",
    "    padded_x = F.pad(x, (0, 0, window_len - 1, 0), mode='constant', value=0)\n",
    "    output = padded_x.unfold(dimension=1, size=window_len, step=1).permute(0, 1, 3, 2)\n",
    "    return output\n",
    "\n",
    "def attend_folded_all_keys_torch(q, k, states, W):\n",
    "    k = sliding_window_fold(k, W) # (B, T, W, C)\n",
    "    all_keys = torch.cat((states, k), dim=2) # (B, T, S, C) + (B, T, W, C) -> (B, T, S+W, C)\n",
    "    scores = torch.einsum(\"btc, btxc -> btx\", q, all_keys) # (B, T, S+W)\n",
    "    return scores\n",
    "\n",
    "def accumulate_folded_all_values_torch(s, v, states, W):\n",
    "    v = sliding_window_fold(v, W) # (B, T, W, C)\n",
    "    all_values = torch.cat((states, v), dim=2) # (B, T, S, C) + (B, T, W, C) -> (B, T, S+W, C)\n",
    "    out = torch.einsum(\"btx, btxc -> btc\", s, all_values) # (B, T, C)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triton kernel\n",
    "@triton.jit\n",
    "def afak_fwd_kernel(\n",
    "    q_ptr, k_ptr, states_ptr, y_ptr,\n",
    "    B: tl.constexpr, T: tl.constexpr, S:tl.constexpr, C: tl.constexpr, W: tl.constexpr,\n",
    "    BLOCK_SIZE_S: tl.constexpr,\n",
    "    BLOCK_SIZE_W: tl.constexpr,\n",
    "):\n",
    "    # Use multiple program IDs for better parallelization\n",
    "    b_id = tl.program_id(axis=0)\n",
    "    t_id = tl.program_id(axis=1)\n",
    "    sw_block_id = tl.program_id(axis=2)\n",
    "    num_s_blocks = triton.cdiv(S, BLOCK_SIZE_S)\n",
    "    num_w_blocks = triton.cdiv(W, BLOCK_SIZE_W)\n",
    "    SW = S + W\n",
    "\n",
    "    # Compute base pointers\n",
    "    q_base = q_ptr + b_id * T * C\n",
    "    k_base = k_ptr + b_id * T * C\n",
    "    states_base = states_ptr + b_id * T * S * C\n",
    "    y_base = y_ptr + b_id * T * W\n",
    "\n",
    "    # Fetch the query at [b_id, t_id, :]\n",
    "    q_block_ptr = tl.make_block_ptr(\n",
    "        base=q_ptr,\n",
    "        shape=(B, T, C),\n",
    "        strides=(T * C, C, 1),\n",
    "        offsets=(b_id, t_id, 0),\n",
    "        block_shape=(1, 1, C),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    q = tl.load(q_block_ptr) # (1, 1, C)\n",
    "\n",
    "    if sw_block_id < num_s_blocks:\n",
    "        s_first_id = sw_block_id * BLOCK_SIZE_S\n",
    "        # Fetch the states at [b_id, t_id, s_first_id:s_first_id+BLOCK_SIZE_S, :]\n",
    "        s_block_ptr = tl.make_block_ptr(\n",
    "            base=states_ptr,\n",
    "            shape=(B, T, S, C),\n",
    "            strides=(T * S * C, S * C, C, 1),\n",
    "            offsets=(b_id, t_id, s_first_id, 0),\n",
    "            block_shape=(1, 1, BLOCK_SIZE_S, C),\n",
    "            order=(0, 1, 2, 3),\n",
    "        )\n",
    "        s = tl.load(s_block_ptr) # (1, 1, BLOCK_SIZE_S, C)\n",
    "        o = q[:, :, None, :] * s # (1, 1, BLOCK_SIZE_S, C)\n",
    "        o = tl.sum(o, axis=-1) # (1, 1, BLOCK_SIZE_S)\n",
    "        # Store the result\n",
    "        y_block_ptr = tl.make_block_ptr(\n",
    "            base=y_ptr,\n",
    "            shape=(B, T, SW),\n",
    "            strides=(T * SW, SW, 1),\n",
    "            offsets=(b_id, t_id, s_first_id),\n",
    "            block_shape=(1, 1, BLOCK_SIZE_S),\n",
    "            order=(0, 1, 2),\n",
    "        )\n",
    "        tl.store(y_block_ptr, o) # (1, 1, BLOCK_SIZE_S)\n",
    "    else:\n",
    "        w_first_id = (sw_block_id - num_s_blocks) * BLOCK_SIZE_W\n",
    "        # Fetch the key at [b_id, t_id-W+1+(w_block_id*BLOCK_SIZE_W):t_id+(w_block_id*BLOCK_SIZE_W), :]\n",
    "        # need to load the keys manually because make_block_ptr doesn't support masks\n",
    "        tw_offs = tl.arange(0, BLOCK_SIZE_W)\n",
    "        c_offs = tl.arange(0, C)\n",
    "        k_block_ptr = k_base + (t_id - W + 1 + (w_first_id + tw_offs[:, None])) * C + c_offs[None, :]\n",
    "        mask = w_first_id + tl.arange(0, BLOCK_SIZE_W)[:, None] > (W - t_id - 2)\n",
    "        k = tl.load(k_block_ptr, mask=mask) # (BLOCK_SIZE_W, C)\n",
    "        # Compute the dot product (but not with tl.dot because it has a minimum size of 16)\n",
    "        y = q * k[None, :] # (1, BLOCK_SIZE_W, C)\n",
    "        y = tl.sum(y, axis=-1) # (1, BLOCK_SIZE_W)\n",
    "        # Store the result\n",
    "        y_block_ptr = tl.make_block_ptr(\n",
    "            base=y_ptr,\n",
    "            shape=(B, T, SW),\n",
    "            strides=(T * SW, SW, 1),\n",
    "            offsets=(b_id, t_id, S + w_first_id),\n",
    "            block_shape=(1, 1, BLOCK_SIZE_W),\n",
    "            order=(0, 1, 2),\n",
    "        )\n",
    "        tl.store(y_block_ptr, y[None, :]) # (1, 1, BLOCK_SIZE_W)\n",
    "\n",
    "@triton.jit\n",
    "def afak_bwd_kernel(\n",
    "    q_ptr, k_ptr, states_ptr, dy_ptr, dq_ptr, dk_ptr, ds_ptr,\n",
    "    B: tl.constexpr, T: tl.constexpr, S: tl.constexpr, C: tl.constexpr, W: tl.constexpr,\n",
    "    BLOCK_SIZE_C: tl.constexpr,\n",
    "):\n",
    "    # Use multiple program IDs for better parallelization\n",
    "    b_id = tl.program_id(axis=0)\n",
    "    t_id = tl.program_id(axis=1)\n",
    "    c_block_id = tl.program_id(axis=2)\n",
    "    c_first_id = c_block_id * BLOCK_SIZE_C\n",
    "    SW = S + W\n",
    "\n",
    "    # Compute base pointers\n",
    "    q_base = q_ptr + b_id * T * C\n",
    "    k_base = k_ptr + b_id * T * C\n",
    "    dy_base = dy_ptr + b_id * T * SW\n",
    "    dq_base = dq_ptr + b_id * T * C\n",
    "    dk_base = dk_ptr + b_id * T * C\n",
    "\n",
    "    # First calculate the gradients for q\n",
    "    # Fetch original keys at [b_id, t_id-W+1:t_id, c_first_id:c_first_id+BLOCK_SIZE_C]\n",
    "    # using a block ptr also disallows the use of masks when loading, so let's just make a ptr manually\n",
    "    tw_offs = tl.arange(0, W)\n",
    "    c_offs = tl.arange(0, BLOCK_SIZE_C)\n",
    "    k_block_ptr = k_base + (t_id - W + 1 + tw_offs[:, None]) * C + c_first_id + c_offs[None, :]\n",
    "    mask = tl.arange(0, W)[:, None] > (W - t_id - 2)\n",
    "    k = tl.load(k_block_ptr, mask=mask) # (W, BLOCK_SIZE_C)\n",
    "    # Fetch output gradients at [b_id, t_id, S:W]\n",
    "    dy_block_ptr = tl.make_block_ptr(\n",
    "        base=dy_ptr,\n",
    "        shape=(B, T, SW),\n",
    "        strides=(T * SW, SW, 1),\n",
    "        offsets=(b_id, t_id, S),\n",
    "        block_shape=(1, 1, W),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    dy = tl.load(dy_block_ptr) # (1, 1, W)\n",
    "    # Compute the gradients for q\n",
    "    dqk = dy.permute(0, 2, 1) * k[None, :] # (1, W, BLOCK_SIZE_C)\n",
    "    dqk = tl.sum(dqk, axis=1) # (1, BLOCK_SIZE_C)\n",
    "    # Then we also have to add the gradients from the states\n",
    "    # Fetch the states at [b_id, t_id, c_first_id:c_first_id+BLOCK_SIZE_C]\n",
    "    s_block_ptr = tl.make_block_ptr(\n",
    "        base=states_ptr,\n",
    "        shape=(B, T, S, C),\n",
    "        strides=(T * S * C, S * C, C, 1),\n",
    "        offsets=(b_id, t_id, 0, c_first_id),\n",
    "        block_shape=(1, 1, S, BLOCK_SIZE_C),\n",
    "        order=(0, 1, 2, 3),\n",
    "    )\n",
    "    s = tl.load(s_block_ptr) # (1, 1, S, BLOCK_SIZE_C)\n",
    "    # Fetch the output gradients at [b_id, t_id, :S]\n",
    "    dy_block_ptr = tl.make_block_ptr(\n",
    "        base=dy_ptr,\n",
    "        shape=(B, T, SW),\n",
    "        strides=(T * SW, SW, 1),\n",
    "        offsets=(b_id, t_id, 0),\n",
    "        block_shape=(1, 1, S),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    dy = tl.load(dy_block_ptr) # (1, 1, S)\n",
    "    # Compute the gradients for q\n",
    "    dqs = dy[:, :, :, None] * s # (1, 1, S, BLOCK_SIZE_C)\n",
    "    dqs = tl.sum(dqs, axis=2) # (1, 1, BLOCK_SIZE_C)\n",
    "    dq = dqk[None, :] + dqs # (1, 1, BLOCK_SIZE_C)\n",
    "    # Store the result\n",
    "    dq_block_ptr = tl.make_block_ptr(\n",
    "        base=dq_ptr,\n",
    "        shape=(B, T, C),\n",
    "        strides=(T * C, C, 1),\n",
    "        offsets=(b_id, t_id, c_first_id),\n",
    "        block_shape=(1, 1, BLOCK_SIZE_C),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    tl.store(dq_block_ptr, dq) # (1, 1, BLOCK_SIZE_C)\n",
    "\n",
    "    # Calculate the gradients for states while we're at it\n",
    "    # Fetch the query at [b_id, t_id, c_first_id:c_first_id+BLOCK_SIZE_C]\n",
    "    q_block_ptr = tl.make_block_ptr(\n",
    "        base=q_ptr,\n",
    "        shape=(B, T, C),\n",
    "        strides=(T * C, C, 1),\n",
    "        offsets=(b_id, t_id, c_first_id),\n",
    "        block_shape=(1, 1, BLOCK_SIZE_C),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    q = tl.load(q_block_ptr) # (1, 1, BLOCK_SIZE_C)\n",
    "    # Compute the gradients for states\n",
    "    ds = dy[:, :, :, None] * q[:, :, None, :] # (1, 1, S, BLOCK_SIZE_C)\n",
    "    # Store the result\n",
    "    ds_block_ptr = tl.make_block_ptr(\n",
    "        base=ds_ptr,\n",
    "        shape=(B, T, S, C),\n",
    "        strides=(T * S * C, S * C, C, 1),\n",
    "        offsets=(b_id, t_id, 0, c_first_id),\n",
    "        block_shape=(1, 1, S, BLOCK_SIZE_C),\n",
    "        order=(0, 1, 2, 3),\n",
    "    )\n",
    "    tl.store(ds_block_ptr, ds) # (1, 1, S, BLOCK_SIZE_C)\n",
    "\n",
    "    # Then calculate the gradients for k\n",
    "    # same thing here, let's just make the ptr manually\n",
    "    tw_offs = tl.arange(0, W)\n",
    "    c_offs = tl.arange(0, BLOCK_SIZE_C)\n",
    "    q_block_ptr = q_base + (t_id + tw_offs[:, None]) * C + c_first_id + c_offs[None, :]\n",
    "    mask = tl.arange(0, W)[:, None] < T - t_id\n",
    "    q = tl.load(q_block_ptr, mask=mask) # (W, BLOCK_SIZE_C)\n",
    "    # Fetch original gradients at [b_id, t_id, :]\n",
    "    # This one is tricky bc we have to fetch a diagonal from dy\n",
    "    # going from [b_id, t_id, W] to [b_id, t_id+W, 0]\n",
    "    # only way to do this is to load the whole dy tensor and then mask it, then sum along the last axis\n",
    "    # tw_offs = tl.arange(0, W)\n",
    "    # w_offs = tl.arange(0, W)\n",
    "    # dy_block_ptr = dy_base + (t_id + tw_offs[:, None]) * SW + S + w_offs[None, :]\n",
    "    # mask = (tl.arange(0, W)[:, None] + tl.arange(0, W)[None, :] == W - 1) & (tl.arange(0, W)[:, None] < T - t_id)\n",
    "    # dy = tl.load(dy_block_ptr, mask=mask) # (W, W)\n",
    "    # dy = tl.sum(dy, axis=-1, keep_dims=True) # (W, 1)\n",
    "    # actually no we are better than this, we can get the exact pointers in the diagonal\n",
    "    w_offs = tl.arange(0, W)\n",
    "    diag_dy_base = dy_base + t_id * SW + S + tl.flip(w_offs, 0)\n",
    "    dy_block_ptr = diag_dy_base + w_offs * SW\n",
    "    mask = tl.arange(0, W) < T - t_id\n",
    "    dy = tl.load(dy_block_ptr, mask=mask) # (W)\n",
    "    # Compute the gradients for k\n",
    "    dk = dy.reshape(W, 1) * q # (W, BLOCK_SIZE_C)\n",
    "    dk = tl.sum(dk, axis=0) # (BLOCK_SIZE_C)\n",
    "    # Store the result\n",
    "    dk_block_ptr = tl.make_block_ptr(\n",
    "        base=dk_ptr,\n",
    "        shape=(B, T, C),\n",
    "        strides=(T * C, C, 1),\n",
    "        offsets=(b_id, t_id, c_first_id),\n",
    "        block_shape=(1, 1, BLOCK_SIZE_C),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    tl.store(dk_block_ptr, dk.reshape(1, 1, BLOCK_SIZE_C)) # (1, 1, BLOCK_SIZE_C)\n",
    "    \n",
    "class AttendFoldedAllKeysTriton(torch.autograd.Function):\n",
    "    # @torch.compiler.disable\n",
    "    @staticmethod\n",
    "    def forward(ctx, q, k, states, W):\n",
    "        B, T, C = q.shape\n",
    "        B, T, S, C = states.shape\n",
    "        q = q.contiguous()\n",
    "        k = k.contiguous()\n",
    "        states = states.contiguous()\n",
    "        ctx.save_for_backward(q, k, states)\n",
    "        ctx.W = W\n",
    "        \n",
    "        # Configure block sizes\n",
    "        BLOCK_SIZE_S = 16\n",
    "        BLOCK_SIZE_W = 16\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        num_s_blocks = triton.cdiv(S, BLOCK_SIZE_S)\n",
    "        num_w_blocks = triton.cdiv(W, BLOCK_SIZE_W)\n",
    "        grid = (B, T, num_s_blocks+num_w_blocks)\n",
    "\n",
    "        # Allocate output tensor\n",
    "        y = torch.zeros((B, T, S+W), dtype=q.dtype, device=q.device).contiguous()\n",
    "        \n",
    "        # Launch kernel\n",
    "        afak_fwd_kernel[grid](\n",
    "            q, k, states, y,\n",
    "            B, T, S, C, W,\n",
    "            BLOCK_SIZE_S=BLOCK_SIZE_S,\n",
    "            BLOCK_SIZE_W=BLOCK_SIZE_W,\n",
    "        )\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # @torch.compiler.disable\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.contiguous()\n",
    "        q, k, states = ctx.saved_tensors\n",
    "        B, T, S, C = states.shape\n",
    "        W = ctx.W\n",
    "        \n",
    "        # Configure block sizes\n",
    "        BLOCK_SIZE_C = 16\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        num_c_blocks = triton.cdiv(C, BLOCK_SIZE_C)\n",
    "        grid = (B, T, num_c_blocks)\n",
    "        \n",
    "        gq = torch.zeros_like(q).contiguous()\n",
    "        gk = torch.zeros_like(k).contiguous()\n",
    "        gs = torch.zeros_like(states).contiguous()\n",
    "\n",
    "        # Launch kernel\n",
    "        afak_bwd_kernel[grid](\n",
    "            q, k, states, grad_output, gq, gk, gs,\n",
    "            B, T, S, C, W,\n",
    "            BLOCK_SIZE_C=BLOCK_SIZE_C,\n",
    "        )\n",
    "\n",
    "        return gq, gk, gs, None\n",
    "\n",
    "# @torch.compile\n",
    "def attend_folded_all_keys_triton(q, k, states, W):\n",
    "    return AttendFoldedAllKeysTriton.apply(q, k, states, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: tensor([[[-1.66,  1.93,  ...,  0.76, -0.56],\n",
      "         [ 0.28,  0.50,  ..., -0.77, -0.10],\n",
      "         ...,\n",
      "         [-0.94,  0.83,  ..., -0.31,  1.99],\n",
      "         [-1.20, -0.74,  ..., -0.40,  0.56]],\n",
      "\n",
      "        [[-1.55, -0.52,  ..., -0.94,  1.08],\n",
      "         [ 1.64,  1.72,  ..., -0.67, -0.35],\n",
      "         ...,\n",
      "         [-0.30,  0.34,  ...,  0.22, -0.48],\n",
      "         [-2.28, -1.45,  ...,  1.63, -0.17]],\n",
      "\n",
      "        [[ 0.08,  0.12,  ..., -0.53, -1.34],\n",
      "         [-0.90, -1.02,  ..., -0.73,  0.11],\n",
      "         ...,\n",
      "         [ 0.47,  0.30,  ...,  0.24,  0.63],\n",
      "         [-1.58, -1.29,  ...,  0.19, -0.87]],\n",
      "\n",
      "        [[-0.09,  1.91,  ..., -1.22, -0.08],\n",
      "         [ 0.14,  0.03,  ..., -0.39, -0.38],\n",
      "         ...,\n",
      "         [ 0.18, -1.77,  ...,  0.73, -0.48],\n",
      "         [-0.11, -0.20,  ...,  1.67, -0.06]]], device='cuda:0')\n",
      "k: tensor([[[ 0.50, -0.05,  ..., -1.19,  1.71],\n",
      "         [-0.77, -1.38,  ...,  0.05, -1.33],\n",
      "         ...,\n",
      "         [ 0.85, -0.29,  ..., -1.34, -0.37],\n",
      "         [-0.79,  0.23,  ..., -0.56,  0.84]],\n",
      "\n",
      "        [[-2.17,  0.40,  ...,  0.74,  0.39],\n",
      "         [-0.35, -1.51,  ...,  1.82, -1.41],\n",
      "         ...,\n",
      "         [-0.11,  1.11,  ..., -0.33,  0.79],\n",
      "         [-0.80,  0.81,  ...,  0.88,  1.11]],\n",
      "\n",
      "        [[-0.25,  0.56,  ..., -0.28,  0.34],\n",
      "         [-0.48,  1.07,  ...,  0.42, -0.49],\n",
      "         ...,\n",
      "         [ 0.51, -1.01,  ..., -0.86, -0.74],\n",
      "         [ 0.74,  0.86,  ..., -0.47, -0.15]],\n",
      "\n",
      "        [[-1.15,  0.49,  ..., -0.15, -0.23],\n",
      "         [ 0.77, -0.77,  ..., -1.31, -0.29],\n",
      "         ...,\n",
      "         [ 0.55,  1.16,  ..., -0.65,  0.61],\n",
      "         [-0.14,  0.66,  ..., -1.32,  1.07]]], device='cuda:0')\n",
      "states: tensor([[[[-0.58, -0.89,  ..., -0.10, -0.60],\n",
      "          [-0.76,  0.90,  ...,  0.39, -2.33],\n",
      "          ...,\n",
      "          [-1.65,  1.45,  ...,  0.21, -1.42],\n",
      "          [ 0.27, -0.90,  ...,  0.01,  1.07]],\n",
      "\n",
      "         [[-0.46, -0.43,  ...,  0.25, -0.68],\n",
      "          [ 1.22,  0.98,  ..., -1.10,  0.21],\n",
      "          ...,\n",
      "          [ 0.96, -0.93,  ...,  0.50,  0.33],\n",
      "          [-1.38, -0.91,  ...,  0.73,  1.91]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.61, -2.57,  ..., -0.89, -1.40],\n",
      "          [ 0.21,  0.51,  ..., -1.42, -0.11],\n",
      "          ...,\n",
      "          [-0.46,  0.27,  ...,  1.71,  0.48],\n",
      "          [-1.83,  0.21,  ...,  0.35, -0.47]],\n",
      "\n",
      "         [[ 0.48, -1.45,  ..., -0.08, -1.42],\n",
      "          [ 0.16, -0.06,  ..., -0.36,  0.93],\n",
      "          ...,\n",
      "          [ 0.62, -1.05,  ...,  0.07, -0.17],\n",
      "          [ 0.26, -0.66,  ..., -0.60,  1.78]]],\n",
      "\n",
      "\n",
      "        [[[-0.28,  0.05,  ...,  0.47, -0.10],\n",
      "          [ 0.37, -0.34,  ...,  2.20,  0.06],\n",
      "          ...,\n",
      "          [-0.56, -0.94,  ...,  0.96,  1.57],\n",
      "          [-0.57, -1.25,  ..., -0.11,  1.23]],\n",
      "\n",
      "         [[ 0.14,  1.46,  ...,  0.12,  1.42],\n",
      "          [ 1.80, -0.00,  ...,  1.23, -1.01],\n",
      "          ...,\n",
      "          [-0.08,  0.25,  ..., -0.66, -0.27],\n",
      "          [ 1.11, -0.92,  ...,  0.36, -0.85]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.58,  0.09,  ..., -0.20,  0.39],\n",
      "          [ 1.14,  2.36,  ...,  0.70,  0.21],\n",
      "          ...,\n",
      "          [ 2.56,  0.75,  ..., -1.01, -1.06],\n",
      "          [ 1.88, -0.30,  ...,  0.12,  1.16]],\n",
      "\n",
      "         [[-0.87, -0.02,  ...,  0.83,  0.71],\n",
      "          [ 0.14, -1.04,  ..., -0.81,  0.64],\n",
      "          ...,\n",
      "          [-0.51, -0.23,  ...,  0.77,  0.97],\n",
      "          [ 2.71, -0.36,  ..., -1.55, -0.91]]],\n",
      "\n",
      "\n",
      "        [[[-0.11, -2.14,  ...,  0.97,  1.61],\n",
      "          [ 0.39,  0.03,  ...,  0.12, -0.28],\n",
      "          ...,\n",
      "          [-0.60,  0.93,  ..., -1.85, -1.88],\n",
      "          [-0.04,  0.28,  ...,  0.81, -0.04]],\n",
      "\n",
      "         [[ 0.96, -0.40,  ..., -1.82, -0.58],\n",
      "          [-0.56, -0.23,  ..., -0.43, -1.35],\n",
      "          ...,\n",
      "          [-0.82, -0.18,  ..., -0.77,  0.72],\n",
      "          [-0.66, -1.19,  ...,  0.85, -1.36]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.64,  0.21,  ..., -0.34, -0.71],\n",
      "          [ 0.42,  0.14,  ..., -0.73, -0.64],\n",
      "          ...,\n",
      "          [ 0.69, -1.71,  ..., -0.70, -0.45],\n",
      "          [-0.98, -0.04,  ...,  1.11,  0.20]],\n",
      "\n",
      "         [[ 1.48,  0.43,  ...,  1.76,  0.08],\n",
      "          [ 0.63, -0.34,  ...,  1.25, -1.36],\n",
      "          ...,\n",
      "          [-1.41,  0.78,  ..., -1.59,  0.03],\n",
      "          [-0.88,  0.68,  ..., -0.92, -1.80]]],\n",
      "\n",
      "\n",
      "        [[[-2.25,  1.99,  ...,  0.36,  1.43],\n",
      "          [-0.20, -2.38,  ...,  0.05,  0.87],\n",
      "          ...,\n",
      "          [-0.96, -0.68,  ...,  0.13,  0.52],\n",
      "          [ 0.62,  2.03,  ...,  0.79,  1.65]],\n",
      "\n",
      "         [[ 0.11,  0.24,  ..., -0.77, -2.15],\n",
      "          [ 0.73,  0.92,  ...,  1.17,  1.23],\n",
      "          ...,\n",
      "          [-1.04, -1.24,  ...,  0.10,  0.71],\n",
      "          [ 0.68, -1.85,  ...,  0.39,  0.42]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.12, -0.22,  ...,  1.06,  0.28],\n",
      "          [-0.89,  0.74,  ..., -0.36,  1.04],\n",
      "          ...,\n",
      "          [ 0.15, -0.57,  ...,  2.18,  0.56],\n",
      "          [-0.16,  0.73,  ...,  2.78, -1.12]],\n",
      "\n",
      "         [[ 0.29,  0.72,  ..., -1.62,  1.10],\n",
      "          [-1.32,  0.17,  ..., -0.48,  0.47],\n",
      "          ...,\n",
      "          [-0.49, -0.16,  ...,  0.43,  0.94],\n",
      "          [-0.35,  0.75,  ...,  0.18, -0.27]]]], device='cuda:0')\n",
      "torch:\n",
      "tensor([[[ 0.15,  1.39,  ...,  0.00,  1.41],\n",
      "         [ 7.82, -1.32,  ..., -1.28, -9.79],\n",
      "         ...,\n",
      "         [ 1.35,  5.41,  ...,  3.74, -6.30],\n",
      "         [-0.91,  6.01,  ..., -5.31, -2.42]],\n",
      "\n",
      "        [[-9.89, -3.15,  ...,  0.00, -5.66],\n",
      "         [ 7.56, -6.74,  ...,  3.67, -3.81],\n",
      "         ...,\n",
      "         [ 0.18, -2.62,  ..., -1.07,  1.15],\n",
      "         [ 4.06, -7.45,  ..., -6.96,  9.54]],\n",
      "\n",
      "        [[-0.22,  4.50,  ...,  0.00,  4.69],\n",
      "         [-1.47, -1.32,  ...,  6.03,  1.80],\n",
      "         ...,\n",
      "         [-3.95,  5.04,  ..., -5.64,  3.75],\n",
      "         [-2.17,  2.77,  ...,  0.78,  2.17]],\n",
      "\n",
      "        [[ 3.73, -9.39,  ...,  0.00, -1.06],\n",
      "         [-3.33, -1.92,  ...,  0.44,  5.53],\n",
      "         ...,\n",
      "         [-2.59,  0.87,  ...,  2.13, -7.99],\n",
      "         [-3.47,  4.26,  ..., -7.57,  6.23]]], device='cuda:0')\n",
      "triton:\n",
      "tensor([[[ 0.15,  1.39,  ...,  0.00,  1.41],\n",
      "         [ 7.82, -1.32,  ..., -1.28, -9.79],\n",
      "         ...,\n",
      "         [ 1.35,  5.41,  ...,  3.74, -6.30],\n",
      "         [-0.91,  6.01,  ..., -5.31, -2.42]],\n",
      "\n",
      "        [[-9.89, -3.15,  ...,  0.00, -5.66],\n",
      "         [ 7.56, -6.74,  ...,  3.67, -3.81],\n",
      "         ...,\n",
      "         [ 0.18, -2.62,  ..., -1.07,  1.15],\n",
      "         [ 4.06, -7.45,  ..., -6.96,  9.54]],\n",
      "\n",
      "        [[-0.22,  4.50,  ...,  0.00,  4.69],\n",
      "         [-1.47, -1.32,  ...,  6.03,  1.80],\n",
      "         ...,\n",
      "         [-3.95,  5.04,  ..., -5.64,  3.75],\n",
      "         [-2.17,  2.77,  ...,  0.78,  2.17]],\n",
      "\n",
      "        [[ 3.73, -9.39,  ...,  0.00, -1.06],\n",
      "         [-3.33, -1.92,  ...,  0.44,  5.53],\n",
      "         ...,\n",
      "         [-2.59,  0.87,  ...,  2.13, -7.99],\n",
      "         [-3.47,  4.26,  ..., -7.57,  6.23]]], device='cuda:0')\n",
      "max diff: tensor(    0.00, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# try the forward pass\n",
    "# test attend_folded_keys_torch with always the same random inputs\n",
    "B, T, S, W, C = 4, 128, 32, 32, 32\n",
    "q = torch.randn(B, T, C, device=device)\n",
    "print(\"q:\", q)\n",
    "k = torch.randn(B, T, C, device=device)\n",
    "print(\"k:\", k)\n",
    "states = torch.randn(B, T, S, C, device=device)\n",
    "print(\"states:\", states)\n",
    "print(\"torch:\")\n",
    "out_torch = attend_folded_all_keys_torch(q, k, states, W)\n",
    "print(out_torch)\n",
    "print(\"triton:\")\n",
    "out_triton = attend_folded_all_keys_triton(q, k, states, W)\n",
    "print(out_triton)\n",
    "print(\"max diff:\", (out_torch - out_triton).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_output: tensor([[[    -0.01,      0.03,  ...,      0.31,     -0.28],\n",
      "         [     0.06,      0.00,  ...,      1.09,     -1.20],\n",
      "         ...,\n",
      "         [     1.79,      0.11,  ...,     -0.56,     -2.66],\n",
      "         [     1.73,     -0.48,  ...,     -0.52,     -0.50]],\n",
      "\n",
      "        [[    -0.83,      0.19,  ...,     -0.74,     -0.80],\n",
      "         [    -1.59,      0.28,  ...,      0.70,     -0.12],\n",
      "         ...,\n",
      "         [    -0.04,      0.01,  ...,     -1.63,      0.49],\n",
      "         [     1.09,      0.65,  ...,      0.47,     -0.57]],\n",
      "\n",
      "        [[    -0.48,     -0.39,  ...,      0.11,     -0.22],\n",
      "         [     0.99,     -1.48,  ...,      0.27,     -1.16],\n",
      "         ...,\n",
      "         [    -2.08,     -1.09,  ...,     -0.79,      0.13],\n",
      "         [    -0.03,     -2.60,  ...,     -0.44,     -0.03]],\n",
      "\n",
      "        [[     1.21,      1.57,  ...,      0.70,      2.28],\n",
      "         [     1.11,      1.02,  ...,      0.65,      0.45],\n",
      "         ...,\n",
      "         [     1.85,     -0.93,  ...,     -0.99,      0.33],\n",
      "         [    -0.13,      0.63,  ...,     -1.06,      0.51]]], device='cuda:0')\n",
      "torch:\n",
      "q grad: tensor([[[   -13.75,      5.57,  ...,     -5.74,     -4.20],\n",
      "         [     0.57,      3.72,  ...,     -0.16,     -2.73],\n",
      "         ...,\n",
      "         [     2.07,     -3.57,  ...,      7.29,     12.38],\n",
      "         [    -5.37,      2.64,  ...,      1.89,     -4.70]],\n",
      "\n",
      "        [[     3.91,     -3.52,  ...,      4.26,     -2.35],\n",
      "         [     0.49,     -3.20,  ...,     -2.77,      2.12],\n",
      "         ...,\n",
      "         [    26.48,      2.99,  ...,     -0.84,    -15.59],\n",
      "         [    -6.82,     -0.06,  ...,     -0.56,      6.80]],\n",
      "\n",
      "        [[    -3.56,     -0.50,  ...,      1.10,     -3.46],\n",
      "         [     2.63,      2.85,  ...,     -8.19,      7.17],\n",
      "         ...,\n",
      "         [    -0.67,     -3.12,  ...,      2.87,     -4.12],\n",
      "         [     0.12,      3.90,  ...,    -13.01,      1.46]],\n",
      "\n",
      "        [[    -3.14,     -2.63,  ...,     -8.50,      2.94],\n",
      "         [    -1.65,      1.08,  ...,      0.57,     -5.08],\n",
      "         ...,\n",
      "         [    -0.79,     -1.96,  ...,    -19.55,     -0.01],\n",
      "         [   -11.68,      1.25,  ...,     -3.89,      8.99]]], device='cuda:0')\n",
      "k grad: tensor([[[     1.19,      5.77,  ...,     -6.42,      1.46],\n",
      "         [     7.37,     -1.93,  ...,      6.06,      5.37],\n",
      "         ...,\n",
      "         [     3.13,     -1.83,  ...,      1.03,     -5.57],\n",
      "         [     0.60,      0.37,  ...,      0.20,     -0.28]],\n",
      "\n",
      "        [[     2.14,     -2.45,  ...,      9.74,     -8.73],\n",
      "         [    -0.92,    -18.77,  ...,     -6.69,     -7.34],\n",
      "         ...,\n",
      "         [    -1.21,     -0.51,  ...,      0.87,     -0.32],\n",
      "         [     1.30,      0.82,  ...,     -0.93,      0.10]],\n",
      "\n",
      "        [[    -6.53,      0.68,  ...,      9.01,      6.08],\n",
      "         [    -4.94,      7.72,  ...,     15.43,      1.59],\n",
      "         ...,\n",
      "         [     0.76,      0.61,  ...,     -0.05,      0.47],\n",
      "         [     0.04,      0.03,  ...,     -0.00,      0.02]],\n",
      "\n",
      "        [[     0.22,      3.20,  ...,      4.05,      1.16],\n",
      "         [    -8.25,     -1.91,  ...,     -7.08,      0.32],\n",
      "         ...,\n",
      "         [     0.18,     -0.37,  ...,     -1.53,     -0.09],\n",
      "         [    -0.06,     -0.10,  ...,      0.84,     -0.03]]], device='cuda:0')\n",
      "states grad: tensor([[[[     0.02,     -0.02,  ...,     -0.01,      0.01],\n",
      "          [    -0.04,      0.05,  ...,      0.02,     -0.02],\n",
      "          ...,\n",
      "          [    -2.88,      3.35,  ...,      1.31,     -0.96],\n",
      "          [    -0.52,      0.60,  ...,      0.24,     -0.17]],\n",
      "\n",
      "         [[     0.02,      0.03,  ...,     -0.05,     -0.01],\n",
      "          [     0.00,      0.00,  ...,     -0.00,     -0.00],\n",
      "          ...,\n",
      "          [     0.08,      0.14,  ...,     -0.22,     -0.03],\n",
      "          [     0.24,      0.43,  ...,     -0.66,     -0.09]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -1.68,      1.49,  ...,     -0.55,      3.55],\n",
      "          [    -0.10,      0.09,  ...,     -0.03,      0.21],\n",
      "          ...,\n",
      "          [    -0.01,      0.01,  ...,     -0.00,      0.01],\n",
      "          [     0.53,     -0.47,  ...,      0.17,     -1.11]],\n",
      "\n",
      "         [[    -2.07,     -1.28,  ...,     -0.69,      0.96],\n",
      "          [     0.57,      0.35,  ...,      0.19,     -0.27],\n",
      "          ...,\n",
      "          [     0.32,      0.20,  ...,      0.11,     -0.15],\n",
      "          [     1.18,      0.73,  ...,      0.39,     -0.55]]],\n",
      "\n",
      "\n",
      "        [[[     1.29,      0.43,  ...,      0.79,     -0.90],\n",
      "          [    -0.29,     -0.10,  ...,     -0.18,      0.20],\n",
      "          ...,\n",
      "          [     1.12,      0.38,  ...,      0.68,     -0.78],\n",
      "          [    -0.97,     -0.33,  ...,     -0.59,      0.68]],\n",
      "\n",
      "         [[    -2.60,     -2.73,  ...,      1.07,      0.55],\n",
      "          [     0.46,      0.49,  ...,     -0.19,     -0.10],\n",
      "          ...,\n",
      "          [     2.03,      2.13,  ...,     -0.83,     -0.43],\n",
      "          [    -1.77,     -1.86,  ...,      0.73,      0.38]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0.01,     -0.01,  ...,     -0.01,      0.02],\n",
      "          [    -0.00,      0.00,  ...,      0.00,     -0.00],\n",
      "          ...,\n",
      "          [    -0.15,      0.17,  ...,      0.11,     -0.24],\n",
      "          [     0.03,     -0.03,  ...,     -0.02,      0.05]],\n",
      "\n",
      "         [[    -2.48,     -1.58,  ...,      1.78,     -0.18],\n",
      "          [    -1.48,     -0.94,  ...,      1.06,     -0.11],\n",
      "          ...,\n",
      "          [    -0.12,     -0.07,  ...,      0.08,     -0.01],\n",
      "          [     2.90,      1.84,  ...,     -2.08,      0.21]]],\n",
      "\n",
      "\n",
      "        [[[    -0.04,     -0.06,  ...,      0.26,      0.65],\n",
      "          [    -0.03,     -0.05,  ...,      0.20,      0.52],\n",
      "          ...,\n",
      "          [    -0.04,     -0.06,  ...,      0.25,      0.62],\n",
      "          [    -0.08,     -0.13,  ...,      0.56,      1.42]],\n",
      "\n",
      "         [[    -0.89,     -1.00,  ...,     -0.72,      0.11],\n",
      "          [     1.33,      1.50,  ...,      1.08,     -0.16],\n",
      "          ...,\n",
      "          [    -2.03,     -2.29,  ...,     -1.65,      0.24],\n",
      "          [     0.56,      0.63,  ...,      0.45,     -0.07]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -0.97,     -0.62,  ...,     -0.49,     -1.30],\n",
      "          [    -0.51,     -0.33,  ...,     -0.26,     -0.69],\n",
      "          ...,\n",
      "          [     0.84,      0.54,  ...,      0.42,      1.13],\n",
      "          [     0.21,      0.14,  ...,      0.11,      0.29]],\n",
      "\n",
      "         [[     0.04,      0.04,  ...,     -0.01,      0.02],\n",
      "          [     4.09,      3.35,  ...,     -0.48,      2.27],\n",
      "          ...,\n",
      "          [    -2.28,     -1.86,  ...,      0.27,     -1.26],\n",
      "          [    -2.51,     -2.05,  ...,      0.30,     -1.39]]],\n",
      "\n",
      "\n",
      "        [[[    -0.11,      2.32,  ...,     -1.47,     -0.10],\n",
      "          [    -0.15,      3.01,  ...,     -1.91,     -0.13],\n",
      "          ...,\n",
      "          [    -0.01,      0.13,  ...,     -0.08,     -0.01],\n",
      "          [     0.03,     -0.58,  ...,      0.37,      0.03]],\n",
      "\n",
      "         [[     0.16,      0.03,  ...,     -0.43,     -0.42],\n",
      "          [     0.15,      0.03,  ...,     -0.39,     -0.39],\n",
      "          ...,\n",
      "          [     0.08,      0.01,  ...,     -0.21,     -0.20],\n",
      "          [     0.05,      0.01,  ...,     -0.13,     -0.13]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0.33,     -3.28,  ...,      1.35,     -0.89],\n",
      "          [    -0.16,      1.65,  ...,     -0.68,      0.45],\n",
      "          ...,\n",
      "          [    -0.16,      1.59,  ...,     -0.65,      0.43],\n",
      "          [    -0.33,      3.28,  ...,     -1.35,      0.89]],\n",
      "\n",
      "         [[     0.01,      0.03,  ...,     -0.22,      0.01],\n",
      "          [    -0.07,     -0.13,  ...,      1.05,     -0.04],\n",
      "          ...,\n",
      "          [    -0.03,     -0.05,  ...,      0.44,     -0.02],\n",
      "          [     0.05,      0.09,  ...,     -0.70,      0.03]]]], device='cuda:0')\n",
      "triton:\n",
      "q grad: tensor([[[   -13.75,      5.57,  ...,     -5.74,     -4.20],\n",
      "         [     0.57,      3.72,  ...,     -0.16,     -2.73],\n",
      "         ...,\n",
      "         [     2.07,     -3.57,  ...,      7.29,     12.38],\n",
      "         [    -5.37,      2.64,  ...,      1.89,     -4.70]],\n",
      "\n",
      "        [[     3.91,     -3.52,  ...,      4.26,     -2.35],\n",
      "         [     0.49,     -3.20,  ...,     -2.77,      2.12],\n",
      "         ...,\n",
      "         [    26.48,      2.99,  ...,     -0.84,    -15.59],\n",
      "         [    -6.82,     -0.06,  ...,     -0.56,      6.80]],\n",
      "\n",
      "        [[    -3.56,     -0.50,  ...,      1.10,     -3.46],\n",
      "         [     2.63,      2.85,  ...,     -8.19,      7.17],\n",
      "         ...,\n",
      "         [    -0.67,     -3.12,  ...,      2.87,     -4.12],\n",
      "         [     0.12,      3.90,  ...,    -13.01,      1.46]],\n",
      "\n",
      "        [[    -3.14,     -2.63,  ...,     -8.50,      2.94],\n",
      "         [    -1.65,      1.08,  ...,      0.57,     -5.08],\n",
      "         ...,\n",
      "         [    -0.79,     -1.96,  ...,    -19.55,     -0.01],\n",
      "         [   -11.68,      1.25,  ...,     -3.89,      8.99]]], device='cuda:0')\n",
      "k grad: tensor([[[     1.19,      5.77,  ...,     -6.42,      1.46],\n",
      "         [     7.37,     -1.93,  ...,      6.06,      5.37],\n",
      "         ...,\n",
      "         [     3.13,     -1.83,  ...,      1.03,     -5.57],\n",
      "         [     0.60,      0.37,  ...,      0.20,     -0.28]],\n",
      "\n",
      "        [[     2.14,     -2.45,  ...,      9.74,     -8.73],\n",
      "         [    -0.92,    -18.77,  ...,     -6.69,     -7.34],\n",
      "         ...,\n",
      "         [    -1.21,     -0.51,  ...,      0.87,     -0.32],\n",
      "         [     1.30,      0.82,  ...,     -0.93,      0.10]],\n",
      "\n",
      "        [[    -6.53,      0.68,  ...,      9.01,      6.08],\n",
      "         [    -4.94,      7.72,  ...,     15.43,      1.59],\n",
      "         ...,\n",
      "         [     0.76,      0.61,  ...,     -0.05,      0.47],\n",
      "         [     0.04,      0.03,  ...,     -0.00,      0.02]],\n",
      "\n",
      "        [[     0.22,      3.20,  ...,      4.05,      1.16],\n",
      "         [    -8.25,     -1.91,  ...,     -7.08,      0.32],\n",
      "         ...,\n",
      "         [     0.18,     -0.37,  ...,     -1.53,     -0.09],\n",
      "         [    -0.06,     -0.10,  ...,      0.84,     -0.03]]], device='cuda:0')\n",
      "states grad: tensor([[[[     0.02,     -0.02,  ...,     -0.01,      0.01],\n",
      "          [    -0.04,      0.05,  ...,      0.02,     -0.02],\n",
      "          ...,\n",
      "          [    -2.88,      3.35,  ...,      1.31,     -0.96],\n",
      "          [    -0.52,      0.60,  ...,      0.24,     -0.17]],\n",
      "\n",
      "         [[     0.02,      0.03,  ...,     -0.05,     -0.01],\n",
      "          [     0.00,      0.00,  ...,     -0.00,     -0.00],\n",
      "          ...,\n",
      "          [     0.08,      0.14,  ...,     -0.22,     -0.03],\n",
      "          [     0.24,      0.43,  ...,     -0.66,     -0.09]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -1.68,      1.49,  ...,     -0.55,      3.55],\n",
      "          [    -0.10,      0.09,  ...,     -0.03,      0.21],\n",
      "          ...,\n",
      "          [    -0.01,      0.01,  ...,     -0.00,      0.01],\n",
      "          [     0.53,     -0.47,  ...,      0.17,     -1.11]],\n",
      "\n",
      "         [[    -2.07,     -1.28,  ...,     -0.69,      0.96],\n",
      "          [     0.57,      0.35,  ...,      0.19,     -0.27],\n",
      "          ...,\n",
      "          [     0.32,      0.20,  ...,      0.11,     -0.15],\n",
      "          [     1.18,      0.73,  ...,      0.39,     -0.55]]],\n",
      "\n",
      "\n",
      "        [[[     1.29,      0.43,  ...,      0.79,     -0.90],\n",
      "          [    -0.29,     -0.10,  ...,     -0.18,      0.20],\n",
      "          ...,\n",
      "          [     1.12,      0.38,  ...,      0.68,     -0.78],\n",
      "          [    -0.97,     -0.33,  ...,     -0.59,      0.68]],\n",
      "\n",
      "         [[    -2.60,     -2.73,  ...,      1.07,      0.55],\n",
      "          [     0.46,      0.49,  ...,     -0.19,     -0.10],\n",
      "          ...,\n",
      "          [     2.03,      2.13,  ...,     -0.83,     -0.43],\n",
      "          [    -1.77,     -1.86,  ...,      0.73,      0.38]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0.01,     -0.01,  ...,     -0.01,      0.02],\n",
      "          [    -0.00,      0.00,  ...,      0.00,     -0.00],\n",
      "          ...,\n",
      "          [    -0.15,      0.17,  ...,      0.11,     -0.24],\n",
      "          [     0.03,     -0.03,  ...,     -0.02,      0.05]],\n",
      "\n",
      "         [[    -2.48,     -1.58,  ...,      1.78,     -0.18],\n",
      "          [    -1.48,     -0.94,  ...,      1.06,     -0.11],\n",
      "          ...,\n",
      "          [    -0.12,     -0.07,  ...,      0.08,     -0.01],\n",
      "          [     2.90,      1.84,  ...,     -2.08,      0.21]]],\n",
      "\n",
      "\n",
      "        [[[    -0.04,     -0.06,  ...,      0.26,      0.65],\n",
      "          [    -0.03,     -0.05,  ...,      0.20,      0.52],\n",
      "          ...,\n",
      "          [    -0.04,     -0.06,  ...,      0.25,      0.62],\n",
      "          [    -0.08,     -0.13,  ...,      0.56,      1.42]],\n",
      "\n",
      "         [[    -0.89,     -1.00,  ...,     -0.72,      0.11],\n",
      "          [     1.33,      1.50,  ...,      1.08,     -0.16],\n",
      "          ...,\n",
      "          [    -2.03,     -2.29,  ...,     -1.65,      0.24],\n",
      "          [     0.56,      0.63,  ...,      0.45,     -0.07]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -0.97,     -0.62,  ...,     -0.49,     -1.30],\n",
      "          [    -0.51,     -0.33,  ...,     -0.26,     -0.69],\n",
      "          ...,\n",
      "          [     0.84,      0.54,  ...,      0.42,      1.13],\n",
      "          [     0.21,      0.14,  ...,      0.11,      0.29]],\n",
      "\n",
      "         [[     0.04,      0.04,  ...,     -0.01,      0.02],\n",
      "          [     4.09,      3.35,  ...,     -0.48,      2.27],\n",
      "          ...,\n",
      "          [    -2.28,     -1.86,  ...,      0.27,     -1.26],\n",
      "          [    -2.51,     -2.05,  ...,      0.30,     -1.39]]],\n",
      "\n",
      "\n",
      "        [[[    -0.11,      2.32,  ...,     -1.47,     -0.10],\n",
      "          [    -0.15,      3.01,  ...,     -1.91,     -0.13],\n",
      "          ...,\n",
      "          [    -0.01,      0.13,  ...,     -0.08,     -0.01],\n",
      "          [     0.03,     -0.58,  ...,      0.37,      0.03]],\n",
      "\n",
      "         [[     0.16,      0.03,  ...,     -0.43,     -0.42],\n",
      "          [     0.15,      0.03,  ...,     -0.39,     -0.39],\n",
      "          ...,\n",
      "          [     0.08,      0.01,  ...,     -0.21,     -0.20],\n",
      "          [     0.05,      0.01,  ...,     -0.13,     -0.13]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0.33,     -3.28,  ...,      1.35,     -0.89],\n",
      "          [    -0.16,      1.65,  ...,     -0.68,      0.45],\n",
      "          ...,\n",
      "          [    -0.16,      1.59,  ...,     -0.65,      0.43],\n",
      "          [    -0.33,      3.28,  ...,     -1.35,      0.89]],\n",
      "\n",
      "         [[     0.01,      0.03,  ...,     -0.22,      0.01],\n",
      "          [    -0.07,     -0.13,  ...,      1.05,     -0.04],\n",
      "          ...,\n",
      "          [    -0.03,     -0.05,  ...,      0.44,     -0.02],\n",
      "          [     0.05,      0.09,  ...,     -0.70,      0.03]]]], device='cuda:0')\n",
      "max diff q: tensor(    0.00, device='cuda:0')\n",
      "max diff k: tensor(    0.00, device='cuda:0')\n",
      "max diff states: tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khalid Zuhri\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "# try the backward pass, compare with torch autograd\n",
    "# test attend_folded_keys_torch with always the same random inputs\n",
    "grad_output = torch.randn(B, T, S+W, device=device)\n",
    "print(\"grad_output:\", grad_output)\n",
    "print(\"torch:\")\n",
    "q.requires_grad = True\n",
    "k.requires_grad = True\n",
    "states.requires_grad = True\n",
    "out = attend_folded_all_keys_torch(q, k, states, W)\n",
    "out.backward(grad_output)\n",
    "torch_q_grad = q.grad.clone()\n",
    "torch_k_grad = k.grad.clone()\n",
    "torch_states_grad = states.grad.clone()\n",
    "print('q grad:', q.grad)\n",
    "print('k grad:', k.grad)\n",
    "print('states grad:', states.grad)\n",
    "# reset gradients\n",
    "q.grad = None\n",
    "k.grad = None\n",
    "states.grad = None\n",
    "print(\"triton:\")\n",
    "out_triton = attend_folded_all_keys_triton(q, k, states, W)\n",
    "out_triton.backward(grad_output)\n",
    "triton_q_grad = q.grad.clone()\n",
    "triton_k_grad = k.grad.clone()\n",
    "triton_states_grad = states.grad.clone()\n",
    "print('q grad:', q.grad)\n",
    "print('k grad:', k.grad)\n",
    "print('states grad:', states.grad)\n",
    "print(\"max diff q:\", (torch_q_grad - triton_q_grad).abs().max())\n",
    "print(\"max diff k:\", (torch_k_grad - triton_k_grad).abs().max())\n",
    "print(\"max diff states:\", (torch_states_grad - triton_states_grad).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch forward:\n",
      "2.77 ms Â± 13.2 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n",
      "torch forward & backward:\n",
      "8.83 ms Â± 43.8 Î¼s per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "triton forward:\n",
      "153 Î¼s Â± 36.6 Î¼s per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "triton forward & backward:\n",
      "3.09 ms Â± 3.53 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# measure the speedup\n",
    "B, T, W, S, C = 128, 1024, 32, 32, 64\n",
    "q = torch.randn(B, T, C, device=device)\n",
    "k = torch.randn(B, T, C, device=device)\n",
    "states = torch.randn(B, T, S, C, device=device)\n",
    "grad_output = torch.randn(B, T, S+W, device=device)\n",
    "# warmup the triton kernel\n",
    "q.requires_grad = True\n",
    "k.requires_grad = True\n",
    "out = attend_folded_all_keys_triton(q, k, states, W)\n",
    "out.backward(grad_output)\n",
    "q.grad = None\n",
    "k.grad = None\n",
    "\n",
    "B, T, W, S, C = 128, 128, 32, 32, 64\n",
    "q = torch.randn(B, T, C, device=device)\n",
    "k = torch.randn(B, T, C, device=device)\n",
    "states = torch.randn(B, T, S, C, device=device)\n",
    "grad_output = torch.randn(B, T, S+W, device=device)\n",
    "print(\"torch forward:\")\n",
    "q.requires_grad = True\n",
    "k.requires_grad = True\n",
    "%timeit attend_folded_all_keys_torch(q, k, states, W)\n",
    "print(\"torch forward & backward:\")\n",
    "def ftorch():\n",
    "    q.grad = None\n",
    "    k.grad = None\n",
    "    out_torch = attend_folded_all_keys_torch(q, k, states, W)\n",
    "    out_torch.backward(grad_output)\n",
    "%timeit ftorch()\n",
    "print(\"\\ntriton forward:\")\n",
    "q.requires_grad = True\n",
    "k.requires_grad = True\n",
    "%timeit attend_folded_all_keys_triton(q, k, states, W)\n",
    "print(\"triton forward & backward:\")\n",
    "def ftriton():\n",
    "    q.grad = None\n",
    "    k.grad = None\n",
    "    out_triton = attend_folded_all_keys_triton(q, k, states, W)\n",
    "    out_triton.backward(grad_output)\n",
    "%timeit ftriton()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulate Folded Values Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_fold(x, window_len):\n",
    "    # x is a (B, T, C) tensor\n",
    "    # output should be a (B, T, window_len, C) tensor that slides over the T dimension\n",
    "    # first window_len-1 elements will have to be padded with zeros in the beginning\n",
    "    # example: x = torch.tensor([[[1,2],[3,4],[5,6],[7,8],[9,10]]])\n",
    "    # sliding_window_fold(x, 2) -> torch.tensor([[[[0,1],[1,2]],[[1,2],[3,4]],[[3,4],[5,6]],[[5,6],[7,8]],[[7,8],[9,10]]]])\n",
    "    padded_x = F.pad(x, (0, 0, window_len - 1, 0), mode='constant', value=0)\n",
    "    output = padded_x.unfold(dimension=1, size=window_len, step=1).permute(0, 1, 3, 2)\n",
    "    return output\n",
    "\n",
    "def attend_folded_all_keys_torch(q, k, states, W):\n",
    "    k = sliding_window_fold(k, W) # (B, T, W, C)\n",
    "    all_keys = torch.cat((states, k), dim=2) # (B, T, S, C) + (B, T, W, C) -> (B, T, S+W, C)\n",
    "    scores = torch.einsum(\"btc, btxc -> btx\", q, all_keys) # (B, T, S+W)\n",
    "    return scores\n",
    "\n",
    "def accumulate_folded_all_values_torch(s, v, states, W):\n",
    "    v = sliding_window_fold(v, W) # (B, T, W, C)\n",
    "    all_values = torch.cat((states, v), dim=2) # (B, T, S, C) + (B, T, W, C) -> (B, T, S+W, C)\n",
    "    out = torch.einsum(\"btx, btxc -> btc\", s, all_values) # (B, T, C)\n",
    "    # out = torch.einsum(\"btx, btxc -> btc\", s[:, :, :S], states) # (B, T, C)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triton kernel\n",
    "@triton.jit\n",
    "def afav_fwd_kernel(\n",
    "    s_ptr, v_ptr, states_ptr, y_ptr,\n",
    "    B: tl.constexpr, T: tl.constexpr, S: tl.constexpr, C: tl.constexpr, W: tl.constexpr,\n",
    "    BLOCK_SIZE_C: tl.constexpr,\n",
    "):\n",
    "    # Use multiple program IDs for better parallelization\n",
    "    b_id = tl.program_id(axis=0)\n",
    "    t_id = tl.program_id(axis=1)\n",
    "    c_block_id = tl.program_id(axis=2)\n",
    "    c_first_id = c_block_id * BLOCK_SIZE_C\n",
    "    SW = S + W\n",
    "\n",
    "    # Compute base pointers\n",
    "    s_base = s_ptr + b_id * T * W\n",
    "    v_base = v_ptr + b_id * T * C\n",
    "    y_base = y_ptr + b_id * T * C\n",
    "\n",
    "    # First we accumulate the values\n",
    "    # Fetch the scores at [b_id, t_id, S:W]\n",
    "    sv_block_ptr = tl.make_block_ptr(\n",
    "        base=s_ptr,\n",
    "        shape=(B, T, SW),\n",
    "        strides=(T * SW, SW, 1),\n",
    "        offsets=(b_id, t_id, S),\n",
    "        block_shape=(1, 1, W),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    sv = tl.load(sv_block_ptr) # (1, 1, W)\n",
    "    # Fetch the value at [b_id, t_id-W+1:t_id, c_first_id:c_first_id+BLOCK_SIZE_C]\n",
    "    # need to load the keys manually because make_block_ptr doesn't support masks\n",
    "    tw_offs = tl.arange(0, W)\n",
    "    c_offs = tl.arange(0, BLOCK_SIZE_C)\n",
    "    v_block_ptr = v_base + (t_id - W + 1 + tw_offs[:, None]) * C + c_first_id + c_offs[None, :]\n",
    "    mask = tl.arange(0, W)[:, None] > (W - t_id - 2)\n",
    "    v = tl.load(v_block_ptr, mask=mask) # (W, BLOCK_SIZE_C) but W can vary <W\n",
    "    # Compute the dot product (but not with tl.dot because it has a minimum size of 16)\n",
    "    # y = sv.permute(0, 2, 1) * v[None, :] # (1, W, BLOCK_SIZE_C)\n",
    "    # y = tl.sum(y, axis=1, keep_dims=True) # (1, 1, BLOCK_SIZE_C)\n",
    "    # turns out keep_dims kinda messes stuff up when later adding the accumulated states\n",
    "\n",
    "    # Then we accumulate the states\n",
    "    # Fetch the scores at [b_id, t_id, :S]\n",
    "    ss_block_ptr = tl.make_block_ptr(\n",
    "        base=s_ptr,\n",
    "        shape=(B, T, SW),\n",
    "        strides=(T * SW, SW, 1),\n",
    "        offsets=(b_id, t_id, 0),\n",
    "        block_shape=(1, 1, S),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    ss = tl.load(ss_block_ptr) # (1, 1, S)\n",
    "    # Fetch the states at [b_id, t_id, c_first_id:c_first_id+BLOCK_SIZE_C]\n",
    "    states_block_ptr = tl.make_block_ptr(\n",
    "        base=states_ptr,\n",
    "        shape=(B, T, S, C),\n",
    "        strides=(T * S * C, S * C, C, 1),\n",
    "        offsets=(b_id, t_id, 0, c_first_id),\n",
    "        block_shape=(1, 1, S, BLOCK_SIZE_C),\n",
    "        order=(0, 1, 2, 3),\n",
    "    )\n",
    "    states = tl.load(states_block_ptr) # (1, 1, S, BLOCK_SIZE_C)\n",
    "    # Compute the dot product\n",
    "    y = tl.sum(sv.permute(0, 2, 1) * v[None, :], axis=1) + tl.sum(ss[:, :, :, None] * states, axis=2).reshape(1, BLOCK_SIZE_C)\n",
    "\n",
    "    # Store the result\n",
    "    y_block_ptr = tl.make_block_ptr(\n",
    "        base=y_ptr,\n",
    "        shape=(B, T, C),\n",
    "        strides=(T * C, C, 1),\n",
    "        offsets=(b_id, t_id, c_first_id),\n",
    "        block_shape=(1, 1, BLOCK_SIZE_C),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    tl.store(y_block_ptr, y[None, :]) # (1, 1, BLOCK_SIZE_C)\n",
    "\n",
    "@triton.jit\n",
    "def afav_bwd_kernel(\n",
    "    s_ptr, v_ptr, states_ptr, dy_ptr, ds_ptr, dv_ptr, dstates_ptr,\n",
    "    B: tl.constexpr, T: tl.constexpr, S:tl.constexpr, C: tl.constexpr, W: tl.constexpr,\n",
    "    BLOCK_SIZE_S: tl.constexpr,\n",
    "    BLOCK_SIZE_W: tl.constexpr,\n",
    "):\n",
    "    # Use multiple program IDs for better parallelization\n",
    "    b_id = tl.program_id(axis=0)\n",
    "    t_id = tl.program_id(axis=1)\n",
    "    sw_block_id = tl.program_id(axis=2)\n",
    "    num_s_blocks = triton.cdiv(S, BLOCK_SIZE_S)\n",
    "    num_w_blocks = triton.cdiv(W, BLOCK_SIZE_W)\n",
    "    is_state = sw_block_id < num_s_blocks\n",
    "    SW = S + W\n",
    "\n",
    "    # Compute base pointers\n",
    "    s_base = s_ptr + b_id * T * C\n",
    "    v_base = v_ptr + b_id * T * C\n",
    "    dy_base = dy_ptr + b_id * T * W\n",
    "    ds_base = ds_ptr + b_id * T * C\n",
    "    dv_base = dv_ptr + b_id * (T+W-1) * C + (W-1) * C # skip the first W-1 elements\n",
    "\n",
    "    if not is_state:\n",
    "        # Here we calculate the gradients for s [:, :, S:W] and for v\n",
    "        w_first_id = (sw_block_id - num_s_blocks) * BLOCK_SIZE_W\n",
    "        # First calculate the gradients for s\n",
    "        # Fetch original output gradients at [b_id, t_id, :]\n",
    "        dy_block_ptr = tl.make_block_ptr(\n",
    "            base=dy_ptr,\n",
    "            shape=(B, T, C),\n",
    "            strides=(T * C, C, 1),\n",
    "            offsets=(b_id, t_id, 0),\n",
    "            block_shape=(1, 1, C),\n",
    "            order=(0, 1, 2),\n",
    "        )\n",
    "        dy = tl.load(dy_block_ptr) # (1, 1, C)\n",
    "        # Then calculate the gradients for v\n",
    "        s_block_ptr = tl.make_block_ptr(\n",
    "            base=s_ptr,\n",
    "            shape=(B, T, SW),\n",
    "            strides=(T * SW, SW, 1),\n",
    "            offsets=(b_id, t_id, S+w_first_id),\n",
    "            block_shape=(1, 1, BLOCK_SIZE_W),\n",
    "            order=(0, 1, 2),\n",
    "        )\n",
    "        s = tl.load(s_block_ptr) # (1, 1, BLOCK_SIZE_W)\n",
    "        # Fetch original values at [b_id, t_id-W+1+(w_block_id*BLOCK_SIZE_W):t_id+(w_block_id*BLOCK_SIZE_W), :]\n",
    "        # using a block ptr also disallows the use of masks when loading, so let's just make a ptr manually\n",
    "        tw_offs = tl.arange(0, BLOCK_SIZE_W)\n",
    "        c_offs = tl.arange(0, C)\n",
    "        v_block_ptr = v_base + (t_id - W + 1 + (w_first_id + tw_offs[:, None])) * C + c_offs[None, :]\n",
    "        mask = w_first_id + tl.arange(0, BLOCK_SIZE_W)[:, None] > (W - t_id - 2)\n",
    "        v = tl.load(v_block_ptr, mask=mask) # (BLOCK_SIZE_W, C)\n",
    "\n",
    "        # We already fetched output gradients dy at [b_id, t_id, :] w/ size (1, 1, C)\n",
    "        # Compute the gradients for v\n",
    "        dv = dy * s.reshape(1, BLOCK_SIZE_W, 1) # (1, BLOCK_SIZE_W, C)\n",
    "\n",
    "        # Compute the gradients for q\n",
    "        dsv = dy * v[None, :] # (1, BLOCK_SIZE_W, C)\n",
    "        dsv = tl.sum(dsv, axis=-1) # (1, BLOCK_SIZE_W)\n",
    "\n",
    "        # Store the result\n",
    "        dsv_block_ptr = tl.make_block_ptr(\n",
    "            base=ds_ptr,\n",
    "            shape=(B, T, SW),\n",
    "            strides=(T * SW, SW, 1),\n",
    "            offsets=(b_id, t_id, S+w_first_id),\n",
    "            block_shape=(1, 1, BLOCK_SIZE_W),\n",
    "            order=(0, 1, 2),\n",
    "        )\n",
    "        tl.store(dsv_block_ptr, dsv[None, :]) # (1, 1, BLOCK_SIZE_W)\n",
    "\n",
    "        # Store the result\n",
    "        # need to make a ptr manually because make_block_ptr doesn't support masks\n",
    "        tw_offs = tl.arange(0, BLOCK_SIZE_W)\n",
    "        c_offs = tl.arange(0, C)\n",
    "        dv_block_ptr = dv_base + (t_id - W + 1 + (w_first_id + tw_offs[:, None])) * C + c_offs[None, :]\n",
    "        mask = w_first_id + tl.arange(0, BLOCK_SIZE_W)[:, None] > (W - t_id - 2)\n",
    "        # now we have to atomically add the gradients to the original values\n",
    "        tl.atomic_add(dv_block_ptr[None, :], dv)\n",
    "    else:\n",
    "        s_first_id = sw_block_id * BLOCK_SIZE_S\n",
    "        # Here we calculate the gradients for s[:, :, :S] and for states\n",
    "        # First calculate the gradients for s\n",
    "        # Fetch states at [b_id, t_id, s_first_id:s_first_id+BLOCK_SIZE_S, :]\n",
    "        states_block_ptr = tl.make_block_ptr(\n",
    "            base=states_ptr,\n",
    "            shape=(B, T, S, C),\n",
    "            strides=(T * S * C, S * C, C, 1),\n",
    "            offsets=(b_id, t_id, s_first_id, 0),\n",
    "            block_shape=(1, 1, BLOCK_SIZE_S, C),\n",
    "            order=(0, 1, 2, 3),\n",
    "        )\n",
    "        states = tl.load(states_block_ptr) # (1, 1, BLOCK_SIZE_S, C)\n",
    "        # Fetch original output gradients at [b_id, t_id, :]\n",
    "        dy_block_ptr = tl.make_block_ptr(\n",
    "            base=dy_ptr,\n",
    "            shape=(B, T, C),\n",
    "            strides=(T * C, C, 1),\n",
    "            offsets=(b_id, t_id, 0),\n",
    "            block_shape=(1, 1, C),\n",
    "            order=(0, 1, 2),\n",
    "        )\n",
    "        dy = tl.load(dy_block_ptr) # (1, 1, C)\n",
    "        # Fetch the scores at [b_id, t_id, :S]\n",
    "        ss_block_ptr = tl.make_block_ptr(\n",
    "            base=s_ptr,\n",
    "            shape=(B, T, SW),\n",
    "            strides=(T * SW, SW, 1),\n",
    "            offsets=(b_id, t_id, s_first_id),\n",
    "            block_shape=(1, 1, BLOCK_SIZE_S),\n",
    "            order=(0, 1, 2),\n",
    "        )\n",
    "        ss = tl.load(ss_block_ptr) # (1, 1, BLOCK_SIZE_S)\n",
    "\n",
    "        # Compute the gradients for s\n",
    "        dss = dy[:, :, None, :] * states # (1, 1, BLOCK_SIZE_S, C)\n",
    "        dss = tl.sum(dss, axis=-1) # (1, 1, BLOCK_SIZE_S)\n",
    "\n",
    "        # Then calculate the gradients for states\n",
    "        dstates = dy[:, :, None, :] * ss[:, :, :, None] # (1, 1, BLOCK_SIZE_S, C)\n",
    "\n",
    "        # Store the result gradients of s at [b_id, t_id, :S]\n",
    "        dss_block_ptr = tl.make_block_ptr(\n",
    "            base=ds_ptr,\n",
    "            shape=(B, T, SW),\n",
    "            strides=(T * SW, SW, 1),\n",
    "            offsets=(b_id, t_id, s_first_id),\n",
    "            block_shape=(1, 1, BLOCK_SIZE_S),\n",
    "            order=(0, 1, 2),\n",
    "        )\n",
    "        tl.store(dss_block_ptr, dss) # (1, 1, BLOCK_SIZE_S)\n",
    "        \n",
    "        # Store the result gradients of states at [b_id, t_id, s_first_id:s_first_id+BLOCK_SIZE_S, :]\n",
    "        dstates_block_ptr = tl.make_block_ptr(\n",
    "            base=dstates_ptr,\n",
    "            shape=(B, T, S, C),\n",
    "            strides=(T * S * C, S * C, C, 1),\n",
    "            offsets=(b_id, t_id, s_first_id, 0),\n",
    "            block_shape=(1, 1, BLOCK_SIZE_S, C),\n",
    "            order=(0, 1, 2, 3),\n",
    "        )\n",
    "        tl.store(dstates_block_ptr, dstates) # (1, 1, BLOCK_SIZE_S, C)\n",
    "\n",
    "class AccumulateFoldedAllValuesTriton(torch.autograd.Function):\n",
    "    # @torch.compiler.disable\n",
    "    @staticmethod\n",
    "    def forward(ctx, s, v, states, W):\n",
    "        B, T, S, C = states.shape\n",
    "        s = s.contiguous()\n",
    "        v = v.contiguous()\n",
    "        states = states.contiguous()\n",
    "        ctx.save_for_backward(s, v, states)\n",
    "        ctx.W = W\n",
    "        \n",
    "        # Configure block sizes\n",
    "        BLOCK_SIZE_C = 16\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        num_c_blocks = triton.cdiv(C, BLOCK_SIZE_C)\n",
    "        grid = (B, T, num_c_blocks)\n",
    "\n",
    "        # Allocate output tensor\n",
    "        y = torch.zeros((B, T, C), dtype=v.dtype, device=v.device).contiguous()\n",
    "        \n",
    "        # Launch kernel\n",
    "        afav_fwd_kernel[grid](\n",
    "            s, v, states, y,\n",
    "            B, T, S, C, W,\n",
    "            BLOCK_SIZE_C=BLOCK_SIZE_C,\n",
    "        )\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # @torch.compiler.disable\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.contiguous()\n",
    "        s, v, states = ctx.saved_tensors\n",
    "        B, T, S, C = states.shape\n",
    "        W = ctx.W\n",
    "\n",
    "        # Configure block sizes\n",
    "        BLOCK_SIZE_S = 16\n",
    "        BLOCK_SIZE_W = 16\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        num_s_blocks = triton.cdiv(S, BLOCK_SIZE_S)\n",
    "        num_w_blocks = triton.cdiv(W, BLOCK_SIZE_W)\n",
    "        grid = (B, T, num_s_blocks+num_w_blocks)\n",
    "        \n",
    "        gs = torch.zeros_like(s).contiguous()\n",
    "        # for gv we want an additional W at the start of the time dimension bc we can't mask atomic add\n",
    "        gv = torch.zeros((B, T+W-1, C), dtype=v.dtype, device=v.device).contiguous()\n",
    "        gst = torch.zeros_like(states).contiguous()\n",
    "\n",
    "        # Launch kernel\n",
    "        afav_bwd_kernel[grid](\n",
    "            s, v, states, grad_output, gs, gv, gst,\n",
    "            B, T, S, C, W,\n",
    "            BLOCK_SIZE_S=BLOCK_SIZE_S,\n",
    "            BLOCK_SIZE_W=BLOCK_SIZE_W,\n",
    "        )\n",
    "\n",
    "        # No need for the additional W at the start of the time dimension for gv\n",
    "        return gs, gv[:, W-1:], gst, None\n",
    "\n",
    "# @torch.compile\n",
    "def accumulate_folded_all_values_triton(s, v, states, W):\n",
    "    return AccumulateFoldedAllValuesTriton.apply(s, v, states, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: tensor([[[-0.30,  0.75,  ..., -1.25, -0.50],\n",
      "         [ 1.73,  0.09,  ...,  0.20,  0.42],\n",
      "         ...,\n",
      "         [ 1.47,  2.17,  ..., -1.25, -0.72],\n",
      "         [ 1.39,  0.26,  ...,  1.53,  0.20]],\n",
      "\n",
      "        [[ 0.17, -0.05,  ..., -0.95, -0.94],\n",
      "         [-0.28,  0.01,  ...,  0.15,  1.08],\n",
      "         ...,\n",
      "         [-0.49,  0.08,  ...,  0.29, -0.04],\n",
      "         [-0.20, -0.01,  ...,  1.21, -0.16]],\n",
      "\n",
      "        [[-0.20, -0.85,  ..., -1.35, -1.11],\n",
      "         [-0.80,  0.44,  ...,  0.04, -0.33],\n",
      "         ...,\n",
      "         [ 1.17, -0.24,  ...,  0.64, -0.57],\n",
      "         [ 0.82,  1.22,  ..., -1.36, -1.62]],\n",
      "\n",
      "        [[-0.41,  0.55,  ...,  0.26,  0.18],\n",
      "         [ 0.02,  1.81,  ..., -1.05, -0.09],\n",
      "         ...,\n",
      "         [ 1.07, -0.09,  ..., -1.32,  0.48],\n",
      "         [-1.75, -0.80,  ...,  0.54, -1.42]]], device='cuda:0')\n",
      "v: tensor([[[ 0.60,  0.40,  ..., -0.43, -0.69],\n",
      "         [-0.38,  0.06,  ...,  0.03, -1.68],\n",
      "         ...,\n",
      "         [-1.12, -1.14,  ..., -0.17,  0.14],\n",
      "         [-0.76,  0.01,  ..., -1.13, -0.78]],\n",
      "\n",
      "        [[ 1.34, -0.83,  ..., -1.67,  0.27],\n",
      "         [-0.21, -0.43,  ..., -0.17,  0.02],\n",
      "         ...,\n",
      "         [-0.36,  0.93,  ..., -0.02,  1.48],\n",
      "         [ 1.13, -0.40,  ..., -0.42, -0.13]],\n",
      "\n",
      "        [[-0.99, -0.02,  ...,  0.11, -0.28],\n",
      "         [-1.38,  0.17,  ..., -0.53,  1.04],\n",
      "         ...,\n",
      "         [ 0.64, -1.10,  ..., -1.05, -0.89],\n",
      "         [-0.63, -0.15,  ..., -0.39, -0.38]],\n",
      "\n",
      "        [[-0.92, -0.57,  ...,  0.08,  0.40],\n",
      "         [-0.73,  0.57,  ..., -0.52, -0.03],\n",
      "         ...,\n",
      "         [-1.60, -0.92,  ..., -1.14,  0.66],\n",
      "         [-0.33, -0.08,  ...,  0.89, -0.14]]], device='cuda:0')\n",
      "states: tensor([[[[-0.86, -1.85,  ...,  0.33, -0.42],\n",
      "          [-0.12,  0.55,  ..., -0.04, -0.53],\n",
      "          ...,\n",
      "          [-1.51, -0.12,  ..., -1.73,  2.11],\n",
      "          [-2.08,  2.06,  ...,  1.48, -0.94]],\n",
      "\n",
      "         [[-1.73,  0.01,  ...,  1.56, -0.89],\n",
      "          [-0.55,  1.12,  ..., -0.06, -0.80],\n",
      "          ...,\n",
      "          [ 2.02, -0.17,  ..., -1.31, -0.30],\n",
      "          [ 0.29, -0.06,  ...,  0.54, -0.39]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.03, -0.13,  ...,  0.67, -1.53],\n",
      "          [ 1.63, -0.71,  ...,  1.02,  1.48],\n",
      "          ...,\n",
      "          [-0.38, -0.82,  ..., -0.01, -0.83],\n",
      "          [-1.28, -1.80,  ...,  1.10,  1.03]],\n",
      "\n",
      "         [[-0.71, -0.27,  ..., -1.11,  0.36],\n",
      "          [-0.21, -0.22,  ..., -0.21,  0.94],\n",
      "          ...,\n",
      "          [ 0.91, -0.43,  ...,  0.03,  0.04],\n",
      "          [-0.80,  1.61,  ...,  0.98,  0.85]]],\n",
      "\n",
      "\n",
      "        [[[ 0.93, -0.36,  ...,  0.86,  1.09],\n",
      "          [-1.52,  1.39,  ...,  0.67,  0.44],\n",
      "          ...,\n",
      "          [-1.69,  0.32,  ...,  1.39,  0.78],\n",
      "          [ 0.67,  0.17,  ..., -0.93, -0.20]],\n",
      "\n",
      "         [[ 0.63,  0.41,  ...,  0.11, -0.89],\n",
      "          [ 0.25, -0.27,  ..., -1.47, -0.37],\n",
      "          ...,\n",
      "          [ 1.04,  0.56,  ...,  0.93, -0.06],\n",
      "          [-0.02,  0.87,  ..., -1.48, -1.24]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.54,  0.86,  ..., -0.00,  0.22],\n",
      "          [ 0.10,  0.02,  ..., -0.26,  1.59],\n",
      "          ...,\n",
      "          [ 0.03,  2.02,  ..., -1.27, -1.65],\n",
      "          [-1.54,  0.93,  ..., -0.63, -0.37]],\n",
      "\n",
      "         [[ 1.55,  1.84,  ...,  0.33, -0.16],\n",
      "          [-0.09, -0.12,  ..., -1.22, -0.32],\n",
      "          ...,\n",
      "          [ 0.20,  1.90,  ...,  0.45, -0.50],\n",
      "          [-0.56,  0.18,  ..., -0.25, -0.15]]],\n",
      "\n",
      "\n",
      "        [[[ 0.75,  0.01,  ...,  1.57,  0.13],\n",
      "          [-0.78,  0.14,  ...,  0.61, -0.80],\n",
      "          ...,\n",
      "          [ 0.50,  0.27,  ...,  0.92, -1.03],\n",
      "          [-2.07, -0.34,  ..., -1.10,  0.06]],\n",
      "\n",
      "         [[ 0.43, -0.45,  ..., -0.66,  0.40],\n",
      "          [-0.40, -0.05,  ...,  1.66,  0.44],\n",
      "          ...,\n",
      "          [ 1.10, -0.87,  ..., -0.61,  0.53],\n",
      "          [-0.03, -1.26,  ..., -1.18,  0.61]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.33,  1.35,  ...,  0.35, -1.24],\n",
      "          [ 0.42, -1.45,  ..., -0.61, -2.72],\n",
      "          ...,\n",
      "          [ 0.96, -0.86,  ...,  0.45, -0.77],\n",
      "          [ 1.23, -0.90,  ...,  0.35,  1.42]],\n",
      "\n",
      "         [[-1.25,  1.49,  ...,  0.27, -0.86],\n",
      "          [ 1.12, -0.38,  ..., -0.04,  1.22],\n",
      "          ...,\n",
      "          [ 2.49,  0.68,  ..., -0.47, -0.76],\n",
      "          [ 1.84,  2.22,  ...,  0.61,  1.49]]],\n",
      "\n",
      "\n",
      "        [[[-0.03, -0.69,  ...,  1.20,  2.42],\n",
      "          [ 0.90, -0.21,  ..., -1.28, -0.45],\n",
      "          ...,\n",
      "          [-0.87,  0.00,  ...,  1.22,  1.57],\n",
      "          [ 0.67,  0.53,  ...,  0.65, -1.05]],\n",
      "\n",
      "         [[ 0.33, -0.07,  ...,  0.13, -0.52],\n",
      "          [ 0.85,  0.30,  ..., -1.09,  0.76],\n",
      "          ...,\n",
      "          [-1.10, -1.08,  ...,  0.78,  1.05],\n",
      "          [-0.34,  1.50,  ...,  0.77, -0.25]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.14,  0.27,  ..., -0.37, -0.31],\n",
      "          [ 0.70,  0.90,  ...,  0.03, -0.07],\n",
      "          ...,\n",
      "          [ 0.29,  1.48,  ..., -1.13, -0.86],\n",
      "          [-0.32, -3.03,  ...,  0.26, -0.19]],\n",
      "\n",
      "         [[ 0.77,  0.99,  ..., -0.14,  0.04],\n",
      "          [-1.17, -0.90,  ...,  1.19, -0.10],\n",
      "          ...,\n",
      "          [ 1.16,  1.22,  ...,  2.03, -0.56],\n",
      "          [ 0.10, -1.21,  ..., -0.45, -0.50]]]], device='cuda:0')\n",
      "torch:\n",
      "tensor([[[  9.63,  -0.38,  ...,  -4.82,   3.30],\n",
      "         [  1.10,  -1.13,  ...,   0.60,  -0.66],\n",
      "         ...,\n",
      "         [ -7.70,  -6.05,  ...,   7.67,   8.68],\n",
      "         [ -1.82,   2.91,  ...,  -0.05,  -1.11]],\n",
      "\n",
      "        [[  1.68,   7.99,  ...,   1.09,   5.43],\n",
      "         [ -9.70,   3.90,  ...,  -8.75,   5.67],\n",
      "         ...,\n",
      "         [-13.26,   0.17,  ...,   2.06, -10.14],\n",
      "         [  2.81,   0.37,  ...,  -3.89,  -8.44]],\n",
      "\n",
      "        [[  1.31,   2.03,  ...,  -1.25,   5.19],\n",
      "         [-10.31,  -1.75,  ...,  -7.42,   1.85],\n",
      "         ...,\n",
      "         [ -1.68,  -6.62,  ...,  -1.72,  -0.66],\n",
      "         [ -1.33,  -7.61,  ...,  12.23,  -5.07]],\n",
      "\n",
      "        [[ -7.81,  -7.98,  ..., -14.41,   2.97],\n",
      "         [  7.26,   0.52,  ...,  -1.01,   6.83],\n",
      "         ...,\n",
      "         [  2.07,  -0.88,  ...,   4.59,  -6.43],\n",
      "         [-10.42,   3.69,  ...,   7.41,  -3.48]]], device='cuda:0')\n",
      "triton:\n",
      "tensor([[[  9.63,  -0.38,  ...,  -4.82,   3.30],\n",
      "         [  1.10,  -1.13,  ...,   0.60,  -0.66],\n",
      "         ...,\n",
      "         [ -7.70,  -6.05,  ...,   7.67,   8.68],\n",
      "         [ -1.82,   2.91,  ...,  -0.05,  -1.11]],\n",
      "\n",
      "        [[  1.68,   7.99,  ...,   1.09,   5.43],\n",
      "         [ -9.70,   3.90,  ...,  -8.75,   5.67],\n",
      "         ...,\n",
      "         [-13.26,   0.17,  ...,   2.06, -10.14],\n",
      "         [  2.81,   0.37,  ...,  -3.89,  -8.44]],\n",
      "\n",
      "        [[  1.31,   2.03,  ...,  -1.25,   5.19],\n",
      "         [-10.31,  -1.75,  ...,  -7.42,   1.85],\n",
      "         ...,\n",
      "         [ -1.68,  -6.62,  ...,  -1.72,  -0.66],\n",
      "         [ -1.33,  -7.61,  ...,  12.23,  -5.07]],\n",
      "\n",
      "        [[ -7.81,  -7.98,  ..., -14.41,   2.97],\n",
      "         [  7.26,   0.52,  ...,  -1.01,   6.83],\n",
      "         ...,\n",
      "         [  2.07,  -0.88,  ...,   4.59,  -6.43],\n",
      "         [-10.42,   3.69,  ...,   7.41,  -3.48]]], device='cuda:0')\n",
      "max diff: tensor(    0.00, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# try the forward pass\n",
    "# test accumulate_folded_values_torch with always the same random inputs\n",
    "B, T, S, W, C = 4, 128, 32, 32, 32\n",
    "s = torch.randn(B, T, S+W, device=device)\n",
    "print(\"s:\", s)\n",
    "v = torch.randn(B, T, C, device=device)\n",
    "print(\"v:\", v)\n",
    "states = torch.randn(B, T, S, C, device=device)\n",
    "print(\"states:\", states)\n",
    "print(\"torch:\")\n",
    "out = accumulate_folded_all_values_torch(s, v, states, W)\n",
    "print(out)\n",
    "print(\"triton:\")\n",
    "out_triton = accumulate_folded_all_values_triton(s, v, states, W)\n",
    "print(out_triton)\n",
    "print(\"max diff:\", (out - out_triton).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_output: tensor([[[-0.13, -0.45,  ..., -0.71,  0.66],\n",
      "         [ 0.34, -0.31,  ..., -0.21,  1.93],\n",
      "         ...,\n",
      "         [-1.69, -1.92,  ...,  0.15,  0.98],\n",
      "         [ 0.99, -0.35,  ..., -1.08,  0.41]],\n",
      "\n",
      "        [[-0.14,  0.77,  ..., -1.92,  0.13],\n",
      "         [ 0.71,  1.61,  ..., -0.32,  0.69],\n",
      "         ...,\n",
      "         [ 0.97,  0.39,  ..., -0.45, -1.76],\n",
      "         [-0.77,  1.04,  ..., -0.49, -0.06]],\n",
      "\n",
      "        [[ 2.93,  0.57,  ...,  1.55, -0.04],\n",
      "         [ 1.65, -0.23,  ...,  1.21,  0.32],\n",
      "         ...,\n",
      "         [ 1.44, -0.13,  ..., -1.37, -0.72],\n",
      "         [-0.41,  1.63,  ..., -2.06,  1.98]],\n",
      "\n",
      "        [[ 0.60,  1.20,  ...,  0.53, -3.35],\n",
      "         [-0.58,  1.06,  ...,  0.08, -0.78],\n",
      "         ...,\n",
      "         [-1.03, -0.58,  ...,  0.84, -0.32],\n",
      "         [ 0.56, -0.13,  ...,  1.25,  0.29]]], device='cuda:0')\n",
      "torch:\n",
      "s grad: tensor([[[  6.06,   7.29,  ...,   0.00,  -1.70],\n",
      "         [ -3.40,  -0.53,  ...,  -0.59,  -5.91],\n",
      "         ...,\n",
      "         [ -1.69,   1.42,  ...,  14.32,   8.19],\n",
      "         [  4.93,   0.14,  ...,  -4.99,  -0.35]],\n",
      "\n",
      "        [[ 10.60,  -6.61,  ...,   0.00,   0.46],\n",
      "         [ -3.76,   2.72,  ...,  -0.78,  -1.00],\n",
      "         ...,\n",
      "         [  0.38,  -2.98,  ...,  -4.33,  -1.67],\n",
      "         [  2.38,   1.60,  ...,  -6.53,  -2.42]],\n",
      "\n",
      "        [[-10.88, -10.00,  ...,   0.00,  -1.78],\n",
      "         [  7.19,  -5.32,  ...,  -1.03,  -6.55],\n",
      "         ...,\n",
      "         [  0.46,  -2.86,  ...,  -0.46,  -0.06],\n",
      "         [  0.67,  -4.12,  ...,  -3.31,   4.02]],\n",
      "\n",
      "        [[ -8.56,  -4.26,  ...,   0.00, -11.66],\n",
      "         [  1.38,   0.32,  ...,   7.86,  -1.52],\n",
      "         ...,\n",
      "         [ -1.58,  -0.98,  ...,  -3.40,   6.76],\n",
      "         [  9.79,   4.70,  ...,  -6.73,   1.54]]], device='cuda:0')\n",
      "v grad: tensor([[[    -1.60,     -9.84,  ...,      0.47,      3.24],\n",
      "         [    -1.17,     -0.79,  ...,     -0.70,     -1.19],\n",
      "         ...,\n",
      "         [     2.74,      0.85,  ...,     -1.77,     -0.07],\n",
      "         [     0.20,     -0.07,  ...,     -0.21,      0.08]],\n",
      "\n",
      "        [[     4.16,      2.72,  ...,     -8.98,      1.77],\n",
      "         [     2.39,      4.49,  ...,     -9.84,     -5.47],\n",
      "         ...,\n",
      "         [    -0.96,      1.24,  ...,     -0.57,     -0.02],\n",
      "         [     0.12,     -0.16,  ...,      0.08,      0.01]],\n",
      "\n",
      "        [[    -4.26,     -1.27,  ...,     -5.68,      0.69],\n",
      "         [    -3.28,     -8.38,  ...,     -4.96,     -2.26],\n",
      "         ...,\n",
      "         [    -0.26,     -2.14,  ...,      3.59,     -2.29],\n",
      "         [     0.66,     -2.64,  ...,      3.34,     -3.21]],\n",
      "\n",
      "        [[     1.46,     -4.78,  ...,     -5.51,     -2.54],\n",
      "         [     2.03,     -7.20,  ...,      6.63,      6.99],\n",
      "         ...,\n",
      "         [    -0.19,     -0.34,  ...,      1.07,      0.00],\n",
      "         [    -0.79,      0.18,  ...,     -1.78,     -0.41]]], device='cuda:0')\n",
      "states grad: tensor([[[[     0.04,      0.13,  ...,      0.21,     -0.19],\n",
      "          [    -0.10,     -0.34,  ...,     -0.54,      0.50],\n",
      "          ...,\n",
      "          [     0.18,      0.61,  ...,      0.96,     -0.89],\n",
      "          [     0.12,      0.41,  ...,      0.65,     -0.61]],\n",
      "\n",
      "         [[     0.59,     -0.53,  ...,     -0.36,      3.33],\n",
      "          [     0.03,     -0.03,  ...,     -0.02,      0.17],\n",
      "          ...,\n",
      "          [     0.22,     -0.19,  ...,     -0.13,      1.21],\n",
      "          [    -0.20,      0.18,  ...,      0.12,     -1.15]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -2.48,     -2.83,  ...,      0.22,      1.44],\n",
      "          [    -3.66,     -4.17,  ...,      0.32,      2.12],\n",
      "          ...,\n",
      "          [     1.27,      1.45,  ...,     -0.11,     -0.74],\n",
      "          [    -2.56,     -2.92,  ...,      0.22,      1.49]],\n",
      "\n",
      "         [[     1.38,     -0.49,  ...,     -1.50,      0.58],\n",
      "          [     0.26,     -0.09,  ...,     -0.28,      0.11],\n",
      "          ...,\n",
      "          [    -1.10,      0.39,  ...,      1.20,     -0.46],\n",
      "          [    -0.11,      0.04,  ...,      0.12,     -0.05]]],\n",
      "\n",
      "\n",
      "        [[[    -0.02,      0.13,  ...,     -0.33,      0.02],\n",
      "          [     0.01,     -0.04,  ...,      0.09,     -0.01],\n",
      "          ...,\n",
      "          [    -0.08,      0.42,  ...,     -1.06,      0.07],\n",
      "          [     0.06,     -0.32,  ...,      0.81,     -0.06]],\n",
      "\n",
      "         [[    -0.20,     -0.45,  ...,      0.09,     -0.19],\n",
      "          [     0.01,      0.01,  ...,     -0.00,      0.01],\n",
      "          ...,\n",
      "          [     0.17,      0.38,  ...,     -0.08,      0.16],\n",
      "          [     1.46,      3.29,  ...,     -0.66,      1.41]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -0.48,     -0.19,  ...,      0.22,      0.87],\n",
      "          [     0.08,      0.03,  ...,     -0.04,     -0.15],\n",
      "          ...,\n",
      "          [     0.78,      0.32,  ...,     -0.37,     -1.42],\n",
      "          [    -1.76,     -0.71,  ...,      0.82,      3.19]],\n",
      "\n",
      "         [[     0.16,     -0.21,  ...,      0.10,      0.01],\n",
      "          [     0.01,     -0.01,  ...,      0.00,      0.00],\n",
      "          ...,\n",
      "          [    -0.80,      1.08,  ...,     -0.51,     -0.07],\n",
      "          [     1.08,     -1.46,  ...,      0.69,      0.09]]],\n",
      "\n",
      "\n",
      "        [[[    -0.58,     -0.11,  ...,     -0.30,      0.01],\n",
      "          [    -2.48,     -0.48,  ...,     -1.31,      0.03],\n",
      "          ...,\n",
      "          [    -4.51,     -0.87,  ...,     -2.38,      0.06],\n",
      "          [    -1.23,     -0.24,  ...,     -0.65,      0.02]],\n",
      "\n",
      "         [[    -1.32,      0.18,  ...,     -0.97,     -0.25],\n",
      "          [     0.73,     -0.10,  ...,      0.54,      0.14],\n",
      "          ...,\n",
      "          [     1.03,     -0.14,  ...,      0.75,      0.20],\n",
      "          [     3.28,     -0.45,  ...,      2.41,      0.63]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     1.68,     -0.16,  ...,     -1.60,     -0.84],\n",
      "          [    -0.35,      0.03,  ...,      0.33,      0.17],\n",
      "          ...,\n",
      "          [     1.34,     -0.12,  ...,     -1.28,     -0.67],\n",
      "          [    -0.84,      0.08,  ...,      0.80,      0.42]],\n",
      "\n",
      "         [[    -0.33,      1.33,  ...,     -1.68,      1.62],\n",
      "          [    -0.50,      1.99,  ...,     -2.52,      2.42],\n",
      "          ...,\n",
      "          [    -0.05,      0.19,  ...,     -0.24,      0.23],\n",
      "          [     0.15,     -0.61,  ...,      0.77,     -0.74]]],\n",
      "\n",
      "\n",
      "        [[[    -0.25,     -0.49,  ...,     -0.22,      1.37],\n",
      "          [     0.33,      0.66,  ...,      0.29,     -1.83],\n",
      "          ...,\n",
      "          [     0.19,      0.39,  ...,      0.17,     -1.08],\n",
      "          [    -0.50,     -1.00,  ...,     -0.44,      2.80]],\n",
      "\n",
      "         [[    -0.01,      0.02,  ...,      0.00,     -0.01],\n",
      "          [    -1.05,      1.92,  ...,      0.15,     -1.41],\n",
      "          ...,\n",
      "          [    -0.31,      0.57,  ...,      0.04,     -0.42],\n",
      "          [    -0.12,      0.22,  ...,      0.02,     -0.16]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -1.10,     -0.61,  ...,      0.89,     -0.34],\n",
      "          [     0.10,      0.05,  ...,     -0.08,      0.03],\n",
      "          ...,\n",
      "          [     0.18,      0.10,  ...,     -0.15,      0.06],\n",
      "          [     0.11,      0.06,  ...,     -0.09,      0.04]],\n",
      "\n",
      "         [[    -0.97,      0.22,  ...,     -2.19,     -0.51],\n",
      "          [    -0.44,      0.10,  ...,     -1.00,     -0.23],\n",
      "          ...,\n",
      "          [     0.46,     -0.10,  ...,      1.03,      0.24],\n",
      "          [     0.75,     -0.17,  ...,      1.70,      0.40]]]], device='cuda:0')\n",
      "triton:\n",
      "s grad: tensor([[[  6.06,   7.29,  ...,   0.00,  -1.70],\n",
      "         [ -3.40,  -0.53,  ...,  -0.59,  -5.91],\n",
      "         ...,\n",
      "         [ -1.69,   1.42,  ...,  14.32,   8.19],\n",
      "         [  4.93,   0.14,  ...,  -4.99,  -0.35]],\n",
      "\n",
      "        [[ 10.60,  -6.61,  ...,   0.00,   0.46],\n",
      "         [ -3.76,   2.72,  ...,  -0.78,  -1.00],\n",
      "         ...,\n",
      "         [  0.38,  -2.98,  ...,  -4.33,  -1.67],\n",
      "         [  2.38,   1.60,  ...,  -6.53,  -2.42]],\n",
      "\n",
      "        [[-10.88, -10.00,  ...,   0.00,  -1.78],\n",
      "         [  7.19,  -5.32,  ...,  -1.03,  -6.55],\n",
      "         ...,\n",
      "         [  0.46,  -2.86,  ...,  -0.46,  -0.06],\n",
      "         [  0.67,  -4.12,  ...,  -3.31,   4.02]],\n",
      "\n",
      "        [[ -8.56,  -4.26,  ...,   0.00, -11.66],\n",
      "         [  1.38,   0.32,  ...,   7.86,  -1.52],\n",
      "         ...,\n",
      "         [ -1.58,  -0.98,  ...,  -3.40,   6.76],\n",
      "         [  9.79,   4.70,  ...,  -6.73,   1.54]]], device='cuda:0')\n",
      "v grad: tensor([[[    -1.60,     -9.84,  ...,      0.47,      3.24],\n",
      "         [    -1.17,     -0.79,  ...,     -0.70,     -1.19],\n",
      "         ...,\n",
      "         [     2.74,      0.85,  ...,     -1.77,     -0.07],\n",
      "         [     0.20,     -0.07,  ...,     -0.21,      0.08]],\n",
      "\n",
      "        [[     4.16,      2.72,  ...,     -8.98,      1.77],\n",
      "         [     2.39,      4.49,  ...,     -9.84,     -5.47],\n",
      "         ...,\n",
      "         [    -0.96,      1.24,  ...,     -0.57,     -0.02],\n",
      "         [     0.12,     -0.16,  ...,      0.08,      0.01]],\n",
      "\n",
      "        [[    -4.26,     -1.27,  ...,     -5.68,      0.69],\n",
      "         [    -3.28,     -8.38,  ...,     -4.96,     -2.26],\n",
      "         ...,\n",
      "         [    -0.26,     -2.14,  ...,      3.59,     -2.29],\n",
      "         [     0.66,     -2.64,  ...,      3.34,     -3.21]],\n",
      "\n",
      "        [[     1.46,     -4.78,  ...,     -5.51,     -2.54],\n",
      "         [     2.03,     -7.20,  ...,      6.63,      6.99],\n",
      "         ...,\n",
      "         [    -0.19,     -0.34,  ...,      1.07,      0.00],\n",
      "         [    -0.79,      0.18,  ...,     -1.78,     -0.41]]], device='cuda:0')\n",
      "states grad: tensor([[[[     0.04,      0.13,  ...,      0.21,     -0.19],\n",
      "          [    -0.10,     -0.34,  ...,     -0.54,      0.50],\n",
      "          ...,\n",
      "          [     0.18,      0.61,  ...,      0.96,     -0.89],\n",
      "          [     0.12,      0.41,  ...,      0.65,     -0.61]],\n",
      "\n",
      "         [[     0.59,     -0.53,  ...,     -0.36,      3.33],\n",
      "          [     0.03,     -0.03,  ...,     -0.02,      0.17],\n",
      "          ...,\n",
      "          [     0.22,     -0.19,  ...,     -0.13,      1.21],\n",
      "          [    -0.20,      0.18,  ...,      0.12,     -1.15]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -2.48,     -2.83,  ...,      0.22,      1.44],\n",
      "          [    -3.66,     -4.17,  ...,      0.32,      2.12],\n",
      "          ...,\n",
      "          [     1.27,      1.45,  ...,     -0.11,     -0.74],\n",
      "          [    -2.56,     -2.92,  ...,      0.22,      1.49]],\n",
      "\n",
      "         [[     1.38,     -0.49,  ...,     -1.50,      0.58],\n",
      "          [     0.26,     -0.09,  ...,     -0.28,      0.11],\n",
      "          ...,\n",
      "          [    -1.10,      0.39,  ...,      1.20,     -0.46],\n",
      "          [    -0.11,      0.04,  ...,      0.12,     -0.05]]],\n",
      "\n",
      "\n",
      "        [[[    -0.02,      0.13,  ...,     -0.33,      0.02],\n",
      "          [     0.01,     -0.04,  ...,      0.09,     -0.01],\n",
      "          ...,\n",
      "          [    -0.08,      0.42,  ...,     -1.06,      0.07],\n",
      "          [     0.06,     -0.32,  ...,      0.81,     -0.06]],\n",
      "\n",
      "         [[    -0.20,     -0.45,  ...,      0.09,     -0.19],\n",
      "          [     0.01,      0.01,  ...,     -0.00,      0.01],\n",
      "          ...,\n",
      "          [     0.17,      0.38,  ...,     -0.08,      0.16],\n",
      "          [     1.46,      3.29,  ...,     -0.66,      1.41]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -0.48,     -0.19,  ...,      0.22,      0.87],\n",
      "          [     0.08,      0.03,  ...,     -0.04,     -0.15],\n",
      "          ...,\n",
      "          [     0.78,      0.32,  ...,     -0.37,     -1.42],\n",
      "          [    -1.76,     -0.71,  ...,      0.82,      3.19]],\n",
      "\n",
      "         [[     0.16,     -0.21,  ...,      0.10,      0.01],\n",
      "          [     0.01,     -0.01,  ...,      0.00,      0.00],\n",
      "          ...,\n",
      "          [    -0.80,      1.08,  ...,     -0.51,     -0.07],\n",
      "          [     1.08,     -1.46,  ...,      0.69,      0.09]]],\n",
      "\n",
      "\n",
      "        [[[    -0.58,     -0.11,  ...,     -0.30,      0.01],\n",
      "          [    -2.48,     -0.48,  ...,     -1.31,      0.03],\n",
      "          ...,\n",
      "          [    -4.51,     -0.87,  ...,     -2.38,      0.06],\n",
      "          [    -1.23,     -0.24,  ...,     -0.65,      0.02]],\n",
      "\n",
      "         [[    -1.32,      0.18,  ...,     -0.97,     -0.25],\n",
      "          [     0.73,     -0.10,  ...,      0.54,      0.14],\n",
      "          ...,\n",
      "          [     1.03,     -0.14,  ...,      0.75,      0.20],\n",
      "          [     3.28,     -0.45,  ...,      2.41,      0.63]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     1.68,     -0.16,  ...,     -1.60,     -0.84],\n",
      "          [    -0.35,      0.03,  ...,      0.33,      0.17],\n",
      "          ...,\n",
      "          [     1.34,     -0.12,  ...,     -1.28,     -0.67],\n",
      "          [    -0.84,      0.08,  ...,      0.80,      0.42]],\n",
      "\n",
      "         [[    -0.33,      1.33,  ...,     -1.68,      1.62],\n",
      "          [    -0.50,      1.99,  ...,     -2.52,      2.42],\n",
      "          ...,\n",
      "          [    -0.05,      0.19,  ...,     -0.24,      0.23],\n",
      "          [     0.15,     -0.61,  ...,      0.77,     -0.74]]],\n",
      "\n",
      "\n",
      "        [[[    -0.25,     -0.49,  ...,     -0.22,      1.37],\n",
      "          [     0.33,      0.66,  ...,      0.29,     -1.83],\n",
      "          ...,\n",
      "          [     0.19,      0.39,  ...,      0.17,     -1.08],\n",
      "          [    -0.50,     -1.00,  ...,     -0.44,      2.80]],\n",
      "\n",
      "         [[    -0.01,      0.02,  ...,      0.00,     -0.01],\n",
      "          [    -1.05,      1.92,  ...,      0.15,     -1.41],\n",
      "          ...,\n",
      "          [    -0.31,      0.57,  ...,      0.04,     -0.42],\n",
      "          [    -0.12,      0.22,  ...,      0.02,     -0.16]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -1.10,     -0.61,  ...,      0.89,     -0.34],\n",
      "          [     0.10,      0.05,  ...,     -0.08,      0.03],\n",
      "          ...,\n",
      "          [     0.18,      0.10,  ...,     -0.15,      0.06],\n",
      "          [     0.11,      0.06,  ...,     -0.09,      0.04]],\n",
      "\n",
      "         [[    -0.97,      0.22,  ...,     -2.19,     -0.51],\n",
      "          [    -0.44,      0.10,  ...,     -1.00,     -0.23],\n",
      "          ...,\n",
      "          [     0.46,     -0.10,  ...,      1.03,      0.24],\n",
      "          [     0.75,     -0.17,  ...,      1.70,      0.40]]]], device='cuda:0')\n",
      "max diff s: tensor(    0.00, device='cuda:0')\n",
      "max diff v: tensor(    0.00, device='cuda:0')\n",
      "max diff states: tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# try the backward pass, compare with torch autograd\n",
    "# test accumulate_folded_values_torch with always the same random inputs\n",
    "grad_output = torch.randn(B, T, C, device=device)\n",
    "print(\"grad_output:\", grad_output)\n",
    "print(\"torch:\")\n",
    "s.requires_grad = True\n",
    "v.requires_grad = True\n",
    "states.requires_grad = True\n",
    "s.grad = None\n",
    "v.grad = None\n",
    "states.grad = None\n",
    "out = accumulate_folded_all_values_torch(s, v, states, W)\n",
    "out.backward(grad_output)\n",
    "torch_s_grad = s.grad.clone()\n",
    "torch_v_grad = v.grad.clone()\n",
    "torch_states_grad = states.grad.clone()\n",
    "print('s grad:', torch_s_grad)\n",
    "print('v grad:', torch_v_grad)\n",
    "print('states grad:', torch_states_grad)\n",
    "# reset gradients\n",
    "s.grad = None\n",
    "v.grad = None\n",
    "states.grad = None\n",
    "print(\"triton:\")\n",
    "out_triton = accumulate_folded_all_values_triton(s, v, states, W)\n",
    "out_triton.backward(grad_output)\n",
    "triton_s_grad = s.grad.clone()\n",
    "triton_v_grad = v.grad.clone()\n",
    "triton_states_grad = states.grad.clone()\n",
    "print('s grad:', triton_s_grad)\n",
    "print('v grad:', triton_v_grad)\n",
    "print('states grad:', triton_states_grad)\n",
    "print(\"max diff s:\", (torch_s_grad - triton_s_grad).abs().max())\n",
    "print(\"max diff v:\", (torch_v_grad - triton_v_grad).abs().max())\n",
    "print(\"max diff states:\", (torch_states_grad - triton_states_grad).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch forward:\n",
      "23.1 ms Â± 109 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n",
      "torch forward & backward:\n",
      "61.3 ms Â± 6.41 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "triton forward:\n",
      "5.78 ms Â± 237 Î¼s per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n",
      "triton forward & backward:\n",
      "24.4 ms Â± 626 Î¼s per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# measure the speedup\n",
    "B, T, W, S, C = 128, 1024, 32, 32, 64\n",
    "s = torch.randn(B, T, S+W, device=device)\n",
    "v = torch.randn(B, T, C, device=device)\n",
    "states = torch.randn(B, T, S, C, device=device)\n",
    "grad_output = torch.randn(B, T, C, device=device)\n",
    "# warmup the triton kernel\n",
    "s.requires_grad = True\n",
    "v.requires_grad = True\n",
    "states.requires_grad = True\n",
    "out = accumulate_folded_all_values_triton(s, v, states, W)\n",
    "out.backward(grad_output)\n",
    "s.grad = None\n",
    "v.grad = None\n",
    "\n",
    "print(\"torch forward:\")\n",
    "s.requires_grad = True\n",
    "v.requires_grad = True\n",
    "states.requires_grad = True\n",
    "%timeit accumulate_folded_all_values_torch(s, v, states, W)\n",
    "print(\"torch forward & backward:\")\n",
    "def ftorch():\n",
    "    s.grad = None\n",
    "    v.grad = None\n",
    "    out_torch = accumulate_folded_all_values_torch(s, v, states, W)\n",
    "    out_torch.backward(grad_output)\n",
    "%timeit ftorch()\n",
    "print(\"\\ntriton forward:\")\n",
    "s.requires_grad = True\n",
    "v.requires_grad = True\n",
    "states.requires_grad = True\n",
    "%timeit accumulate_folded_all_values_triton(s, v, states, W)\n",
    "print(\"triton forward & backward:\")\n",
    "def ftriton():\n",
    "    s.grad = None\n",
    "    v.grad = None\n",
    "    out_triton = accumulate_folded_all_values_triton(s, v, states, W)\n",
    "    out_triton.backward(grad_output)\n",
    "%timeit ftriton()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Gating Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no reverse cumprod in pytorch :(\n",
    "def reverse_cumprod(x, dim=-1):\n",
    "    # cp = torch.cumprod(x, dim=dim)\n",
    "    # i = [slice(None)] * x.dim()\n",
    "    # i[dim] = -1\n",
    "    # return (x / (cp + 1e-8)) * cp[i].unsqueeze(dim)\n",
    "    return torch.flip(torch.cumprod(torch.flip(x, [dim]), dim), [dim])\n",
    "\n",
    "# this is for stator and integrator in SCAN\n",
    "# @torch.compile\n",
    "def cum_gating_2d_torch(x, g):\n",
    "    # x is (B, T, C) and g is (B, T, S), return (B, T, S, C)\n",
    "    # the parallel cumulative version of the recursion o[i] = o[i-1](1-g[i])^T + x[i]g[i]^T\n",
    "    inv_g = 1 - g\n",
    "    r_inv_g = reverse_cumprod(inv_g, dim=1)\n",
    "    r_inv_g = torch.cat((r_inv_g[:, 1:], torch.ones_like(r_inv_g[:, -1:])), dim=1)\n",
    "    g_r_inv_g = g * r_inv_g\n",
    "    x_g_r_inv_g = torch.einsum('bts,btc->btsc', g_r_inv_g, x)\n",
    "    c_x_g_r_inv_g = torch.cumsum(x_g_r_inv_g, dim=1)\n",
    "    return c_x_g_r_inv_g / (r_inv_g.unsqueeze(-1) + 1e-8) # avoid division by zero\n",
    "\n",
    "def g_sliding_window_fold(x, window_len):\n",
    "    # x is a (B, T, C) tensor\n",
    "    # output should be a (B, T, window_len, C) tensor that slides over the T dimension\n",
    "    padded_x = F.pad(x, (0, 0, 0, window_len - 1), mode='constant', value=1)\n",
    "    output = padded_x.unfold(dimension=1, size=window_len, step=1).permute(0, 1, 3, 2)\n",
    "    return output\n",
    "\n",
    "# for debugging, now a naive version\n",
    "def cum_gating_2d_naive(x, g, gi=None):\n",
    "    # x is (B, T, C) and g is (B, T, S), return (B, T, S, C)\n",
    "    # the naive iterative version of the recursion o[i] = o[i-1](1-g[i])^T + x[i]g[i]^T\n",
    "    B, T, C = x.shape\n",
    "    S = g.shape[-1]\n",
    "    x_g = torch.einsum('bts,btc->btsc', g, x)\n",
    "    \n",
    "    out_list = [x_g[:, 0].unsqueeze(1)]  # Start with the first time step\n",
    "    complement_g = 1 - g if gi is None else gi\n",
    "    \n",
    "    for i in range(1, T):\n",
    "        prev_out = out_list[-1][:, -1]  # Get the last time step from previous output\n",
    "        current_out = torch.einsum('bsc,bs->bsc', prev_out, complement_g[:, i]) + x_g[:, i]\n",
    "        out_list.append(current_out.unsqueeze(1))\n",
    "    \n",
    "    return torch.cat(out_list, dim=1)\n",
    "\n",
    "# wait a second... we can do all that CUDA/triton multistage accumulation stuff in pytorch.. right??\n",
    "# @torch.compile\n",
    "# def cum_gating_2d_torch_multistage(x, g):\n",
    "#     xg = torch.einsum('bts,btc->btsc', g, x)\n",
    "#     gi = 1 - g\n",
    "#     B, T, S, C = xg.shape\n",
    "#     halfT = T // 2\n",
    "#     nstages = math.ceil(math.log2(T))\n",
    "#     for stage in range(nstages):\n",
    "#         group_stride = 1 << stage\n",
    "#         initial_indices = group_stride + ((torch.arange(halfT) // group_stride) * group_stride * 2)\n",
    "#         t_targets = initial_indices + (torch.arange(halfT) % group_stride)\n",
    "#         t_adders = initial_indices - 1\n",
    "#         inverse_gates = torch.cumprod(gi[:, t_targets, :].reshape(B, halfT//group_stride, group_stride, S), dim=-2).reshape(B, halfT, S)\n",
    "#         xg[:, t_targets] += xg[:, t_adders] * inverse_gates.unsqueeze(-1)\n",
    "#     return xg\n",
    "\n",
    "# this should be faster but somehow it's slower???\n",
    "# @torch.compile\n",
    "def cum_gating_2d_torch_multistage(x, g):\n",
    "    xg = torch.einsum('bts,btc->btsc', g, x)\n",
    "    gi = 1 - g\n",
    "    B, T, S, C = xg.shape\n",
    "    halfT = T // 2\n",
    "    nstages = math.ceil(math.log2(T))\n",
    "    t_arange = torch.arange(halfT)\n",
    "    for stage in range(nstages):\n",
    "        group_stride = 1 << stage\n",
    "        initial_indices = group_stride + ((t_arange // group_stride) * group_stride * 2)\n",
    "        t_targets = initial_indices + (t_arange % group_stride)\n",
    "        t_adders = initial_indices - 1\n",
    "        xg[:, t_targets] += xg[:, t_adders] * gi[:, t_targets].unsqueeze(-1)\n",
    "        gi[:, t_targets] *= gi[:, t_adders]\n",
    "    return xg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def cg2d_fwd_kernel(\n",
    "    xg_ptr, gi_ptr, \n",
    "    B: tl.constexpr, S: tl.constexpr, C: tl.constexpr, T: tl.constexpr, nstages: tl.constexpr,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "    # Add more constants for tiling\n",
    "    BLOCK_SIZE_S: tl.constexpr,\n",
    "    BLOCK_SIZE_C: tl.constexpr,\n",
    "):\n",
    "    # Use multiple program IDs for better parallelization\n",
    "    pid = tl.program_id(axis=0)\n",
    "    # Compute batch, spatial, and channel indices\n",
    "    num_s_blocks = tl.cdiv(S, BLOCK_SIZE_S)\n",
    "    num_c_blocks = tl.cdiv(C, BLOCK_SIZE_C)\n",
    "    b = pid // (num_s_blocks * num_c_blocks)\n",
    "    rem = pid % (num_s_blocks * num_c_blocks)\n",
    "    s_block = rem // num_c_blocks\n",
    "    c_block = rem % num_c_blocks\n",
    "\n",
    "    # Compute actual indices\n",
    "    s_offs = tl.arange(0, BLOCK_SIZE_S)\n",
    "    c_offs = tl.arange(0, BLOCK_SIZE_C)\n",
    "    s_mask = s_offs < (S - s_block * BLOCK_SIZE_S)\n",
    "    c_mask = c_offs < (C - c_block * BLOCK_SIZE_C)\n",
    "    s_offs = s_block * BLOCK_SIZE_S + s_offs\n",
    "    c_offs = c_block * BLOCK_SIZE_C + c_offs\n",
    "\n",
    "    # Compute base pointers\n",
    "    xg_base = xg_ptr + b * T * S * C\n",
    "    gi_base = gi_ptr + b * T * S\n",
    "\n",
    "    # Precompute stages for better efficiency\n",
    "    # nstages = tl.ceil(tl.log2(float(T))).to(tl.int32)\n",
    "    offs = tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    for stage in tl.static_range(nstages):\n",
    "        group_stride = 1 << stage\n",
    "        # Process multiple elements per thread using BLOCK_SIZE\n",
    "        for block_start in tl.static_range(0, T//2, BLOCK_SIZE):\n",
    "            block_mask = offs < (T//2 - block_start)\n",
    "            block_s_mask = block_mask[:, None] & s_mask[None, :]\n",
    "            block_s_c_mask = block_mask[:, None, None] & s_mask[None, :, None] & c_mask[None, None, :]\n",
    "            \n",
    "            # Compute indices with vectorization\n",
    "            initial_indices = group_stride + ((offs + block_start) // group_stride) * group_stride * 2\n",
    "            t_targets = initial_indices + ((offs + block_start) % group_stride)\n",
    "            t_adders = initial_indices - 1\n",
    "\n",
    "            xg_targets_ptr = xg_base + t_targets[:, None, None] * S * C + s_offs[None, :, None] * C + c_offs[None, None, :] # (BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C)\n",
    "            xg_adders_ptr = xg_base + t_adders[:, None, None] * S * C + s_offs[None, :, None] * C + c_offs[None, None, :] # (BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C)\n",
    "            gi_targets_ptr = gi_base + t_targets[:, None] * S + s_offs[None, :] # (BLOCK_SIZE, BLOCK_SIZE_S)\n",
    "            gi_adders_ptr = gi_base + t_adders[:, None] * S + s_offs[None, :] # (BLOCK_SIZE, BLOCK_SIZE_S)\n",
    "            \n",
    "            xg_targets = tl.load(xg_targets_ptr, mask=block_s_c_mask) # (BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C)\n",
    "            xg_adders = tl.load(xg_adders_ptr, mask=block_s_c_mask) # (BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C)\n",
    "            gi_targets = tl.load(gi_targets_ptr, mask=block_s_mask) # (BLOCK_SIZE, BLOCK_SIZE_S)\n",
    "            gi_adders = tl.load(gi_adders_ptr, mask=block_s_mask) # (BLOCK_SIZE, BLOCK_SIZE_S)\n",
    "            \n",
    "            # Compute and store results\n",
    "            xg_targets += xg_adders * gi_targets[:, :, None] # (BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C)\n",
    "            # Update gates\n",
    "            gi_targets *= gi_adders # (BLOCK_SIZE, BLOCK_SIZE_S)\n",
    "            \n",
    "            tl.store(xg_targets_ptr, xg_targets, mask=block_s_c_mask) # (BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C)\n",
    "            tl.store(gi_targets_ptr, gi_targets, mask=block_s_mask) # (BLOCK_SIZE, BLOCK_SIZE_S)\n",
    "\n",
    "@triton.jit\n",
    "def cg2d_gxg_bwd_kernel(\n",
    "    gi_ptr, go_ptr,\n",
    "    B: tl.constexpr, S: tl.constexpr, C: tl.constexpr, T: tl.constexpr, nstages: tl.constexpr,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "    BLOCK_SIZE_S: tl.constexpr,\n",
    "    BLOCK_SIZE_C: tl.constexpr,\n",
    "):\n",
    "    # Similar structure to forward kernel with reversed indices\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_s_blocks = tl.cdiv(S, BLOCK_SIZE_S)\n",
    "    num_c_blocks = tl.cdiv(C, BLOCK_SIZE_C)\n",
    "    b = pid // (num_s_blocks * num_c_blocks)\n",
    "    rem = pid % (num_s_blocks * num_c_blocks)\n",
    "    s_block = rem // num_c_blocks\n",
    "    c_block = rem % num_c_blocks\n",
    "\n",
    "    s_offs = tl.arange(0, BLOCK_SIZE_S)\n",
    "    c_offs = tl.arange(0, BLOCK_SIZE_C)\n",
    "    s_mask = s_offs < (S - s_block * BLOCK_SIZE_S)\n",
    "    c_mask = c_offs < (C - c_block * BLOCK_SIZE_C)\n",
    "    s_offs = s_block * BLOCK_SIZE_S + s_offs\n",
    "    c_offs = c_block * BLOCK_SIZE_C + c_offs\n",
    "\n",
    "    gi_base = gi_ptr + b * T * S\n",
    "    go_base = go_ptr + b * T * S * C\n",
    "\n",
    "    # nstages = tl.ceil(tl.log2(float(T))).to(tl.int32)\n",
    "    offs = tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    for stage in tl.static_range(nstages):\n",
    "        group_stride = 1 << stage\n",
    "        for block_start in tl.range(0, T//2, BLOCK_SIZE):\n",
    "            block_mask = offs < (T//2 - block_start)\n",
    "            block_s_mask = block_mask[:, None] & s_mask[None, :]\n",
    "            block_s_c_mask = block_mask[:, None, None] & s_mask[None, :, None] & c_mask[None, None, :]\n",
    "            \n",
    "            initial_indices = T - 1 - group_stride - ((offs + block_start) // group_stride) * group_stride * 2\n",
    "            t_targets = initial_indices - ((offs + block_start) % group_stride)\n",
    "            t_adders = initial_indices + 1\n",
    "\n",
    "            go_targets_ptr = go_base + t_targets[:, None, None] * S * C + s_offs[None, :, None] * C + c_offs[None, None, :] # (BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C)\n",
    "            go_adders_ptr = go_base + t_adders[:, None, None] * S * C + s_offs[None, :, None] * C + c_offs[None, None, :] # (BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C)\n",
    "            gi_targets_ptr = gi_base + t_targets[:, None] * S + s_offs[None, :] # (BLOCK_SIZE, BLOCK_SIZE_S) \n",
    "            gi_adders_ptr = gi_base + t_adders[:, None] * S + s_offs[None, :] # (BLOCK_SIZE, BLOCK_SIZE_S)\n",
    "            \n",
    "            # Load with block masking\n",
    "            go_targets = tl.load(go_targets_ptr, mask=block_s_c_mask) # (BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C)\n",
    "            go_adders = tl.load(go_adders_ptr, mask=block_s_c_mask) # (BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C)\n",
    "            gi_targets = tl.load(gi_targets_ptr, mask=block_s_mask) # (BLOCK_SIZE, BLOCK_SIZE_S)\n",
    "            gi_adders = tl.load(gi_adders_ptr, mask=block_s_mask) # (BLOCK_SIZE, BLOCK_SIZE_S)\n",
    "            \n",
    "            # Compute and store results\n",
    "            go_targets += go_adders * gi_targets[:, :, None] # (BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C)\n",
    "            gi_targets *= gi_adders # (BLOCK_SIZE, BLOCK_SIZE_S)\n",
    "            \n",
    "            tl.store(go_targets_ptr, go_targets, mask=block_s_c_mask) # (BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C)\n",
    "            tl.store(gi_targets_ptr, gi_targets, mask=block_s_mask) # (BLOCK_SIZE, BLOCK_SIZE_S)\n",
    "\n",
    "@triton.jit\n",
    "def cg2d_ggi_bwd_kernel(\n",
    "    go_ptr, y_ptr, grad_gi_ptr,\n",
    "    B: tl.constexpr, S: tl.constexpr, C: tl.constexpr, T: tl.constexpr,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "    BLOCK_SIZE_S: tl.constexpr,\n",
    "    BLOCK_SIZE_C: tl.constexpr\n",
    "):\n",
    "    b = tl.program_id(axis=0)\n",
    "    pid = tl.program_id(axis=1)\n",
    "    num_t_blocks = tl.cdiv(T, BLOCK_SIZE)\n",
    "    num_s_blocks = tl.cdiv(S, BLOCK_SIZE_S)\n",
    "    num_c_blocks = tl.cdiv(C, BLOCK_SIZE_C)\n",
    "    t_block = pid // (num_s_blocks * num_c_blocks)\n",
    "    rem = pid % (num_s_blocks * num_c_blocks)\n",
    "    s_block = rem // num_c_blocks\n",
    "    c_block = rem % num_c_blocks\n",
    "\n",
    "    t_offs = tl.arange(0, BLOCK_SIZE)\n",
    "    s_offs = tl.arange(0, BLOCK_SIZE_S)\n",
    "    c_offs = tl.arange(0, BLOCK_SIZE_C)\n",
    "    t_mask = t_offs < (T - t_block * BLOCK_SIZE)\n",
    "    s_mask = s_offs < (S - s_block * BLOCK_SIZE_S)\n",
    "    c_mask = c_offs < (C - c_block * BLOCK_SIZE_C)\n",
    "    t_offs = t_block * BLOCK_SIZE + t_offs\n",
    "    s_offs = s_block * BLOCK_SIZE_S + s_offs\n",
    "    c_offs = c_block * BLOCK_SIZE_C + c_offs\n",
    "\n",
    "    # Compute grad_gi\n",
    "    # torch:\n",
    "    # grad_gi = grad_output * y\n",
    "    # grad_gi = grad_gi.sum(-1)\n",
    "    grad_gi_base = grad_gi_ptr + b * T * S\n",
    "    t_first_id = t_block * BLOCK_SIZE\n",
    "    s_first_id = s_block * BLOCK_SIZE_S\n",
    "    c_first_id = c_block * BLOCK_SIZE_C\n",
    "    # We can use make_block_ptr since the blocks we need are contiguous\n",
    "    go_block_ptr = tl.make_block_ptr(\n",
    "        base=go_ptr,\n",
    "        shape=(B, T, S, C),\n",
    "        strides=(T * S * C, S * C, C, 1),\n",
    "        offsets=(b, t_first_id, s_first_id, c_first_id),\n",
    "        block_shape=(1, BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C),\n",
    "        order=(0, 1, 2, 3)\n",
    "    )\n",
    "    go_block = tl.load(go_block_ptr) # (1, BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C)\n",
    "    y_block_ptr = tl.make_block_ptr(\n",
    "        base=y_ptr,\n",
    "        shape=(B, T, S, C),\n",
    "        strides=(T * S * C, S * C, C, 1),\n",
    "        offsets=(b, t_first_id, s_first_id, c_first_id), # y is already shifted to the right by 1\n",
    "        block_shape=(1, BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C),\n",
    "        order=(0, 1, 2, 3)\n",
    "    )\n",
    "    y_block = tl.load(y_block_ptr) # (1, BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C)\n",
    "\n",
    "    grad_gi = go_block * y_block  # (1, BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C)\n",
    "    grad_gi = tl.sum(grad_gi, axis=-1)  # (1, BLOCK_SIZE, BLOCK_SIZE_S)\n",
    "\n",
    "    # Need to use atomic add for accumulation between S blocks, so we also need to use manual pointer bc it's what atomic add accepts\n",
    "    grad_gi_block_ptr = grad_gi_base + t_offs[:, None] * S + s_offs[None, :]\n",
    "    grad_gi_mask = t_mask[:, None] & s_mask[None, :]\n",
    "    tl.atomic_add(grad_gi_block_ptr[None, :], grad_gi, mask=grad_gi_mask[None, :])\n",
    "\n",
    "class CumulativeGating2DTriton(torch.autograd.Function):\n",
    "    # @torch.compiler.disable\n",
    "    @staticmethod\n",
    "    def forward(ctx, xg, gi):\n",
    "        xg = xg.contiguous()\n",
    "        gi = gi.contiguous()\n",
    "        orig_gi = gi.clone()\n",
    "        B, T, S, C = xg.shape\n",
    "        \n",
    "        # Configure block sizes\n",
    "        BLOCK_SIZE = 32\n",
    "        BLOCK_SIZE_S = 16\n",
    "        BLOCK_SIZE_C = 16\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        num_s_blocks = triton.cdiv(S, BLOCK_SIZE_S)\n",
    "        num_c_blocks = triton.cdiv(C, BLOCK_SIZE_C)\n",
    "        nstages = math.ceil(math.log2(T))\n",
    "        grid = (B * num_s_blocks * num_c_blocks,)\n",
    "        \n",
    "        # Launch kernel\n",
    "        cg2d_fwd_kernel[grid](\n",
    "            xg, gi,\n",
    "            B, S, C, T, nstages,\n",
    "            BLOCK_SIZE=BLOCK_SIZE,\n",
    "            BLOCK_SIZE_S=BLOCK_SIZE_S,\n",
    "            BLOCK_SIZE_C=BLOCK_SIZE_C,\n",
    "        )\n",
    "        \n",
    "        ctx.save_for_backward(xg, orig_gi)\n",
    "        return xg\n",
    "    \n",
    "    # @torch.compiler.disable\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.contiguous()\n",
    "        y, gi = ctx.saved_tensors\n",
    "        B, T, S, C = y.shape\n",
    "        \n",
    "        # Configure block sizes\n",
    "        BLOCK_SIZE = 32\n",
    "        BLOCK_SIZE_S = 16\n",
    "        BLOCK_SIZE_C = 16\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        num_t_blocks = triton.cdiv(T, BLOCK_SIZE)\n",
    "        num_s_blocks = triton.cdiv(S, BLOCK_SIZE_S)\n",
    "        num_c_blocks = triton.cdiv(C, BLOCK_SIZE_C)\n",
    "        nstages = math.ceil(math.log2(T))\n",
    "        grid = (B * num_s_blocks * num_c_blocks,)\n",
    "        \n",
    "        gi = torch.cat((gi[:, 1:], torch.ones_like(gi[:, -1:])), dim=1).contiguous()\n",
    "        grad_xg = grad_output.clone()\n",
    "        y = torch.cat((torch.zeros_like(y[:, :1]), y[:, :-1]), dim=1).contiguous()\n",
    "        grad_gi = torch.zeros_like(gi)\n",
    "\n",
    "        # Launch kernel\n",
    "        cg2d_gxg_bwd_kernel[grid](\n",
    "            gi, grad_xg,\n",
    "            B, S, C, T, nstages,\n",
    "            BLOCK_SIZE=BLOCK_SIZE,\n",
    "            BLOCK_SIZE_S=BLOCK_SIZE_S,\n",
    "            BLOCK_SIZE_C=BLOCK_SIZE_C,\n",
    "        )\n",
    "\n",
    "        grid = (B, num_t_blocks * num_s_blocks * num_c_blocks)\n",
    "        cg2d_ggi_bwd_kernel[grid](\n",
    "            grad_xg, y, grad_gi,\n",
    "            B, S, C, T,\n",
    "            BLOCK_SIZE=BLOCK_SIZE,\n",
    "            BLOCK_SIZE_S=BLOCK_SIZE_S,\n",
    "            BLOCK_SIZE_C=BLOCK_SIZE_C,\n",
    "        )\n",
    "\n",
    "        return grad_xg, grad_gi\n",
    "\n",
    "# @torch.compile\n",
    "def cum_gating_2d_triton(x, g):\n",
    "    x_g = torch.einsum('bts,btc->btsc', g, x)\n",
    "    inv_g = 1 - g\n",
    "    return CumulativeGating2DTriton.apply(x_g, inv_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[[-1.66,  1.93,  ...,  0.76, -0.56],\n",
      "         [ 0.28,  0.50,  ..., -0.77, -0.10],\n",
      "         ...,\n",
      "         [ 0.18, -1.77,  ...,  0.73, -0.48],\n",
      "         [-0.11, -0.20,  ...,  1.67, -0.06]],\n",
      "\n",
      "        [[ 1.10,  0.26,  ...,  0.27, -0.22],\n",
      "         [ 0.74, -0.59,  ..., -1.01,  0.94],\n",
      "         ...,\n",
      "         [-0.53, -0.85,  ..., -0.38,  0.09],\n",
      "         [ 1.18, -0.04,  ...,  0.29, -0.66]]], device='cuda:0')\n",
      "g: tensor([[[0.03, 0.02,  ..., 0.01, 0.10],\n",
      "         [0.01, 0.00,  ..., 0.02, 0.00],\n",
      "         ...,\n",
      "         [0.03, 0.06,  ..., 0.01, 0.03],\n",
      "         [0.01, 0.03,  ..., 0.00, 0.04]],\n",
      "\n",
      "        [[0.03, 0.04,  ..., 0.03, 0.01],\n",
      "         [0.03, 0.02,  ..., 0.00, 0.02],\n",
      "         ...,\n",
      "         [0.08, 0.05,  ..., 0.01, 0.01],\n",
      "         [0.02, 0.02,  ..., 0.03, 0.02]]], device='cuda:0')\n",
      "torch:\n",
      "tensor([[[[-0.04,  0.04,  ...,  0.02, -0.01],\n",
      "          [-0.02,  0.03,  ...,  0.01, -0.01],\n",
      "          ...,\n",
      "          [-0.00,  0.00,  ...,  0.00, -0.00],\n",
      "          [-0.10,  0.12,  ...,  0.05, -0.03]],\n",
      "\n",
      "         [[-0.03,  0.04,  ...,  0.01, -0.01],\n",
      "          [-0.02,  0.03,  ...,  0.01, -0.01],\n",
      "          ...,\n",
      "          [-0.00,  0.01,  ..., -0.00, -0.00],\n",
      "          [-0.10,  0.12,  ...,  0.04, -0.03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.00, -0.07,  ...,  0.22,  0.03],\n",
      "          [-0.13, -0.18,  ..., -0.03, -0.22],\n",
      "          ...,\n",
      "          [-0.01, -0.18,  ..., -0.05,  0.01],\n",
      "          [-0.06, -0.16,  ...,  0.01, -0.09]],\n",
      "\n",
      "         [[ 0.00, -0.07,  ...,  0.23,  0.03],\n",
      "          [-0.13, -0.18,  ...,  0.01, -0.22],\n",
      "          ...,\n",
      "          [-0.01, -0.18,  ..., -0.04,  0.01],\n",
      "          [-0.06, -0.16,  ...,  0.09, -0.09]]],\n",
      "\n",
      "\n",
      "        [[[ 0.03,  0.01,  ...,  0.01, -0.01],\n",
      "          [ 0.04,  0.01,  ...,  0.01, -0.01],\n",
      "          ...,\n",
      "          [ 0.02,  0.01,  ...,  0.01, -0.00],\n",
      "          [ 0.01,  0.00,  ...,  0.00, -0.00]],\n",
      "\n",
      "         [[ 0.04, -0.01,  ..., -0.02,  0.02],\n",
      "          [ 0.05,  0.00,  ..., -0.00,  0.01],\n",
      "          ...,\n",
      "          [ 0.03,  0.00,  ...,  0.00, -0.00],\n",
      "          [ 0.01, -0.00,  ..., -0.01,  0.01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.36,  0.13,  ..., -0.12, -0.02],\n",
      "          [-0.38, -0.20,  ...,  0.01,  0.11],\n",
      "          ...,\n",
      "          [-0.11,  0.06,  ...,  0.12,  0.10],\n",
      "          [-0.21,  0.27,  ...,  0.05,  0.02]],\n",
      "\n",
      "         [[-0.33,  0.13,  ..., -0.11, -0.04],\n",
      "          [-0.35, -0.20,  ...,  0.02,  0.09],\n",
      "          ...,\n",
      "          [-0.07,  0.06,  ...,  0.13,  0.08],\n",
      "          [-0.19,  0.27,  ...,  0.06,  0.01]]]], device='cuda:0')\n",
      "naive:\n",
      "tensor([[[[-0.05,  0.06,  ...,  0.02, -0.02],\n",
      "          [-0.03,  0.03,  ...,  0.01, -0.01],\n",
      "          ...,\n",
      "          [-0.01,  0.01,  ...,  0.00, -0.00],\n",
      "          [-0.16,  0.19,  ...,  0.07, -0.05]],\n",
      "\n",
      "         [[-0.05,  0.06,  ...,  0.02, -0.02],\n",
      "          [-0.03,  0.03,  ...,  0.01, -0.01],\n",
      "          ...,\n",
      "          [-0.00,  0.02,  ..., -0.01, -0.00],\n",
      "          [-0.16,  0.19,  ...,  0.07, -0.05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.00, -0.07,  ...,  0.22,  0.03],\n",
      "          [-0.13, -0.18,  ..., -0.03, -0.22],\n",
      "          ...,\n",
      "          [-0.01, -0.18,  ..., -0.05,  0.01],\n",
      "          [-0.06, -0.16,  ...,  0.01, -0.09]],\n",
      "\n",
      "         [[ 0.00, -0.07,  ...,  0.23,  0.03],\n",
      "          [-0.13, -0.18,  ...,  0.01, -0.22],\n",
      "          ...,\n",
      "          [-0.01, -0.18,  ..., -0.04,  0.01],\n",
      "          [-0.06, -0.16,  ...,  0.09, -0.09]]],\n",
      "\n",
      "\n",
      "        [[[ 0.03,  0.01,  ...,  0.01, -0.01],\n",
      "          [ 0.05,  0.01,  ...,  0.01, -0.01],\n",
      "          ...,\n",
      "          [ 0.03,  0.01,  ...,  0.01, -0.01],\n",
      "          [ 0.01,  0.00,  ...,  0.00, -0.00]],\n",
      "\n",
      "         [[ 0.05, -0.01,  ..., -0.02,  0.02],\n",
      "          [ 0.06,  0.00,  ..., -0.01,  0.01],\n",
      "          ...,\n",
      "          [ 0.03,  0.01,  ...,  0.00, -0.00],\n",
      "          [ 0.03, -0.01,  ..., -0.02,  0.02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.36,  0.13,  ..., -0.12, -0.02],\n",
      "          [-0.38, -0.20,  ...,  0.01,  0.11],\n",
      "          ...,\n",
      "          [-0.11,  0.06,  ...,  0.12,  0.10],\n",
      "          [-0.21,  0.27,  ...,  0.05,  0.02]],\n",
      "\n",
      "         [[-0.33,  0.13,  ..., -0.11, -0.04],\n",
      "          [-0.35, -0.20,  ...,  0.02,  0.09],\n",
      "          ...,\n",
      "          [-0.07,  0.06,  ...,  0.13,  0.08],\n",
      "          [-0.19,  0.27,  ...,  0.06,  0.01]]]], device='cuda:0')\n",
      "triton:\n",
      "tensor([[[[-0.05,  0.06,  ...,  0.02, -0.02],\n",
      "          [-0.03,  0.03,  ...,  0.01, -0.01],\n",
      "          ...,\n",
      "          [-0.01,  0.01,  ...,  0.00, -0.00],\n",
      "          [-0.16,  0.19,  ...,  0.07, -0.05]],\n",
      "\n",
      "         [[-0.05,  0.06,  ...,  0.02, -0.02],\n",
      "          [-0.03,  0.03,  ...,  0.01, -0.01],\n",
      "          ...,\n",
      "          [-0.00,  0.02,  ..., -0.01, -0.00],\n",
      "          [-0.16,  0.19,  ...,  0.07, -0.05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.00, -0.07,  ...,  0.22,  0.03],\n",
      "          [-0.13, -0.18,  ..., -0.03, -0.22],\n",
      "          ...,\n",
      "          [-0.01, -0.18,  ..., -0.05,  0.01],\n",
      "          [-0.06, -0.16,  ...,  0.01, -0.09]],\n",
      "\n",
      "         [[ 0.00, -0.07,  ...,  0.23,  0.03],\n",
      "          [-0.13, -0.18,  ...,  0.01, -0.22],\n",
      "          ...,\n",
      "          [-0.01, -0.18,  ..., -0.04,  0.01],\n",
      "          [-0.06, -0.16,  ...,  0.09, -0.09]]],\n",
      "\n",
      "\n",
      "        [[[ 0.03,  0.01,  ...,  0.01, -0.01],\n",
      "          [ 0.05,  0.01,  ...,  0.01, -0.01],\n",
      "          ...,\n",
      "          [ 0.03,  0.01,  ...,  0.01, -0.01],\n",
      "          [ 0.01,  0.00,  ...,  0.00, -0.00]],\n",
      "\n",
      "         [[ 0.05, -0.01,  ..., -0.02,  0.02],\n",
      "          [ 0.06,  0.00,  ..., -0.01,  0.01],\n",
      "          ...,\n",
      "          [ 0.03,  0.01,  ...,  0.00, -0.00],\n",
      "          [ 0.03, -0.01,  ..., -0.02,  0.02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.36,  0.13,  ..., -0.12, -0.02],\n",
      "          [-0.38, -0.20,  ...,  0.01,  0.11],\n",
      "          ...,\n",
      "          [-0.11,  0.06,  ...,  0.12,  0.10],\n",
      "          [-0.21,  0.27,  ...,  0.05,  0.02]],\n",
      "\n",
      "         [[-0.33,  0.13,  ..., -0.11, -0.04],\n",
      "          [-0.35, -0.20,  ...,  0.02,  0.09],\n",
      "          ...,\n",
      "          [-0.07,  0.06,  ...,  0.13,  0.08],\n",
      "          [-0.19,  0.27,  ...,  0.06,  0.01]]]], device='cuda:0')\n",
      "max diff: tensor(    0.00, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Difference between naive and triton')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try the forward pass\n",
    "# test cumulative_gating_2d_torch with always the same random inputs\n",
    "B, T, S, W, C = 2, 512, 32, 4, 32\n",
    "# B, T, S, W, C = 2, 16, 4, 4, 4\n",
    "x = torch.randn(B, T, C, device=device)\n",
    "g = F.softmax(torch.randn(B, T, S, device=device), dim=-1)\n",
    "gi = 1 - g\n",
    "print(\"x:\", x)\n",
    "print(\"g:\", g)\n",
    "print(\"torch:\")\n",
    "print(cum_gating_2d_torch(x, g))\n",
    "print(\"naive:\")\n",
    "out_naive = cum_gating_2d_naive(x, g)\n",
    "print(out_naive)\n",
    "# print(\"torch multistage:\")\n",
    "# out = cum_gating_2d_torch_multistage(x, g)\n",
    "# print(out)\n",
    "print(\"triton:\")\n",
    "out = cum_gating_2d_triton(x, g)\n",
    "print(out)\n",
    "print(\"max diff:\", (out - out_naive).abs().max())\n",
    "# visualize the difference as a heatmap\n",
    "plt.figure()\n",
    "plt.imshow((out - out_naive).abs().mean(0).mean(-1).cpu().numpy())\n",
    "plt.colorbar()\n",
    "plt.title(\"Difference between naive and triton\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_output: tensor([[[[-0.58, -0.89,  ..., -0.10, -0.60],\n",
      "          [-0.76,  0.90,  ...,  0.39, -2.33],\n",
      "          ...,\n",
      "          [-1.65,  1.45,  ...,  0.21, -1.42],\n",
      "          [ 0.27, -0.90,  ...,  0.01,  1.07]],\n",
      "\n",
      "         [[-0.46, -0.43,  ...,  0.25, -0.68],\n",
      "          [ 1.22,  0.98,  ..., -1.10,  0.21],\n",
      "          ...,\n",
      "          [ 0.96, -0.93,  ...,  0.50,  0.33],\n",
      "          [-1.38, -0.91,  ...,  0.73,  1.91]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.12, -0.22,  ...,  1.06,  0.28],\n",
      "          [-0.89,  0.74,  ..., -0.36,  1.04],\n",
      "          ...,\n",
      "          [ 0.15, -0.57,  ...,  2.18,  0.56],\n",
      "          [-0.16,  0.73,  ...,  2.78, -1.12]],\n",
      "\n",
      "         [[ 0.29,  0.72,  ..., -1.62,  1.10],\n",
      "          [-1.32,  0.17,  ..., -0.48,  0.47],\n",
      "          ...,\n",
      "          [-0.49, -0.16,  ...,  0.43,  0.94],\n",
      "          [-0.35,  0.75,  ...,  0.18, -0.27]]],\n",
      "\n",
      "\n",
      "        [[[-1.05, -0.35,  ...,  0.15,  0.40],\n",
      "          [-0.01, -1.40,  ...,  0.84, -1.73],\n",
      "          ...,\n",
      "          [ 0.73, -0.64,  ...,  0.67, -0.26],\n",
      "          [-0.96,  1.47,  ..., -1.47, -0.59]],\n",
      "\n",
      "         [[ 0.45, -1.70,  ..., -0.72,  1.51],\n",
      "          [ 1.37, -0.03,  ...,  1.46, -0.33],\n",
      "          ...,\n",
      "          [-1.55, -0.59,  ...,  0.42,  0.03],\n",
      "          [-0.72,  0.43,  ...,  0.39, -1.32]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.07,  0.84,  ...,  1.15,  0.41],\n",
      "          [ 0.47,  0.99,  ..., -0.68,  0.15],\n",
      "          ...,\n",
      "          [-0.11, -0.47,  ..., -0.81,  0.20],\n",
      "          [ 0.60, -1.02,  ...,  0.02,  1.86]],\n",
      "\n",
      "         [[ 0.31,  0.33,  ...,  0.42, -0.41],\n",
      "          [ 1.60, -0.98,  ...,  0.65, -3.13],\n",
      "          ...,\n",
      "          [ 1.71,  0.10,  ...,  1.55,  0.29],\n",
      "          [ 1.06, -0.20,  ...,  0.21,  1.60]]]], device='cuda:0')\n",
      "torch:\n",
      "x grad: tensor([[[-0.26, -2.05,  ..., -0.33,  2.83],\n",
      "         [ 0.56, -1.13,  ...,  0.16,  1.63],\n",
      "         ...,\n",
      "         [-0.41, -0.41,  ..., -0.15,  0.33],\n",
      "         [ 0.03, -0.41,  ..., -0.44,  0.15]],\n",
      "\n",
      "        [[-0.33, -0.67,  ...,  0.04, -0.19],\n",
      "         [ 0.26, -0.50,  ..., -1.24,  0.23],\n",
      "         ...,\n",
      "         [ 0.08,  0.17,  ...,  0.49, -0.21],\n",
      "         [ 0.19, -0.21,  ..., -0.10, -0.17]]], device='cuda:0')\n",
      "g grad: tensor([[[-33.18, -15.27,  ...,  18.63,   5.81],\n",
      "         [  1.84,  -2.52,  ...,  -2.47,   0.59],\n",
      "         ...,\n",
      "         [-14.10,  -0.09,  ...,   6.92,  11.31],\n",
      "         [ -4.86,   3.01,  ...,  -8.94,  -4.17]],\n",
      "\n",
      "        [[-22.09,  26.54,  ...,  -2.72,  -8.29],\n",
      "         [ 23.51,   7.10,  ...,  27.29, -10.69],\n",
      "         ...,\n",
      "         [  7.12,   0.98,  ...,   6.65,  -4.60],\n",
      "         [  4.30,  -4.69,  ...,   2.93,  -4.02]]], device='cuda:0')\n",
      "naive:\n",
      "x grad: tensor([[[-0.22, -2.34,  ..., -0.48,  3.16],\n",
      "         [ 0.62, -1.44,  ...,  0.09,  1.47],\n",
      "         ...,\n",
      "         [-0.41, -0.41,  ..., -0.15,  0.33],\n",
      "         [ 0.03, -0.41,  ..., -0.44,  0.15]],\n",
      "\n",
      "        [[-0.23, -1.04,  ...,  0.32, -0.17],\n",
      "         [ 0.40, -0.41,  ..., -1.32,  0.33],\n",
      "         ...,\n",
      "         [ 0.08,  0.17,  ...,  0.49, -0.21],\n",
      "         [ 0.19, -0.21,  ..., -0.10, -0.17]]], device='cuda:0')\n",
      "g grad: tensor([[[-42.78, -15.75,  ...,  26.08,   7.08],\n",
      "         [  4.64,  -2.91,  ...,  -3.76,   3.07],\n",
      "         ...,\n",
      "         [-15.11,   0.22,  ...,   6.38,  10.36],\n",
      "         [ -5.85,   3.32,  ...,  -9.47,  -5.12]],\n",
      "\n",
      "        [[-23.79,  28.70,  ...,  -2.99, -16.70],\n",
      "         [ 25.50,   7.96,  ...,  29.02, -16.33],\n",
      "         ...,\n",
      "         [  7.30,   1.84,  ...,   6.49,  -6.09],\n",
      "         [  4.47,  -3.86,  ...,   2.76,  -5.52]]], device='cuda:0')\n",
      "gi grad: None\n",
      "triton:\n",
      "x grad: tensor([[[-0.22, -2.34,  ..., -0.48,  3.16],\n",
      "         [ 0.62, -1.44,  ...,  0.09,  1.47],\n",
      "         ...,\n",
      "         [-0.41, -0.41,  ..., -0.15,  0.33],\n",
      "         [ 0.03, -0.41,  ..., -0.44,  0.15]],\n",
      "\n",
      "        [[-0.23, -1.04,  ...,  0.32, -0.17],\n",
      "         [ 0.40, -0.41,  ..., -1.32,  0.33],\n",
      "         ...,\n",
      "         [ 0.08,  0.17,  ...,  0.49, -0.21],\n",
      "         [ 0.19, -0.21,  ..., -0.10, -0.17]]], device='cuda:0')\n",
      "g grad: tensor([[[-42.78, -15.75,  ...,  26.08,   7.08],\n",
      "         [  4.64,  -2.91,  ...,  -3.76,   3.07],\n",
      "         ...,\n",
      "         [-15.11,   0.22,  ...,   6.38,  10.36],\n",
      "         [ -5.85,   3.32,  ...,  -9.47,  -5.12]],\n",
      "\n",
      "        [[-23.79,  28.70,  ...,  -2.99, -16.70],\n",
      "         [ 25.50,   7.96,  ...,  29.02, -16.33],\n",
      "         ...,\n",
      "         [  7.30,   1.84,  ...,   6.49,  -6.09],\n",
      "         [  4.47,  -3.86,  ...,   2.76,  -5.52]]], device='cuda:0')\n",
      "max diff x grad: tensor(    0.00, device='cuda:0')\n",
      "max diff g grad: tensor(    0.00, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# try the backward pass, compare with torch autograd\n",
    "# test cumulative_gating_2d_torch with always the same random inputs\n",
    "grad_output = torch.randn(B, T, S, C, device=device)\n",
    "print(\"grad_output:\", grad_output)\n",
    "print(\"torch:\")\n",
    "x.requires_grad = True\n",
    "g.requires_grad = True\n",
    "gi.requires_grad = True\n",
    "out = cum_gating_2d_torch(x, g)\n",
    "out.backward(grad_output)\n",
    "print('x grad:', x.grad)\n",
    "print('g grad:', g.grad)\n",
    "x.grad = None\n",
    "g.grad = None\n",
    "gi.grad = None\n",
    "print(\"naive:\")\n",
    "x.requires_grad = True\n",
    "g.requires_grad = True\n",
    "gi.requires_grad = True\n",
    "out = cum_gating_2d_naive(x, g)\n",
    "out.backward(grad_output)\n",
    "naive_x_grad = x.grad.clone()\n",
    "naive_g_grad = g.grad.clone()\n",
    "naive_gi_grad = gi.grad.clone() if gi.grad is not None else None\n",
    "print('x grad:', x.grad)\n",
    "print('g grad:', g.grad)\n",
    "print('gi grad:', gi.grad)\n",
    "# reset gradients\n",
    "x.grad = None\n",
    "g.grad = None\n",
    "gi.grad = None\n",
    "print(\"triton:\")\n",
    "x.requires_grad = True\n",
    "g.requires_grad = True\n",
    "out = cum_gating_2d_triton(x, g)\n",
    "out.backward(grad_output)\n",
    "print('x grad:', x.grad)\n",
    "print('g grad:', g.grad)\n",
    "print(\"max diff x grad:\", (x.grad - naive_x_grad).abs().max())\n",
    "print(\"max diff g grad:\", (g.grad - naive_g_grad).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch approximation (cheating) forward:\n",
      "9.44 ms Â± 7.24 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n",
      "torch approximation (cheating) forward & backward:\n",
      "49.6 ms Â± 42.2 Î¼s per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "triton forward:\n",
      "31.1 ms Â± 15.2 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n",
      "triton forward & backward:\n",
      "124 ms Â± 28.6 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# measure the speedup\n",
    "# B, T, S, C = 8, 1024, 32, 64\n",
    "B, T, S, C = 2, 2048, 128, 256\n",
    "x = torch.randn(B, T, C, device=device)\n",
    "g = F.softmax(torch.randn(B, T, S, device=device), dim=-1)\n",
    "grad_output = torch.randn(B, T, S, C, device=device)\n",
    "# warmup all torch.compile functions first\n",
    "x.requires_grad = True\n",
    "g.requires_grad = True\n",
    "out = cum_gating_2d_torch(x, g)\n",
    "out.backward(grad_output)\n",
    "x.grad = None\n",
    "g.grad = None\n",
    "out = cum_gating_2d_triton(x, g)\n",
    "out.backward(grad_output)\n",
    "x.grad = None\n",
    "g.grad = None\n",
    "\n",
    "print(\"torch approximation (cheating) forward:\")\n",
    "x.requires_grad = True\n",
    "g.requires_grad = True\n",
    "%timeit cum_gating_2d_torch(x, g)\n",
    "print(\"torch approximation (cheating) forward & backward:\")\n",
    "def ftorch():\n",
    "    x.grad = None\n",
    "    g.grad = None\n",
    "    out_torch = cum_gating_2d_torch(x, g)\n",
    "    out_torch.backward(grad_output)\n",
    "%timeit ftorch()\n",
    "# print(\"\\ntorch naive forward:\")\n",
    "# x.requires_grad = True\n",
    "# g.requires_grad = True\n",
    "# %timeit cum_gating_2d_naive(x, g)\n",
    "# print(\"torch naive forward & backward:\")\n",
    "# def fnaive():\n",
    "#     x.grad = None\n",
    "#     g.grad = None\n",
    "#     out_naive = cum_gating_2d_naive(x, g)\n",
    "#     out_naive.backward(grad_output)\n",
    "# %timeit fnaive()\n",
    "# print(\"\\ntorch multistage forward:\")\n",
    "# x.requires_grad = True\n",
    "# g.requires_grad = True\n",
    "# %timeit cum_gating_2d_torch_multistage(x, g)\n",
    "# print(\"torch multistage forward & backward:\")\n",
    "# def ftorch():\n",
    "#     x.grad = None\n",
    "#     g.grad = None\n",
    "#     out_torch = cum_gating_2d_torch_multistage(x, g)\n",
    "#     out_torch.backward(grad_output)\n",
    "# %timeit ftorch()\n",
    "print(\"\\ntriton forward:\")\n",
    "x.requires_grad = True\n",
    "g.requires_grad = True\n",
    "%timeit cum_gating_2d_triton(x, g)\n",
    "print(\"triton forward & backward:\")\n",
    "def ftriton():\n",
    "    x.grad = None\n",
    "    g.grad = None\n",
    "    out_triton = cum_gating_2d_triton(x, g)\n",
    "    out_triton.backward(grad_output)\n",
    "%timeit ftriton()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCANTransformerConfig:\n",
    "    def __init__(self, vocab_size, seq_len, window_len, embed_size, head_num, layer_num, state_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.window_len = window_len\n",
    "        self.embed_size = embed_size\n",
    "        self.head_num = head_num\n",
    "        self.layer_num = layer_num\n",
    "        self.state_size = state_size\n",
    "\n",
    "# no reverse cumprod in pytorch :(\n",
    "def reverse_cumprod(x, dim=-1):\n",
    "    # cp = torch.cumprod(x, dim=dim)\n",
    "    # i = [slice(None)] * x.dim()\n",
    "    # i[dim] = -1\n",
    "    # return (x / (cp + 1e-8)) * cp[i].unsqueeze(dim)\n",
    "    return torch.flip(torch.cumprod(torch.flip(x, [dim]), dim), [dim])\n",
    "\n",
    "# this is for stator and integrator in SCAN\n",
    "def cum_gating_2d_torch(x, g):\n",
    "    # x is (B, T, C) and g is (B, T, S), return (B, T, S, C)\n",
    "    # the parallel cumulative version of the recursion o[i] = o[i-1](1-g[i])^T + x[i]g[i]^T\n",
    "    inv_g = 1 - g\n",
    "    r_inv_g = reverse_cumprod(inv_g, dim=1)\n",
    "    r_inv_g = torch.cat((r_inv_g[:, 1:], torch.ones_like(r_inv_g[:, -1:])), dim=1)\n",
    "    g_r_inv_g = g * r_inv_g\n",
    "    x_g_r_inv_g = torch.einsum('bts,btc->btsc', g_r_inv_g, x)\n",
    "    c_x_g_r_inv_g = torch.cumsum(x_g_r_inv_g, dim=1)\n",
    "    return c_x_g_r_inv_g / (r_inv_g.unsqueeze(-1) + 1e-8) # avoid division by zero\n",
    "\n",
    "# for debugging, now a naive version\n",
    "def cum_gating_2d_naive(x, g):\n",
    "    # x is (B, T, C) and g is (B, T, S), return (B, T, S, C)\n",
    "    # the naive iterative version of the recursion o[i] = o[i-1](1-g[i])^T + x[i]g[i]^T\n",
    "    B, T, C = x.shape\n",
    "    S = g.shape[-1]\n",
    "    x_g = torch.einsum('bts,btc->btsc', g, x)\n",
    "    \n",
    "    out_list = [x_g[:, 0].unsqueeze(1)]  # Start with the first time step\n",
    "    \n",
    "    for i in range(1, T):\n",
    "        prev_out = out_list[-1][:, -1]  # Get the last time step from previous output\n",
    "        complement_g = 1 - g[:, i]\n",
    "        current_out = torch.einsum('bsc,bs->bsc', prev_out, complement_g) + x_g[:, i]\n",
    "        out_list.append(current_out.unsqueeze(1))\n",
    "    \n",
    "    return torch.cat(out_list, dim=1)\n",
    "\n",
    "# sliding window fold\n",
    "def sliding_window_fold(x, window_len):\n",
    "    # x is a (B, T, C) tensor\n",
    "    # output should be a (B, T, window_len, C) tensor that slides over the T dimension\n",
    "    # first window_len-1 elements will have to be padded with zeros in the beginning\n",
    "    # example: x = torch.tensor([[[1,2],[3,4],[5,6],[7,8],[9,10]]])\n",
    "    # sliding_window_fold(x, 2) -> torch.tensor([[[[0,0],[1,2]],[[1,2],[3,4]],[[3,4],[5,6]],[[5,6],[7,8]],[[7,8],[9,10]]]])\n",
    "    padded_x = F.pad(x, (0, 0, window_len - 1, 0), mode='constant', value=0)\n",
    "    output = padded_x.unfold(dimension=1, size=window_len, step=1).permute(0, 1, 3, 2)\n",
    "    return output\n",
    "\n",
    "def attend_folded_all_keys_torch(q, k, states, W):\n",
    "    k = sliding_window_fold(k, W) # (B, T, W, C)\n",
    "    all_keys = torch.cat((states, k), dim=2) # (B, T, S, C) + (B, T, W, C) -> (B, T, S+W, C)\n",
    "    scores = torch.einsum(\"btc, btxc -> btx\", q, all_keys) # (B, T, S+W)\n",
    "    return scores\n",
    "\n",
    "def accumulate_folded_all_values_torch(s, v, states, W):\n",
    "    v = sliding_window_fold(v, W) # (B, T, W, C)\n",
    "    all_values = torch.cat((states, v), dim=2) # (B, T, S, C) + (B, T, W, C) -> (B, T, S+W, C)\n",
    "    out = torch.einsum(\"btx, btxc -> btc\", s, all_values) # (B, T, C)\n",
    "    return out\n",
    "\n",
    "# repeat last element in dim n_repeat times\n",
    "def repeat_last(x, n_repeat, dim):\n",
    "    last = x.select(dim, -1).unsqueeze(dim)\n",
    "    last_repeated = last.expand(*x.shape[:dim], n_repeat, *x.shape[dim+1:])\n",
    "    return torch.cat([x, last_repeated], dim=dim)\n",
    "\n",
    "def scores_mask(T, W, S):\n",
    "    # create lower right triangle mask (W, W)\n",
    "    mask = torch.tril(torch.ones(W, W)).flip(1)\n",
    "    # concat ones with size (T-W, W) in 0th dim\n",
    "    mask = torch.cat((mask, torch.ones(T-W, W)), dim=0)\n",
    "    # concat ones with size (T, S) in 1st dim\n",
    "    mask = torch.cat((torch.ones(T, S), mask), dim=1)\n",
    "    return mask\n",
    "\n",
    "def build_alibi_tensor_SCAN(head_num, seq_len, window_len, state_size):\n",
    "    slopes = torch.tensor([2 ** (-8.0 * i / head_num) for i in range(head_num)])\n",
    "    alibi = torch.zeros((head_num, seq_len, window_len))\n",
    "    for i in range(seq_len):\n",
    "        for j in range(window_len):\n",
    "            if i < window_len:\n",
    "                alibi[:, i, j] = slopes * (j - window_len + 1) if i > (window_len - j - 2) else 0\n",
    "            else:\n",
    "                alibi[:, i, j] = alibi[:, window_len-1, j]\n",
    "    # Now concat a zeros tensor of size (head_num, seq_len, state_size) to the left of the above square tensor\n",
    "    alibi = torch.cat((torch.zeros(head_num, seq_len, state_size), alibi), dim=2)\n",
    "    return alibi.to(device)\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "    \n",
    "class SCAttentionHead(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seq_len = config.seq_len\n",
    "        self.window_len = config.window_len\n",
    "        self.state_size = config.state_size\n",
    "        self.head_size = config.embed_size // config.head_num\n",
    "        self.stator = nn.Linear(config.embed_size, self.head_size, bias=False)\n",
    "        self.integrator = nn.Linear(config.embed_size, self.state_size, bias=False)\n",
    "        self.key = nn.Linear(config.embed_size, self.head_size, bias=False)\n",
    "        self.query = nn.Linear(config.embed_size, self.head_size, bias=False)\n",
    "        self.value = nn.Linear(config.embed_size, self.head_size, bias=False)\n",
    "        self.register_buffer('mask', scores_mask(config.seq_len, config.window_len, config.state_size))\n",
    "\n",
    "    def forward(self, x, alibi, kv_cache=None, state=None):\n",
    "        B, T, E = x.shape\n",
    "        train = T == self.seq_len\n",
    "        W = self.window_len\n",
    "        S = self.state_size\n",
    "        C = self.head_size\n",
    "        _, T_past, _ = kv_cache[0].shape if kv_cache is not None and kv_cache[0] is not None else (0, 0, 0)\n",
    "\n",
    "        stator = self.stator(x) # (B, T, C)\n",
    "        integrator = self.integrator(x) # (B, T, S)\n",
    "        integrator = F.softmax(integrator, dim=-1) # (B, T, S)\n",
    "        # integrator = F.sigmoid(integrator) # (B, T, S)\n",
    "        if T > 1:\n",
    "            # assuming there is never an inference step with a given state AND prompt for now...\n",
    "            states = cum_gating_2d_triton(stator, integrator) # (B, T, S, C)\\\n",
    "            state = states[:, -1] if state is not None else None\n",
    "        else:\n",
    "            # recurrent update rule: o[i] = o[i-1](1-g[i])^T + x[i]g[i]^T\n",
    "            states = torch.einsum(\"bts, btc -> bsc\", integrator, stator) # (B, S, C)\n",
    "            prev_state = torch.einsum(\"bts, bsc -> bsc\", 1 - integrator, state) if state is not None else 0 # (B, S, C)\n",
    "            state = prev_state + states # (B, S, C)\n",
    "\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        k = self.key(x)   # (B, T, C)\n",
    "        v = self.value(x) # (B, T, C)\n",
    "\n",
    "        if kv_cache is not None: # never in training mode\n",
    "            k_past, v_past = kv_cache\n",
    "            if k_past is not None:\n",
    "                k = torch.cat((k_past, k), dim=1)\n",
    "                v = torch.cat((v_past, v), dim=1)\n",
    "            if k.shape[1] > self.window_len - 1:\n",
    "                k = k[:, -self.window_len:]\n",
    "                v = v[:, -self.window_len:]\n",
    "            kv_cache = (k, v) \n",
    "        T_k = k.shape[1]\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        if T > 1:\n",
    "            scores = attend_folded_all_keys_triton(q, k, states, W) * C ** -0.5 # (B, T, S+W)\n",
    "            scores += alibi[:T_k]\n",
    "            scores = scores.masked_fill(self.mask[:T] == 0, float('-inf')) \n",
    "            scores = F.softmax(scores, dim=-1) # (B, T, S+W)\n",
    "            out = accumulate_folded_all_values_triton(scores, v, states, W) # (B, T, C)\n",
    "        else:\n",
    "            # q is (B, 1, C), k/v are (B, T_past+1, C), state is (B, S, C) \n",
    "            all_keys = torch.cat((state, k), dim=1) # (B, S+T_past+1, C)\n",
    "            all_values = torch.cat((state, v), dim=1) # (B, S+T_past+1, C)\n",
    "            scores = torch.einsum(\"boc, bwc -> bw\", q, all_keys) * C ** -0.5 # (B, S+T_past+1)\n",
    "            scores += alibi[-1] # (B, S+T_past+1) TODO: indexing for T_k < window_len\n",
    "            scores = F.softmax(scores, dim=-1)\n",
    "            out = torch.einsum(\"bw, bwc -> bc\", scores, all_values).reshape(B, 1, C)\n",
    "        \n",
    "        return out, state, kv_cache\n",
    "\n",
    "class MultiHeadSCAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SCAttentionHead(config) for _ in range(config.head_num)])\n",
    "        self.o = nn.Linear(config.head_num*config.embed_size//config.head_num, config.embed_size)\n",
    "\n",
    "    def forward(self, x, alibi, kv_cache=None, state=None):\n",
    "        head_outs = [h(x, alibi[i], None if kv_cache is None else kv_cache[i], None if state is None else state[:, i]) for i, h in enumerate(self.heads)]\n",
    "        kv_cache = [h[2] for h in head_outs]\n",
    "        state = torch.stack([h[1] for h in head_outs], dim=1) if state is not None else None\n",
    "        out = torch.cat([h[0] for h in head_outs], dim=-1) # concat single-head results\n",
    "        out = self.o(out)\n",
    "        return out, state, kv_cache\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(config.embed_size, config.embed_size*4)\n",
    "        self.lin_2 = nn.Linear(config.embed_size*4, config.embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin_1(x) # (B, T, C*multiplier)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin_2(x) # (B, T, C)\n",
    "        return x\n",
    "\n",
    "class SCANBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.sa_heads = MultiHeadSCAttention(config)\n",
    "        self.ff_layer = FeedForward(config)\n",
    "        self.sa_norm = RMSNorm(config.embed_size)\n",
    "        self.ff_norm = RMSNorm(config.embed_size)\n",
    "    \n",
    "    def forward(self, x, alibi, kv_cache=None, state=None):\n",
    "        a, state, kv_cache = self.sa_heads(x, alibi, kv_cache, state)\n",
    "        h = x + self.sa_norm(a)\n",
    "        o = h + self.ff_norm(self.ff_layer(h))\n",
    "        return o, state, kv_cache\n",
    "    \n",
    "class SCANTransformerLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_num = config.layer_num\n",
    "        self.head_num = config.head_num\n",
    "        self.seq_len = config.seq_len\n",
    "        self.window_len = config.window_len\n",
    "        self.head_size = config.embed_size // config.head_num\n",
    "        self.state_size = config.state_size\n",
    "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.embed_size)\n",
    "        # Language Modelling (?) Head is a standard linear layer to go from \n",
    "        # embeddings back to logits of vocab_size\n",
    "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size)\n",
    "        # transformer blocks\n",
    "        self.blocks = nn.ModuleList([SCANBlock(config) for _ in range(config.layer_num)])\n",
    "        # precompute AliBi tensor\n",
    "        self.alibi = build_alibi_tensor_SCAN(config.head_num, config.seq_len, config.window_len, config.state_size) \n",
    "\n",
    "    def forward(self, idx, targets=None, kv_cache=None, states=None):\n",
    "        train = targets is not None\n",
    "        B, T = idx.shape\n",
    "        if train: \n",
    "            assert T == self.seq_len\n",
    "            assert T % self.window_len == 0\n",
    "            x = self.token_embedding_table(idx)\n",
    "            # go through blocks\n",
    "            alibi = self.alibi\n",
    "            for i, block in enumerate(self.blocks):\n",
    "                x, _, _ = block(x, alibi, None, None)\n",
    "            logits = self.lm_head(x)\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            _, T_past, _ = kv_cache[0][0][0].shape if kv_cache is not None and kv_cache[0][0][0] is not None else (0, 0, 0)\n",
    "            x = self.token_embedding_table(idx)\n",
    "            _, T_now, _ = x.shape\n",
    "            # go through blocks\n",
    "            alibi = self.alibi\n",
    "            for i, block in enumerate(self.blocks):\n",
    "                x, state, cache = block(x, alibi, None if kv_cache is None else kv_cache[i], states[:, i])\n",
    "                if kv_cache is not None:\n",
    "                    kv_cache[i] = cache\n",
    "                states[:, i] = state\n",
    "            logits = self.lm_head(x) # this will only be the last window_len tokens\n",
    "            loss = None\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1, use_cache=True):\n",
    "        if use_cache:\n",
    "            # initialize key-value cache\n",
    "            kv_cache = [[(None, None) for _ in range(self.head_num)] for _ in range(self.layer_num)]\n",
    "            # initialize states\n",
    "            states = torch.zeros(1, self.layer_num, self.head_num, self.state_size, self.head_size).to(idx.device)\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            B, T = idx.shape\n",
    "            # now if T > window_len, we need to do one window_len forward pass first, then pass the rest of the tokens one by one to prefill\n",
    "            # I'll come up with a better way to do this later\n",
    "            if T > self.window_len:\n",
    "                idx_context = idx[:, :self.window_len] # just the first pass\n",
    "                n_passes = T-self.window_len\n",
    "                for i in range(n_passes):\n",
    "                    _, _ = self(idx_context, kv_cache=kv_cache, states=states)\n",
    "                    idx_context = idx[:, self.window_len+i:self.window_len+i+1]\n",
    "            else:\n",
    "                idx_context = idx\n",
    "\n",
    "            for _ in range(max_new_tokens):\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context, kv_cache=kv_cache, states=states)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "                # since we have kv cache, only need to pass new token\n",
    "                idx_context = idx_next\n",
    "            return idx\n",
    "        else:\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            idx_context = idx[:, -self.seq_len:]\n",
    "            for _ in range(max_new_tokens):\n",
    "                # get the predictions\n",
    "                logits, _ = self(idx_context)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "                # crop idx to the last window_len tokens\n",
    "                idx_context = idx[:, -self.window_len:]\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "688983"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test forward pass\n",
    "config = SCANTransformerConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    seq_len=seq_len,\n",
    "    window_len=64,\n",
    "    embed_size=128,\n",
    "    head_num=2,\n",
    "    layer_num=3,\n",
    "    state_size=32\n",
    ")\n",
    "# config = SCANTransformerConfig(\n",
    "#     vocab_size=vocab_size,\n",
    "#     seq_len=seq_len,\n",
    "#     window_len=128,\n",
    "#     embed_size=256,\n",
    "#     head_num=2,\n",
    "#     layer_num=6,\n",
    "#     state_size=32\n",
    "# )tate_size=32\n",
    "m = SCANTransformerLM(config)\n",
    "m.to(device)\n",
    "xb, yb = get_batch('train', seq_len, 1)\n",
    "logits, loss = m(xb, yb)\n",
    "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e5b46a9d854011a26fa79ad83f12f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss: 1.337822675704956 final val loss: 1.3927876472473144\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training!\n",
    "model = SCANTransformerLM(config)\n",
    "# model = torch.compile(model)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "train(model, optimizer, seq_len, batch_size, total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), 'models/TritonSCANTransformerLM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khalid Zuhri\\AppData\\Local\\Temp\\ipykernel_2936\\994828852.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('models/TritonSCANTransformerLM.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SCANTransformerLM(\n",
       "  (token_embedding_table): Embedding(87, 128)\n",
       "  (lm_head): Linear(in_features=128, out_features=87, bias=True)\n",
       "  (blocks): ModuleList(\n",
       "    (0-2): 3 x SCANBlock(\n",
       "      (sa_heads): MultiHeadSCAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-1): 2 x SCAttentionHead(\n",
       "            (stator): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (integrator): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (key): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=64, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff_layer): FeedForward(\n",
       "        (lin_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (lin_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (sa_norm): RMSNorm()\n",
       "      (ff_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = SCANTransformerLM(config)\n",
    "# model = torch.compile(model)\n",
    "model.load_state_dict(torch.load('models/TritonSCANTransformerLM.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75,  1, 59, 62,  1, 58, 76,  1, 64, 72, 72, 61,  1, 58, 76,  1, 70, 62, 10,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0, 37,  1, 65, 58, 79, 62,  1, 77, 75, 58, 66, 71, 62, 61,  1, 63, 72, 75,  1, 77, 65, 72, 78, 76, 58, 71]])\n",
      "You will never be as good as me.\n",
      "             \n",
      "I have trained for thousand that phone to me all.\n",
      "\n",
      "She was always having to the door a night of the station should be in the same way.\n",
      "\n",
      "I was so looking for you need to help my moment so far away.\n",
      "\n",
      "If you don't make it in my mother way.\n",
      "\n",
      "I don't think you two many curitious and do it.\n",
      "\n",
      "You're being so much then she's been leavily to sell the student.\n",
      "\n",
      "I think it's so that you were hope that way.\n",
      "\n",
      "It's the meeting on the power, right?\n",
      "\n",
      "I think she doesn't have to think every search of the Magic Knights Shimori Pressure Marie Comparies her is here.\n",
      "\n",
      "I'm sure it being started to the start to be a step beautiful.\n",
      "\n",
      "I was so suddenly happening and the professor collect serves that continue with the time in our place.\n",
      "\n",
      "The short of the seconds of the news that would be where it that would be able to be the cursed by his time.\n",
      "\n",
      "The bell over the truth the end the whole said of the battle said that was a strength to do that.\n",
      "\n",
      "Now, that's why I was supposed to talk to her about.\n",
      "\n",
      "You should do around this power to use a \n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "idx = encode(\"\"\"You will never be as good as me.\n",
    "             \n",
    "I have trained for thousan\"\"\")\n",
    "print(torch.tensor([idx]))\n",
    "print(decode(model.generate(idx=torch.tensor([idx], dtype=torch.long).to(device), max_new_tokens=1000, temperature=0.5, use_cache=True)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerConfig:\n",
    "    def __init__(self, vocab_size, seq_len, embed_size, head_num, layer_num):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = embed_size\n",
    "        self.head_num = head_num\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "def build_alibi_tensor(head_num: int, seq_len: int) -> torch.Tensor:\n",
    "    # Create position indices\n",
    "    pos = torch.arange(seq_len, device=device)\n",
    "    # Create slopes for each head: 2^(-8i/h) for i in [0,h)\n",
    "    slopes = torch.pow(2, -8.0 * torch.arange(head_num, device=device) / head_num)\n",
    "    # Compute relative positions using broadcasting\n",
    "    # This creates a seq_len x seq_len matrix of position differences\n",
    "    relative_pos = pos[None, :] - pos[:, None]  # shape: (seq_len, seq_len)\n",
    "    # Multiply slopes by relative positions using broadcasting\n",
    "    # slopes[:, None, None] creates a (head_num, 1, 1) tensor\n",
    "    # relative_pos[None, :, :] creates a (1, seq_len, seq_len) tensor\n",
    "    alibi = slopes[:, None, None] * relative_pos[None, :, :]\n",
    "    return alibi\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "class SelfAttentionHead(nn.Module):\n",
    "    \"\"\" one head of self-attention with AliBi \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seq_len = config.seq_len\n",
    "        self.key = nn.Linear(config.embed_size, config.embed_size//config.head_num, bias=False)\n",
    "        self.query = nn.Linear(config.embed_size, config.embed_size//config.head_num, bias=False)\n",
    "        self.value = nn.Linear(config.embed_size, config.embed_size//config.head_num, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(config.seq_len, config.seq_len)))\n",
    "\n",
    "    def forward(self, x, alibi, kv_cache=None):\n",
    "        B, T, C = x.shape\n",
    "        _, T_past, _ = kv_cache[0].shape if kv_cache is not None and kv_cache[0] is not None else (0, 0, 0)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            k_past, v_past = kv_cache\n",
    "            if k_past is not None:\n",
    "                k = torch.cat((k_past, k), dim=1)\n",
    "                v = torch.cat((v_past, v), dim=1)\n",
    "            if k.shape[1] > self.seq_len:\n",
    "                k = k[:, -self.seq_len:]\n",
    "                v = v[:, -self.seq_len:]\n",
    "                alibi = alibi[-self.seq_len:, -self.seq_len:]\n",
    "            kv_cache = (k, v)\n",
    "\n",
    "        T_k = k.shape[1]\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei * C**-0.5 # scaled attention\n",
    "        wei = wei + alibi[-T:, -T_k:] # add AliBi\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "\n",
    "        # perform the weighted aggregation of the values\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out, kv_cache\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(config.embed_size, config.embed_size*4)\n",
    "        self.lin_2 = nn.Linear(config.embed_size*4, config.embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin_2(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention with AliBi in parallel \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(config) for _ in range(config.head_num)])\n",
    "        self.o = nn.Linear(config.embed_size, config.embed_size)\n",
    "\n",
    "    def forward(self, x, alibi, kv_cache=None):\n",
    "        head_outs = [h(x, alibi[i], None if kv_cache is None else kv_cache[i]) for i, h in enumerate(self.heads)]\n",
    "        kv_cache = [h[1] for h in head_outs]\n",
    "        out = torch.cat([h[0] for h in head_outs], dim=-1) # concat single-head results\n",
    "        out = self.o(out)\n",
    "        return out, kv_cache\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.sa_heads = MultiHeadAttention(config)\n",
    "        self.ff_layer = FeedForward(config)\n",
    "        self.sa_norm = RMSNorm(config.embed_size)\n",
    "        self.ff_norm = RMSNorm(config.embed_size)\n",
    "    \n",
    "    def forward(self, x, alibi, kv_cache=None):\n",
    "        a, kv_cache = self.sa_heads(self.sa_norm(x), alibi, kv_cache)\n",
    "        h = x + a\n",
    "        o = h + self.ff_layer(self.ff_norm(h))\n",
    "        return o, kv_cache\n",
    "    \n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_num = config.layer_num\n",
    "        self.head_num = config.head_num\n",
    "        self.seq_len = config.seq_len\n",
    "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.embed_size)\n",
    "        # Language Modelling (?) Head is a standard linear layer to go from \n",
    "        # embeddings back to logits of vocab_size\n",
    "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size)\n",
    "        # transformer blocks\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.layer_num)])\n",
    "        # precompute AliBi tensor\n",
    "        self.alibi = build_alibi_tensor(config.head_num, config.seq_len)\n",
    "\n",
    "    def forward(self, idx, targets=None, kv_cache=None):\n",
    "        B, T = idx.shape\n",
    "        _, T_past, _ = kv_cache[0][0][0].shape if kv_cache is not None and kv_cache[0][0][0] is not None else (0, 0, 0)\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        x = tok_embd\n",
    "        # go through blocks\n",
    "        alibi = self.alibi[:, :T_past+T, :T_past+T]\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, cache = block(x, alibi, None if kv_cache is None else kv_cache[i])\n",
    "            if kv_cache is not None:\n",
    "                kv_cache[i] = cache\n",
    "        # get logits with linear layer\n",
    "        logits = self.lm_head(x) # (B,T,C)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1, use_cache=True):\n",
    "        if use_cache:\n",
    "            # initialize key-value cache\n",
    "            kv_cache = [[(None, None) for _ in range(self.head_num)] for _ in range(self.layer_num)]\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            # crop idx to the last seq_len tokens\n",
    "            idx_context = idx[:, -self.seq_len:]\n",
    "            for _ in range(max_new_tokens):\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context, kv_cache=kv_cache)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "                # since we have kv cache, only need to pass new token\n",
    "                idx_context = idx_next\n",
    "            return idx\n",
    "        else:\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            for _ in range(max_new_tokens):\n",
    "                #crop idx to the last seq_len tokens\n",
    "                idx_context = idx[:, -self.seq_len:]\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615255"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test forward pass\n",
    "config = TransformerConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    seq_len=seq_len,\n",
    "    embed_size=128,\n",
    "    head_num=2,\n",
    "    layer_num=3\n",
    ")\n",
    "m = TransformerLM(config)\n",
    "m.to(device)\n",
    "xb, yb = get_batch('train', 5, 1)\n",
    "logits, loss = m(xb, yb)\n",
    "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b426cabcbb36446a9640af7f91d42f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss: 1.2193772792816162 final val loss: 1.3547022700309754\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training!\n",
    "model = TransformerLM(config)\n",
    "# model = torch.compile(model)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "train(model, optimizer, seq_len, batch_size, total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), 'models/TransformerLM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khalid Zuhri\\AppData\\Local\\Temp\\ipykernel_2936\\215420273.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('models/TransformerLM.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerLM(\n",
       "  (token_embedding_table): Embedding(87, 128)\n",
       "  (lm_head): Linear(in_features=128, out_features=87, bias=True)\n",
       "  (blocks): ModuleList(\n",
       "    (0-2): 3 x Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-1): 2 x SelfAttentionHead(\n",
       "            (key): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=64, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff_layer): FeedForward(\n",
       "        (lin_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (lin_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (sa_norm): RMSNorm()\n",
       "      (ff_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = TransformerLM(config)\n",
    "# model = torch.compile(model)\n",
    "model.load_state_dict(torch.load('models/TransformerLM.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75,  1, 59]])\n",
      "You will never be taken back to the proper.\n",
      "\n",
      "I want to know the enemy will be working to surprise.\n",
      "\n",
      "I was so so for her.\n",
      "\n",
      "I want to say that at the most important that the second that matter words of that.\n",
      "\n",
      "It's okay home to move to the sun from the most place.\n",
      "\n",
      "You have something to do it too much of a terrifying at the money.\n",
      "\n",
      "You're the one who took the way to play the worlds with the selfier of the face.\n",
      "\n",
      "I'm not completely from the truth.\n",
      "\n",
      "I want to get back at all this desire to the world.\n",
      "\n",
      "I don't like that this way.\n",
      "\n",
      "I didn't have a lot of places!\n",
      "\n",
      "That was the sucked of the other way to the moon.\n",
      "\n",
      "I won't let you to get the same time, and let me to the first time left.\n",
      "\n",
      "But there were they something the way before the trainer,\n",
      "\n",
      "the operation put the best to me in the last situation.\n",
      "\n",
      "The world said that she was a man who already surface the wall of the follow that the two of the witch with the last time.\n",
      "\n",
      "You are the same as a lot of it like the master world and fall for the world.\n",
      "\n",
      "The littl\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "idx = encode(\"You will never b\")\n",
    "print(torch.tensor([idx]))\n",
    "print(decode(model.generate(idx=torch.tensor([idx], dtype=torch.long).to(device), max_new_tokens=1000, temperature=0.5, use_cache=True)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
