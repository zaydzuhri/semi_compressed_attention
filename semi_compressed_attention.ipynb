{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sdtDsu1Y0EqL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "# os.environ[\"TRITON_INTERPRET\"] = \"0\"\n",
    "torch.manual_seed(69)\n",
    "torch.set_printoptions(profile=\"short\", sci_mode=False, linewidth=100000)\n",
    "# this script is configured to run on a RTX 3060 12GB GPU. you'll want to adjust the model sizes and batch sizes for other devices\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "plt.rcParams['figure.dpi'] = 50\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['xtick.minor.visible'] = True\n",
    "plt.rcParams['ytick.minor.visible'] = True\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANr5dn7W0EqR",
    "outputId": "a00cbe8d-e1c6-454b-b552-4ffd17ce17e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  39526018\n"
     ]
    }
   ],
   "source": [
    "# we use this 40mb file of concatenated anime subtitles as our dataset\n",
    "# just the right size for toy experiments like this I think\n",
    "with open('animesubs.txt', 'r', encoding='latin') as f:\n",
    "    text = f.read()\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pANiObIZ0EqU",
    "outputId": "98f70469-1c89-4341-89e8-c142a14331b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open your mind. Open your mind.\n",
      "\n",
      "Far beyond the deep blue Earth, you and I shall meet...\n",
      "\n",
      "AH! MY GODDESS\n",
      "\n",
      "A snow-white feather comes fluttering down, swaying gently in the air.\n",
      "\n",
      "Without holding back, I want to envelope you, my one and only love.\n",
      "\n",
      "I know I have the power to protect the one I love, right here in my hands.\n",
      "\n",
      "Open your mind. Just as I've always dreamed.\n",
      "\n",
      "Let the wind carry off your hopes, faraway.\n",
      "\n",
      "I have wings nobody can see. Look, you have them, too.\n",
      "\n",
      "They'll take us to where we ca\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WvM5h6_i3KsM"
   },
   "outputs": [],
   "source": [
    "# remove japanese characters\n",
    "text = ''.join(filter(lambda character:ord(character) < 0x3000, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7SOcWJM0EqW",
    "outputId": "a82a4da8-a8ed-47bf-cfb5-2e4c59dee2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters: 86 \n",
      " !'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|Â”\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"unique characters:\", vocab_size, ''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yobmmaeK0EqX",
    "outputId": "26669f38-4b26-4b53-f642-ee7543e7c578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, '*': 6, '+': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '<': 24, '=': 25, '>': 26, '?': 27, '@': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, '[': 55, ']': 56, '_': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63, 'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70, 'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77, 'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, '|': 84, '\\x94': 85, '': 86}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: '*', 7: '+', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '<', 25: '=', 26: '>', 27: '?', 28: '@', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '|', 85: '\\x94', 86: ''}\n",
      "encoded: [43, 73, 62, 71, 1, 82, 72, 78, 75, 1, 70, 66, 71, 61, 10, 1, 43, 73, 62, 71]\n",
      "decoded: Open your mind. Open\n",
      "vocab size: 87\n"
     ]
    }
   ],
   "source": [
    "# yes, all language models will be character level, which isn't ideal but it's good for simplicity\n",
    "# very simple tokenizer\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "# add special token for padding\n",
    "stoi[''] = len(stoi)\n",
    "itos[len(itos)] = ''\n",
    "print(stoi)\n",
    "print(itos)\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "print(\"encoded:\", encode(text[:20]))\n",
    "print(\"decoded:\", decode(encode(text[:20])))\n",
    "vocab_size = len(itos)\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Pnf9KfP0EqY",
    "outputId": "ff581168-335a-4f0f-efeb-0d4bd5b225ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39526018])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZ2fY1pR0EqY",
    "outputId": "5eb599e0-fb79-4cdb-c4b9-52a781d67151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  1, 43, 73, 62, 71,  1, 82, 72, 78, 75,  1, 70, 66, 71, 61, 10,  0,  0, 34, 58, 75,  1, 59, 62, 82, 72, 71, 61,  1, 77, 65, 62,  1, 61, 62, 62, 73,  1, 59, 69, 78, 62,  1, 33, 58, 75, 77, 65,  8,  1, 82, 72, 78,  1, 58, 71, 61,  1, 37,  1, 76, 65, 58, 69, 69,  1, 70, 62, 62, 77, 10, 10, 10,  0,  0, 29, 36,  2,  1, 41, 53,  1, 35, 43, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIaYesPh0Eqa",
    "outputId": "caa27cbb-5ae3-4406-8194-646264d4ba99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39130757]) torch.Size([395261])\n"
     ]
    }
   ],
   "source": [
    "n = int(len(data) * 0.99)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bFhizcI0Eqa",
    "outputId": "c93a4d43-6fb2-4de7-aefc-8fed47e4a861"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 73, 62, 71,  1, 82, 72, 78, 75])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 8\n",
    "train_data[:seq_len+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skFCPvQC0Eqc",
    "outputId": "08990c5b-88af-4a51-ce37-3c59989d6d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([2, 64])\n",
      "tensor([[64,  1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71],\n",
      "        [62, 70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([2, 64])\n",
      "tensor([[ 1, 72, 78, 75,  1, 66, 71, 77, 62, 75, 71, 76, 65, 66, 73, 76, 10,  0,  0, 48, 65, 62,  1, 71, 66, 64, 65, 77,  1, 37, 66, 61, 58,  1, 64, 72, 77,  1, 77, 65, 62,  1, 75, 62, 76, 78, 69, 77, 76,  1, 72, 63,  1, 77, 65, 62,  1, 62, 81, 58, 70, 66, 71, 58],\n",
      "        [70,  2,  0,  0, 37, 77,  1, 69, 72, 72, 68, 76,  1, 69, 66, 68, 62,  1, 80, 62,  1, 77, 72, 72, 68,  1, 60, 58, 75, 62,  1, 72, 63,  1, 77, 65, 62, 70, 10,  0,  0, 48, 65, 72, 76, 62,  1, 64, 78, 82, 76,  1, 80, 62, 75, 62,  1, 66, 71, 60, 75, 62, 61, 66]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, seq_len, batch_size=4):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    # targets are just inputs shifted by 1\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = get_batch('train', 64, 2)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81920000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make all steps, sequence lengths, and batch size the same\n",
    "total_steps = 5000\n",
    "seq_len = 2048\n",
    "batch_size = 8 # these are small models so we can use large batch sizes to fully utilize the GPU\n",
    "# should cover around 2x the dataset\n",
    "total_steps * seq_len * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, seq_len, batch_size, total_steps, val_steps=10, val_interval=50):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    # live plot\n",
    "    fig, ax = plt.subplots()\n",
    "    dh = display.display(fig, display_id=True)\n",
    "    for steps in (bar := tqdm(range(total_steps))):  # increase number of steps for good results...\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train', seq_len=seq_len, batch_size=batch_size)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bar.set_description(f\"loss: {loss.item():.2f}, val loss: {val_losses[-1] if val_losses else 0:.2f}\")\n",
    "        losses.append(loss.item())\n",
    "        if steps % val_interval == 0:\n",
    "            # Calculate validation loss\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for _ in range(val_steps):\n",
    "                    xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "                    _, loss = model(xb, yb)\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= val_steps\n",
    "                val_losses.append(val_loss)\n",
    "            ax.clear()\n",
    "            ax.plot(losses, color='blue', label='train loss', alpha=0.7)\n",
    "            ax.plot(range(0, len(losses), val_interval), val_losses, color='red', label='val loss', alpha=0.7)\n",
    "            ax.set_ylim(0, 4)\n",
    "            ax.legend()\n",
    "            dh.update(fig)\n",
    "    print('final loss:', loss.item(), 'final val loss:', val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure post training perplexity on validation set\n",
    "# Create function that receives a model, context length, and PPL sequence length, and returns the perplexity\n",
    "# The PPL sequence length is the number of characters the function uses to calculate the perplexity\n",
    "# We take the logits and calculate the cross entropy loss from scratch, then exponentiate it to get the perplexity\n",
    "# not only that, but we want the models to do this in actual inference\n",
    "def perplexity(model, seq_len, ppl_seq_len, batch_size=128):\n",
    "    with torch.no_grad():\n",
    "        val_steps = 10\n",
    "        val_loss = 0\n",
    "        for _ in range(val_steps):\n",
    "            xb, yb = get_batch('val', seq_len=seq_len, batch_size=batch_size)\n",
    "            logits, _ = model(xb, yb)\n",
    "            logits = logits.reshape(batch_size, seq_len, vocab_size)\n",
    "            logits = logits[:, :ppl_seq_len]\n",
    "            yb = yb[:, :ppl_seq_len]\n",
    "            # flatten logits and targets\n",
    "            logits = logits.reshape(batch_size*ppl_seq_len, vocab_size)\n",
    "            yb = yb.reshape(batch_size*ppl_seq_len)\n",
    "            # calculate cross entropy loss from scratch\n",
    "            loss = F.cross_entropy(logits, yb)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= val_steps\n",
    "        ppl = torch.exp(torch.tensor(val_loss))\n",
    "        return ppl.item(), val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Compressed AttentioN (SCAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attend Folded Keys Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_fold(x, window_len):\n",
    "    # x is a (B, T, C) tensor\n",
    "    # output should be a (B, T, window_len, C) tensor that slides over the T dimension\n",
    "    # first window_len-1 elements will have to be padded with zeros in the beginning\n",
    "    # example: x = torch.tensor([[[1,2],[3,4],[5,6],[7,8],[9,10]]])\n",
    "    # sliding_window_fold(x, 2) -> torch.tensor([[[[0,1],[1,2]],[[1,2],[3,4]],[[3,4],[5,6]],[[5,6],[7,8]],[[7,8],[9,10]]]])\n",
    "    padded_x = F.pad(x, (0, 0, window_len - 1, 0), mode='constant', value=0)\n",
    "    output = padded_x.unfold(dimension=1, size=window_len, step=1).permute(0, 1, 3, 2)\n",
    "    return output\n",
    "\n",
    "def attend_folded_all_keys_torch(q, k, states, W):\n",
    "    k = sliding_window_fold(k, W) # (B, T, W, C)\n",
    "    all_keys = torch.cat((states, k), dim=2) # (B, T, S, C) + (B, T, W, C) -> (B, T, S+W, C)\n",
    "    scores = torch.einsum(\"btc, btxc -> btx\", q, all_keys) # (B, T, S+W)\n",
    "    return scores\n",
    "\n",
    "def accumulate_folded_all_values_torch(s, v, states, W):\n",
    "    v = sliding_window_fold(v, W) # (B, T, W, C)\n",
    "    all_values = torch.cat((states, v), dim=2) # (B, T, S, C) + (B, T, W, C) -> (B, T, S+W, C)\n",
    "    out = torch.einsum(\"btx, btxc -> btc\", s, all_values) # (B, T, C)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triton kernel\n",
    "@triton.jit\n",
    "def afak_fwd_kernel(\n",
    "    q_ptr, k_ptr, states_ptr, y_ptr,\n",
    "    B: tl.constexpr, T: tl.constexpr, S:tl.constexpr, C: tl.constexpr, W: tl.constexpr,\n",
    "    BLOCK_SIZE_S: tl.constexpr,\n",
    "    BLOCK_SIZE_W: tl.constexpr,\n",
    "):\n",
    "    # Use multiple program IDs for better parallelization\n",
    "    b_id = tl.program_id(axis=0)\n",
    "    t_id = tl.program_id(axis=1)\n",
    "    sw_block_id = tl.program_id(axis=2)\n",
    "    num_s_blocks = triton.cdiv(S, BLOCK_SIZE_S)\n",
    "    num_w_blocks = triton.cdiv(W, BLOCK_SIZE_W)\n",
    "    SW = S + W\n",
    "\n",
    "    # Compute base pointers\n",
    "    q_base = q_ptr + b_id * T * C\n",
    "    k_base = k_ptr + b_id * T * C\n",
    "    states_base = states_ptr + b_id * T * S * C\n",
    "    y_base = y_ptr + b_id * T * W\n",
    "\n",
    "    # Fetch the query at [b_id, t_id, :]\n",
    "    q_block_ptr = tl.make_block_ptr(\n",
    "        base=q_ptr,\n",
    "        shape=(B, T, C),\n",
    "        strides=(T * C, C, 1),\n",
    "        offsets=(b_id, t_id, 0),\n",
    "        block_shape=(1, 1, C),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    q = tl.load(q_block_ptr) # (1, 1, C)\n",
    "\n",
    "    if sw_block_id < num_s_blocks:\n",
    "        s_first_id = sw_block_id * BLOCK_SIZE_S\n",
    "        # Fetch the states at [b_id, t_id, s_first_id:s_first_id+BLOCK_SIZE_S, :]\n",
    "        s_block_ptr = tl.make_block_ptr(\n",
    "            base=states_ptr,\n",
    "            shape=(B, T, S, C),\n",
    "            strides=(T * S * C, S * C, C, 1),\n",
    "            offsets=(b_id, t_id, s_first_id, 0),\n",
    "            block_shape=(1, 1, BLOCK_SIZE_S, C),\n",
    "            order=(0, 1, 2, 3),\n",
    "        )\n",
    "        s = tl.load(s_block_ptr) # (1, 1, BLOCK_SIZE_S, C)\n",
    "        o = q[:, :, None, :] * s # (1, 1, BLOCK_SIZE_S, C)\n",
    "        o = tl.sum(o, axis=-1) # (1, 1, BLOCK_SIZE_S)\n",
    "        # Store the result\n",
    "        y_block_ptr = tl.make_block_ptr(\n",
    "            base=y_ptr,\n",
    "            shape=(B, T, SW),\n",
    "            strides=(T * SW, SW, 1),\n",
    "            offsets=(b_id, t_id, s_first_id),\n",
    "            block_shape=(1, 1, BLOCK_SIZE_S),\n",
    "            order=(0, 1, 2),\n",
    "        )\n",
    "        tl.store(y_block_ptr, o) # (1, 1, BLOCK_SIZE_S)\n",
    "    else:\n",
    "        w_first_id = (sw_block_id - num_s_blocks) * BLOCK_SIZE_W\n",
    "        # Fetch the key at [b_id, t_id-W+1+(w_block_id*BLOCK_SIZE_W):t_id+(w_block_id*BLOCK_SIZE_W), :]\n",
    "        # need to load the keys manually because make_block_ptr doesn't support masks\n",
    "        tw_offs = tl.arange(0, BLOCK_SIZE_W)\n",
    "        c_offs = tl.arange(0, C)\n",
    "        k_block_ptr = k_base + (t_id - W + 1 + (w_first_id + tw_offs[:, None])) * C + c_offs[None, :]\n",
    "        mask = w_first_id + tl.arange(0, BLOCK_SIZE_W)[:, None] > (W - t_id - 2)\n",
    "        k = tl.load(k_block_ptr, mask=mask) # (BLOCK_SIZE_W, C)\n",
    "        # Compute the dot product (but not with tl.dot because it has a minimum size of 16)\n",
    "        y = q * k[None, :] # (1, BLOCK_SIZE_W, C)\n",
    "        y = tl.sum(y, axis=-1) # (1, BLOCK_SIZE_W)\n",
    "        # Store the result\n",
    "        y_block_ptr = tl.make_block_ptr(\n",
    "            base=y_ptr,\n",
    "            shape=(B, T, SW),\n",
    "            strides=(T * SW, SW, 1),\n",
    "            offsets=(b_id, t_id, S + w_first_id),\n",
    "            block_shape=(1, 1, BLOCK_SIZE_W),\n",
    "            order=(0, 1, 2),\n",
    "        )\n",
    "        tl.store(y_block_ptr, y[None, :]) # (1, 1, BLOCK_SIZE_W)\n",
    "\n",
    "@triton.jit\n",
    "def afak_bwd_kernel(\n",
    "    q_ptr, k_ptr, states_ptr, dy_ptr, dq_ptr, dk_ptr, ds_ptr,\n",
    "    B: tl.constexpr, T: tl.constexpr, S: tl.constexpr, C: tl.constexpr, W: tl.constexpr,\n",
    "    BLOCK_SIZE_C: tl.constexpr,\n",
    "):\n",
    "    # Use multiple program IDs for better parallelization\n",
    "    b_id = tl.program_id(axis=0)\n",
    "    t_id = tl.program_id(axis=1)\n",
    "    c_block_id = tl.program_id(axis=2)\n",
    "    c_first_id = c_block_id * BLOCK_SIZE_C\n",
    "    SW = S + W\n",
    "\n",
    "    # Compute base pointers\n",
    "    q_base = q_ptr + b_id * T * C\n",
    "    k_base = k_ptr + b_id * T * C\n",
    "    dy_base = dy_ptr + b_id * T * SW\n",
    "    dq_base = dq_ptr + b_id * T * C\n",
    "    dk_base = dk_ptr + b_id * T * C\n",
    "\n",
    "    # First calculate the gradients for q\n",
    "    # Fetch original keys at [b_id, t_id-W+1:t_id, c_first_id:c_first_id+BLOCK_SIZE_C]\n",
    "    # using a block ptr also disallows the use of masks when loading, so let's just make a ptr manually\n",
    "    tw_offs = tl.arange(0, W)\n",
    "    c_offs = tl.arange(0, BLOCK_SIZE_C)\n",
    "    k_block_ptr = k_base + (t_id - W + 1 + tw_offs[:, None]) * C + c_first_id + c_offs[None, :]\n",
    "    mask = tl.arange(0, W)[:, None] > (W - t_id - 2)\n",
    "    k = tl.load(k_block_ptr, mask=mask) # (W, BLOCK_SIZE_C)\n",
    "    # Fetch output gradients at [b_id, t_id, S:W]\n",
    "    dy_block_ptr = tl.make_block_ptr(\n",
    "        base=dy_ptr,\n",
    "        shape=(B, T, SW),\n",
    "        strides=(T * SW, SW, 1),\n",
    "        offsets=(b_id, t_id, S),\n",
    "        block_shape=(1, 1, W),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    dy = tl.load(dy_block_ptr) # (1, 1, W)\n",
    "    # Compute the gradients for q\n",
    "    dqk = dy.permute(0, 2, 1) * k[None, :] # (1, W, BLOCK_SIZE_C)\n",
    "    dqk = tl.sum(dqk, axis=1) # (1, BLOCK_SIZE_C)\n",
    "    # Then we also have to add the gradients from the states\n",
    "    # Fetch the states at [b_id, t_id, c_first_id:c_first_id+BLOCK_SIZE_C]\n",
    "    s_block_ptr = tl.make_block_ptr(\n",
    "        base=states_ptr,\n",
    "        shape=(B, T, S, C),\n",
    "        strides=(T * S * C, S * C, C, 1),\n",
    "        offsets=(b_id, t_id, 0, c_first_id),\n",
    "        block_shape=(1, 1, S, BLOCK_SIZE_C),\n",
    "        order=(0, 1, 2, 3),\n",
    "    )\n",
    "    s = tl.load(s_block_ptr) # (1, 1, S, BLOCK_SIZE_C)\n",
    "    # Fetch the output gradients at [b_id, t_id, :S]\n",
    "    dy_block_ptr = tl.make_block_ptr(\n",
    "        base=dy_ptr,\n",
    "        shape=(B, T, SW),\n",
    "        strides=(T * SW, SW, 1),\n",
    "        offsets=(b_id, t_id, 0),\n",
    "        block_shape=(1, 1, S),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    dy = tl.load(dy_block_ptr) # (1, 1, S)\n",
    "    # Compute the gradients for q\n",
    "    dqs = dy[:, :, :, None] * s # (1, 1, S, BLOCK_SIZE_C)\n",
    "    dqs = tl.sum(dqs, axis=2) # (1, 1, BLOCK_SIZE_C)\n",
    "    dq = dqk[None, :] + dqs # (1, 1, BLOCK_SIZE_C)\n",
    "    # Store the result\n",
    "    dq_block_ptr = tl.make_block_ptr(\n",
    "        base=dq_ptr,\n",
    "        shape=(B, T, C),\n",
    "        strides=(T * C, C, 1),\n",
    "        offsets=(b_id, t_id, c_first_id),\n",
    "        block_shape=(1, 1, BLOCK_SIZE_C),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    tl.store(dq_block_ptr, dq) # (1, 1, BLOCK_SIZE_C)\n",
    "\n",
    "    # Calculate the gradients for states while we're at it\n",
    "    # Fetch the query at [b_id, t_id, c_first_id:c_first_id+BLOCK_SIZE_C]\n",
    "    q_block_ptr = tl.make_block_ptr(\n",
    "        base=q_ptr,\n",
    "        shape=(B, T, C),\n",
    "        strides=(T * C, C, 1),\n",
    "        offsets=(b_id, t_id, c_first_id),\n",
    "        block_shape=(1, 1, BLOCK_SIZE_C),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    q = tl.load(q_block_ptr) # (1, 1, BLOCK_SIZE_C)\n",
    "    # Compute the gradients for states\n",
    "    ds = dy[:, :, :, None] * q[:, :, None, :] # (1, 1, S, BLOCK_SIZE_C)\n",
    "    # Store the result\n",
    "    ds_block_ptr = tl.make_block_ptr(\n",
    "        base=ds_ptr,\n",
    "        shape=(B, T, S, C),\n",
    "        strides=(T * S * C, S * C, C, 1),\n",
    "        offsets=(b_id, t_id, 0, c_first_id),\n",
    "        block_shape=(1, 1, S, BLOCK_SIZE_C),\n",
    "        order=(0, 1, 2, 3),\n",
    "    )\n",
    "    tl.store(ds_block_ptr, ds) # (1, 1, S, BLOCK_SIZE_C)\n",
    "\n",
    "    # Then calculate the gradients for k\n",
    "    # same thing here, let's just make the ptr manually\n",
    "    tw_offs = tl.arange(0, W)\n",
    "    c_offs = tl.arange(0, BLOCK_SIZE_C)\n",
    "    q_block_ptr = q_base + (t_id + tw_offs[:, None]) * C + c_first_id + c_offs[None, :]\n",
    "    mask = tl.arange(0, W)[:, None] < T - t_id\n",
    "    q = tl.load(q_block_ptr, mask=mask) # (W, C)\n",
    "    # Fetch original gradients at [b_id, t_id, :]\n",
    "    # This one is tricky bc we have to fetch a diagonal from dy\n",
    "    # going from [b_id, t_id, W] to [b_id, t_id+W, 0]\n",
    "    # only way to do this is to load the whole dy tensor and then mask it, then sum along the last axis\n",
    "    tw_offs = tl.arange(0, W)\n",
    "    w_offs = tl.arange(0, W)\n",
    "    dy_block_ptr = dy_base + (t_id + tw_offs[:, None]) * SW + S + w_offs[None, :]\n",
    "    mask = (tl.arange(0, W)[:, None] + tl.arange(0, W)[None, :] == W - 1) & (tl.arange(0, W)[:, None] < T - t_id)\n",
    "    dy = tl.load(dy_block_ptr, mask=mask) # (W, W)\n",
    "    dy = tl.sum(dy, axis=-1, keep_dims=True) # (W, 1)\n",
    "    # Compute the gradients for k\n",
    "    dk = dy * q\n",
    "    dk = tl.sum(dk, axis=0, keep_dims=True)\n",
    "    # Store the result\n",
    "    dk_block_ptr = tl.make_block_ptr(\n",
    "        base=dk_ptr,\n",
    "        shape=(B, T, C),\n",
    "        strides=(T * C, C, 1),\n",
    "        offsets=(b_id, t_id, c_first_id),\n",
    "        block_shape=(1, 1, BLOCK_SIZE_C),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    tl.store(dk_block_ptr, dk[None, :])\n",
    "    \n",
    "\n",
    "class AttendFoldedAllKeysTriton(torch.autograd.Function):\n",
    "    # @torch.compiler.disable\n",
    "    @staticmethod\n",
    "    def forward(ctx, q, k, states, W):\n",
    "        B, T, C = q.shape\n",
    "        B, T, S, C = states.shape\n",
    "        q = q.contiguous()\n",
    "        k = k.contiguous()\n",
    "        states = states.contiguous()\n",
    "        ctx.save_for_backward(q, k, states)\n",
    "        ctx.W = W\n",
    "        \n",
    "        # Configure block sizes\n",
    "        BLOCK_SIZE_S = 16\n",
    "        BLOCK_SIZE_W = 16\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        num_s_blocks = triton.cdiv(S, BLOCK_SIZE_S)\n",
    "        num_w_blocks = triton.cdiv(W, BLOCK_SIZE_W)\n",
    "        grid = (B, T, num_s_blocks+num_w_blocks)\n",
    "\n",
    "        # Allocate output tensor\n",
    "        y = torch.zeros((B, T, S+W), dtype=q.dtype, device=q.device).contiguous()\n",
    "        \n",
    "        # Launch kernel\n",
    "        afak_fwd_kernel[grid](\n",
    "            q, k, states, y,\n",
    "            B, T, S, C, W,\n",
    "            BLOCK_SIZE_S=BLOCK_SIZE_S,\n",
    "            BLOCK_SIZE_W=BLOCK_SIZE_W,\n",
    "        )\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # @torch.compiler.disable\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.contiguous()\n",
    "        q, k, states = ctx.saved_tensors\n",
    "        B, T, S, C = states.shape\n",
    "        W = ctx.W\n",
    "        \n",
    "        # Configure block sizes\n",
    "        BLOCK_SIZE_C = 16\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        num_c_blocks = triton.cdiv(C, BLOCK_SIZE_C)\n",
    "        grid = (B, T, num_c_blocks)\n",
    "        \n",
    "        gq = torch.zeros_like(q).contiguous()\n",
    "        gk = torch.zeros_like(k).contiguous()\n",
    "        gs = torch.zeros_like(states).contiguous()\n",
    "\n",
    "        # Launch kernel\n",
    "        afak_bwd_kernel[grid](\n",
    "            q, k, states, grad_output, gq, gk, gs,\n",
    "            B, T, S, C, W,\n",
    "            BLOCK_SIZE_C=BLOCK_SIZE_C,\n",
    "        )\n",
    "\n",
    "        return gq, gk, gs, None\n",
    "\n",
    "# @torch.compile\n",
    "def attend_folded_all_keys_triton(q, k, states, W):\n",
    "    return AttendFoldedAllKeysTriton.apply(q, k, states, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: tensor([[[-1.66,  1.93,  ...,  0.76, -0.56],\n",
      "         [ 0.28,  0.50,  ..., -0.77, -0.10],\n",
      "         ...,\n",
      "         [-0.94,  0.83,  ..., -0.31,  1.99],\n",
      "         [-1.20, -0.74,  ..., -0.40,  0.56]],\n",
      "\n",
      "        [[-1.55, -0.52,  ..., -0.94,  1.08],\n",
      "         [ 1.64,  1.72,  ..., -0.67, -0.35],\n",
      "         ...,\n",
      "         [-0.30,  0.34,  ...,  0.22, -0.48],\n",
      "         [-2.28, -1.45,  ...,  1.63, -0.17]],\n",
      "\n",
      "        [[ 0.08,  0.12,  ..., -0.53, -1.34],\n",
      "         [-0.90, -1.02,  ..., -0.73,  0.11],\n",
      "         ...,\n",
      "         [ 0.47,  0.30,  ...,  0.24,  0.63],\n",
      "         [-1.58, -1.29,  ...,  0.19, -0.87]],\n",
      "\n",
      "        [[-0.09,  1.91,  ..., -1.22, -0.08],\n",
      "         [ 0.14,  0.03,  ..., -0.39, -0.38],\n",
      "         ...,\n",
      "         [ 0.18, -1.77,  ...,  0.73, -0.48],\n",
      "         [-0.11, -0.20,  ...,  1.67, -0.06]]], device='cuda:0')\n",
      "k: tensor([[[ 0.50, -0.05,  ..., -1.19,  1.71],\n",
      "         [-0.77, -1.38,  ...,  0.05, -1.33],\n",
      "         ...,\n",
      "         [ 0.85, -0.29,  ..., -1.34, -0.37],\n",
      "         [-0.79,  0.23,  ..., -0.56,  0.84]],\n",
      "\n",
      "        [[-2.17,  0.40,  ...,  0.74,  0.39],\n",
      "         [-0.35, -1.51,  ...,  1.82, -1.41],\n",
      "         ...,\n",
      "         [-0.11,  1.11,  ..., -0.33,  0.79],\n",
      "         [-0.80,  0.81,  ...,  0.88,  1.11]],\n",
      "\n",
      "        [[-0.25,  0.56,  ..., -0.28,  0.34],\n",
      "         [-0.48,  1.07,  ...,  0.42, -0.49],\n",
      "         ...,\n",
      "         [ 0.51, -1.01,  ..., -0.86, -0.74],\n",
      "         [ 0.74,  0.86,  ..., -0.47, -0.15]],\n",
      "\n",
      "        [[-1.15,  0.49,  ..., -0.15, -0.23],\n",
      "         [ 0.77, -0.77,  ..., -1.31, -0.29],\n",
      "         ...,\n",
      "         [ 0.55,  1.16,  ..., -0.65,  0.61],\n",
      "         [-0.14,  0.66,  ..., -1.32,  1.07]]], device='cuda:0')\n",
      "states: tensor([[[[-0.58, -0.89,  ..., -0.10, -0.60],\n",
      "          [-0.76,  0.90,  ...,  0.39, -2.33],\n",
      "          ...,\n",
      "          [-0.06,  0.74,  ...,  0.81, -0.96],\n",
      "          [-0.21,  2.05,  ..., -1.88, -1.11]],\n",
      "\n",
      "         [[-0.48,  1.52,  ..., -0.67,  0.48],\n",
      "          [-0.42, -1.28,  ..., -0.32,  0.22],\n",
      "          ...,\n",
      "          [-1.65,  1.45,  ...,  0.21, -1.42],\n",
      "          [ 0.27, -0.90,  ...,  0.01,  1.07]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.33, -0.08,  ..., -1.09,  1.02],\n",
      "          [ 0.24, -0.01,  ...,  1.01, -0.07],\n",
      "          ...,\n",
      "          [-1.56, -0.64,  ...,  0.53, -0.16],\n",
      "          [-0.97,  0.31,  ..., -0.11, -0.53]],\n",
      "\n",
      "         [[-2.93,  0.10,  ..., -0.60,  1.69],\n",
      "          [-0.14, -0.76,  ...,  0.18, -0.07],\n",
      "          ...,\n",
      "          [-0.66, -0.23,  ..., -1.29, -0.17],\n",
      "          [-0.27, -1.97,  ..., -0.91, -0.41]]],\n",
      "\n",
      "\n",
      "        [[[-1.10,  1.26,  ..., -0.77, -0.43],\n",
      "          [ 0.48,  0.09,  ..., -0.34,  0.65],\n",
      "          ...,\n",
      "          [ 0.07,  1.44,  ...,  0.68,  0.13],\n",
      "          [-1.68,  0.06,  ..., -0.53,  0.52]],\n",
      "\n",
      "         [[ 0.45,  0.92,  ...,  1.78, -0.67],\n",
      "          [ 0.79,  0.95,  ...,  0.29, -0.70],\n",
      "          ...,\n",
      "          [-1.51, -0.53,  ..., -0.03,  0.37],\n",
      "          [-0.39, -0.58,  ..., -0.60,  2.98]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.48, -1.45,  ..., -0.08, -1.42],\n",
      "          [ 0.16, -0.06,  ..., -0.36,  0.93],\n",
      "          ...,\n",
      "          [ 0.24, -0.39,  ..., -0.97, -0.00],\n",
      "          [ 0.69, -1.12,  ..., -0.12,  1.06]],\n",
      "\n",
      "         [[ 1.36, -2.49,  ...,  0.51,  0.13],\n",
      "          [ 1.23, -0.67,  ...,  0.13,  2.37],\n",
      "          ...,\n",
      "          [ 0.62, -1.05,  ...,  0.07, -0.17],\n",
      "          [ 0.26, -0.66,  ..., -0.60,  1.78]]],\n",
      "\n",
      "\n",
      "        [[[-0.28,  0.05,  ...,  0.47, -0.10],\n",
      "          [ 0.37, -0.34,  ...,  2.20,  0.06],\n",
      "          ...,\n",
      "          [ 0.04, -0.26,  ...,  0.69,  2.04],\n",
      "          [-0.69,  1.57,  ...,  0.63, -0.11]],\n",
      "\n",
      "         [[-1.26, -2.09,  ...,  0.88, -0.97],\n",
      "          [ 0.95, -1.34,  ..., -0.18,  0.18],\n",
      "          ...,\n",
      "          [-0.56, -0.94,  ...,  0.96,  1.57],\n",
      "          [-0.57, -1.25,  ..., -0.11,  1.23]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.15, -1.90,  ...,  0.55,  1.37],\n",
      "          [-1.01,  0.21,  ...,  0.25, -1.09],\n",
      "          ...,\n",
      "          [ 1.66, -0.94,  ..., -1.35,  0.09],\n",
      "          [ 0.96, -1.72,  ...,  0.48, -0.39]],\n",
      "\n",
      "         [[-1.37,  0.49,  ..., -0.30, -0.48],\n",
      "          [-0.15, -0.88,  ..., -0.59, -1.08],\n",
      "          ...,\n",
      "          [ 0.08, -0.46,  ..., -1.90, -1.28],\n",
      "          [-0.44, -0.77,  ..., -0.64, -0.03]]],\n",
      "\n",
      "\n",
      "        [[[ 0.15,  1.59,  ...,  0.70, -2.09],\n",
      "          [ 0.36, -0.10,  ...,  0.11,  0.81],\n",
      "          ...,\n",
      "          [-0.96, -2.05,  ..., -0.16, -0.36],\n",
      "          [ 0.22, -1.66,  ..., -0.05,  0.12]],\n",
      "\n",
      "         [[-1.04, -0.29,  ...,  0.26, -0.94],\n",
      "          [ 1.47,  0.06,  ...,  0.36, -0.27],\n",
      "          ...,\n",
      "          [-1.13,  1.02,  ...,  0.35, -0.18],\n",
      "          [-1.60, -1.09,  ...,  0.40, -0.19]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.87, -0.02,  ...,  0.83,  0.71],\n",
      "          [ 0.14, -1.04,  ..., -0.81,  0.64],\n",
      "          ...,\n",
      "          [-0.59, -0.01,  ..., -0.35, -0.15],\n",
      "          [ 0.12,  0.21,  ...,  1.23,  0.97]],\n",
      "\n",
      "         [[-1.73,  0.76,  ...,  0.33, -0.18],\n",
      "          [ 1.50,  1.37,  ..., -1.00,  0.65],\n",
      "          ...,\n",
      "          [-0.51, -0.23,  ...,  0.77,  0.97],\n",
      "          [ 2.71, -0.36,  ..., -1.55, -0.91]]]], device='cuda:0')\n",
      "torch:\n",
      "tensor([[[  0.15,   1.39,  ...,   0.00,   1.41],\n",
      "         [ -0.23,   3.15,  ...,  -1.28,  -9.79],\n",
      "         ...,\n",
      "         [  5.03,   9.69,  ...,   3.74,  -6.30],\n",
      "         [  0.27,   0.03,  ...,  -5.31,  -2.42]],\n",
      "\n",
      "        [[ -2.38,  -8.80,  ...,   0.00,  -5.66],\n",
      "         [  5.70,  -1.00,  ...,   3.67,  -3.81],\n",
      "         ...,\n",
      "         [ -3.16,  -5.51,  ...,  -1.07,   1.15],\n",
      "         [  0.72,  -4.29,  ...,  -6.96,   9.54]],\n",
      "\n",
      "        [[ -7.18,  -3.89,  ...,   0.00,   4.69],\n",
      "         [  8.90,  -3.31,  ...,   6.03,   1.80],\n",
      "         ...,\n",
      "         [  4.86,  -7.38,  ...,  -5.64,   3.75],\n",
      "         [ -5.05,   5.75,  ...,   0.78,   2.17]],\n",
      "\n",
      "        [[  1.93,  -2.73,  ...,   0.00,  -1.06],\n",
      "         [  0.96, -12.00,  ...,   0.44,   5.53],\n",
      "         ...,\n",
      "         [ -9.25,  -0.27,  ...,   2.13,  -7.99],\n",
      "         [  4.22,   5.86,  ...,  -7.57,   6.23]]], device='cuda:0')\n",
      "triton:\n",
      "tensor([[[  0.15,   1.39,  ...,   0.00,   1.41],\n",
      "         [ -0.23,   3.15,  ...,  -1.28,  -9.79],\n",
      "         ...,\n",
      "         [  5.03,   9.69,  ...,   3.74,  -6.30],\n",
      "         [  0.27,   0.03,  ...,  -5.31,  -2.42]],\n",
      "\n",
      "        [[ -2.38,  -8.80,  ...,   0.00,  -5.66],\n",
      "         [  5.70,  -1.00,  ...,   3.67,  -3.81],\n",
      "         ...,\n",
      "         [ -3.16,  -5.51,  ...,  -1.07,   1.15],\n",
      "         [  0.72,  -4.29,  ...,  -6.96,   9.54]],\n",
      "\n",
      "        [[ -7.18,  -3.89,  ...,   0.00,   4.69],\n",
      "         [  8.90,  -3.31,  ...,   6.03,   1.80],\n",
      "         ...,\n",
      "         [  4.86,  -7.38,  ...,  -5.64,   3.75],\n",
      "         [ -5.05,   5.75,  ...,   0.78,   2.17]],\n",
      "\n",
      "        [[  1.93,  -2.73,  ...,   0.00,  -1.06],\n",
      "         [  0.96, -12.00,  ...,   0.44,   5.53],\n",
      "         ...,\n",
      "         [ -9.25,  -0.27,  ...,   2.13,  -7.99],\n",
      "         [  4.22,   5.86,  ...,  -7.57,   6.23]]], device='cuda:0')\n",
      "max diff: tensor(    0.00, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# try the forward pass\n",
    "# test attend_folded_keys_torch with always the same random inputs\n",
    "B, T, S, W, C = 4, 128, 16, 32, 32\n",
    "q = torch.randn(B, T, C, device=device)\n",
    "print(\"q:\", q)\n",
    "k = torch.randn(B, T, C, device=device)\n",
    "print(\"k:\", k)\n",
    "states = torch.randn(B, T, S, C, device=device)\n",
    "print(\"states:\", states)\n",
    "print(\"torch:\")\n",
    "out_torch = attend_folded_all_keys_torch(q, k, states, W)\n",
    "print(out_torch)\n",
    "print(\"triton:\")\n",
    "out_triton = attend_folded_all_keys_triton(q, k, states, W)\n",
    "print(out_triton)\n",
    "print(\"max diff:\", (out_torch - out_triton).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_output: tensor([[[ 1.76,  0.76,  ...,  2.53,  0.46],\n",
      "         [-1.75,  0.75,  ..., -0.72,  0.70],\n",
      "         ...,\n",
      "         [-0.06, -1.62,  ..., -0.38,  1.67],\n",
      "         [-0.57, -0.76,  ..., -0.37, -1.88]],\n",
      "\n",
      "        [[-0.52,  0.22,  ..., -0.16,  0.81],\n",
      "         [-1.28,  0.40,  ..., -1.01, -0.74],\n",
      "         ...,\n",
      "         [ 0.06, -1.23,  ..., -0.73, -0.95],\n",
      "         [-0.91,  0.99,  ..., -0.38,  0.30]],\n",
      "\n",
      "        [[ 0.71,  1.19,  ...,  2.27, -0.88],\n",
      "         [ 0.57,  0.41,  ..., -2.63,  2.07],\n",
      "         ...,\n",
      "         [ 1.07,  0.15,  ..., -0.02, -0.47],\n",
      "         [ 1.09,  0.12,  ...,  0.44, -0.03]],\n",
      "\n",
      "        [[ 0.49,  0.07,  ...,  0.43,  0.42],\n",
      "         [-0.47, -0.42,  ..., -1.76, -0.68],\n",
      "         ...,\n",
      "         [ 2.14,  1.18,  ..., -0.45, -1.33],\n",
      "         [-1.65,  1.52,  ..., -1.03,  0.40]]], device='cuda:0')\n",
      "torch:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khalid Zuhri\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q grad: tensor([[[  4.87,   1.65,  ...,  -1.53,  -2.43],\n",
      "         [  5.53,  -8.25,  ...,  -0.64,   0.64],\n",
      "         ...,\n",
      "         [ -3.31, -13.54,  ...,  -8.48,  -3.98],\n",
      "         [  4.67,   6.31,  ...,  -2.21,  -4.84]],\n",
      "\n",
      "        [[ -0.61,   3.50,  ...,   2.47,   1.22],\n",
      "         [  3.13,   1.01,  ...,  -4.44,   2.77],\n",
      "         ...,\n",
      "         [  6.54, -12.02,  ...,   4.00,  -3.65],\n",
      "         [-13.45,  -4.70,  ...,   7.30,   6.88]],\n",
      "\n",
      "        [[ -3.34,  -9.33,  ...,   8.09,  -4.37],\n",
      "         [ -6.19,  -6.89,  ...,  -2.63,  -4.40],\n",
      "         ...,\n",
      "         [ -2.95,   4.27,  ...,   6.94,   3.48],\n",
      "         [ -3.17,   1.13,  ...,   4.81,  -1.45]],\n",
      "\n",
      "        [[ -3.79,   3.77,  ...,   2.92,   5.33],\n",
      "         [  1.39,   0.26,  ...,   2.54,   0.49],\n",
      "         ...,\n",
      "         [-17.65,  -0.56,  ...,   7.50,   0.95],\n",
      "         [  8.09,  -4.75,  ...,   7.55,  -5.78]]], device='cuda:0')\n",
      "k grad: tensor([[[    -4.42,      6.85,  ...,     -5.69,     -2.79],\n",
      "         [    -2.80,      4.68,  ...,    -12.58,      6.92],\n",
      "         ...,\n",
      "         [    -1.13,      1.67,  ...,     -0.37,      3.11],\n",
      "         [     2.25,      1.39,  ...,      0.75,     -1.05]],\n",
      "\n",
      "        [[    -4.62,     -3.70,  ...,     -4.99,      1.89],\n",
      "         [     1.22,     -2.72,  ...,     -1.24,      5.86],\n",
      "         ...,\n",
      "         [     1.15,      0.23,  ...,     -0.83,      0.52],\n",
      "         [    -0.70,     -0.44,  ...,      0.50,     -0.05]],\n",
      "\n",
      "        [[     2.02,      4.98,  ...,     -3.59,      5.78],\n",
      "         [     3.37,      1.83,  ...,      1.11,      1.46],\n",
      "         ...,\n",
      "         [    -0.92,     -0.71,  ...,     -0.03,     -0.68],\n",
      "         [     0.04,      0.03,  ...,     -0.00,      0.02]],\n",
      "\n",
      "        [[     1.53,     -3.82,  ...,      2.85,      6.70],\n",
      "         [     2.91,      6.31,  ...,     -1.96,     -4.52],\n",
      "         ...,\n",
      "         [    -0.12,      2.57,  ...,     -2.70,      0.71],\n",
      "         [    -0.04,     -0.08,  ...,      0.68,     -0.03]]], device='cuda:0')\n",
      "states grad: tensor([[[[    -2.92,      3.39,  ...,      1.33,     -0.98],\n",
      "          [    -1.26,      1.46,  ...,      0.57,     -0.42],\n",
      "          ...,\n",
      "          [    -1.02,      1.18,  ...,      0.46,     -0.34],\n",
      "          [    -1.63,      1.89,  ...,      0.74,     -0.55]],\n",
      "\n",
      "         [[    -0.49,     -0.88,  ...,      1.34,      0.18],\n",
      "          [     0.21,      0.38,  ...,     -0.58,     -0.08],\n",
      "          ...,\n",
      "          [    -0.32,     -0.58,  ...,      0.88,      0.12],\n",
      "          [    -0.17,     -0.30,  ...,      0.46,      0.06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0.05,     -0.05,  ...,      0.02,     -0.11],\n",
      "          [     1.52,     -1.35,  ...,      0.50,     -3.21],\n",
      "          ...,\n",
      "          [    -2.87,      2.54,  ...,     -0.94,      6.05],\n",
      "          [     1.07,     -0.94,  ...,      0.35,     -2.25]],\n",
      "\n",
      "         [[     0.68,      0.42,  ...,      0.23,     -0.31],\n",
      "          [     0.90,      0.56,  ...,      0.30,     -0.42],\n",
      "          ...,\n",
      "          [    -0.91,     -0.57,  ...,     -0.31,      0.43],\n",
      "          [     0.35,      0.22,  ...,      0.12,     -0.16]]],\n",
      "\n",
      "\n",
      "        [[[     0.81,      0.27,  ...,      0.49,     -0.56],\n",
      "          [    -0.34,     -0.12,  ...,     -0.21,      0.24],\n",
      "          ...,\n",
      "          [    -1.72,     -0.58,  ...,     -1.04,      1.20],\n",
      "          [    -0.25,     -0.08,  ...,     -0.15,      0.17]],\n",
      "\n",
      "         [[    -2.10,     -2.20,  ...,      0.86,      0.44],\n",
      "          [     0.66,      0.70,  ...,     -0.27,     -0.14],\n",
      "          ...,\n",
      "          [    -1.24,     -1.30,  ...,      0.51,      0.26],\n",
      "          [     0.67,      0.70,  ...,     -0.27,     -0.14]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -0.02,      0.02,  ...,      0.01,     -0.03],\n",
      "          [     0.37,     -0.42,  ...,     -0.27,      0.59],\n",
      "          ...,\n",
      "          [     0.00,     -0.00,  ...,     -0.00,      0.00],\n",
      "          [     0.27,     -0.31,  ...,     -0.20,      0.44]],\n",
      "\n",
      "         [[     2.07,      1.31,  ...,     -1.48,      0.15],\n",
      "          [    -2.25,     -1.43,  ...,      1.61,     -0.17],\n",
      "          ...,\n",
      "          [    -0.75,     -0.48,  ...,      0.54,     -0.06],\n",
      "          [    -0.33,     -0.21,  ...,      0.23,     -0.02]]],\n",
      "\n",
      "\n",
      "        [[[     0.06,      0.08,  ...,     -0.37,     -0.95],\n",
      "          [     0.09,      0.14,  ...,     -0.63,     -1.60],\n",
      "          ...,\n",
      "          [    -0.06,     -0.10,  ...,      0.42,      1.07],\n",
      "          [    -0.07,     -0.11,  ...,      0.47,      1.19]],\n",
      "\n",
      "         [[    -0.51,     -0.58,  ...,     -0.42,      0.06],\n",
      "          [    -0.37,     -0.41,  ...,     -0.30,      0.04],\n",
      "          ...,\n",
      "          [    -0.54,     -0.61,  ...,     -0.44,      0.07],\n",
      "          [    -0.47,     -0.53,  ...,     -0.38,      0.06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0.50,      0.32,  ...,      0.25,      0.67],\n",
      "          [     0.07,      0.04,  ...,      0.03,      0.09],\n",
      "          ...,\n",
      "          [    -0.52,     -0.34,  ...,     -0.26,     -0.70],\n",
      "          [     0.28,      0.18,  ...,      0.14,      0.38]],\n",
      "\n",
      "         [[    -1.72,     -1.41,  ...,      0.20,     -0.96],\n",
      "          [    -0.19,     -0.16,  ...,      0.02,     -0.11],\n",
      "          ...,\n",
      "          [    -1.00,     -0.82,  ...,      0.12,     -0.55],\n",
      "          [    -0.01,     -0.01,  ...,      0.00,     -0.01]]],\n",
      "\n",
      "\n",
      "        [[[    -0.05,      0.93,  ...,     -0.59,     -0.04],\n",
      "          [    -0.01,      0.12,  ...,     -0.08,     -0.01],\n",
      "          ...,\n",
      "          [     0.09,     -1.96,  ...,      1.24,      0.08],\n",
      "          [     0.14,     -2.79,  ...,      1.77,      0.12]],\n",
      "\n",
      "         [[    -0.07,     -0.01,  ...,      0.18,      0.18],\n",
      "          [    -0.06,     -0.01,  ...,      0.16,      0.16],\n",
      "          ...,\n",
      "          [    -0.13,     -0.02,  ...,      0.36,      0.35],\n",
      "          [     0.10,      0.02,  ...,     -0.27,     -0.27]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0.38,     -3.78,  ...,      1.56,     -1.03],\n",
      "          [     0.21,     -2.09,  ...,      0.86,     -0.57],\n",
      "          ...,\n",
      "          [     0.14,     -1.45,  ...,      0.60,     -0.39],\n",
      "          [    -0.15,      1.46,  ...,     -0.60,      0.40]],\n",
      "\n",
      "         [[     0.18,      0.34,  ...,     -2.76,      0.11],\n",
      "          [    -0.17,     -0.31,  ...,      2.55,     -0.10],\n",
      "          ...,\n",
      "          [     0.04,      0.07,  ...,     -0.55,      0.02],\n",
      "          [     0.01,      0.01,  ...,     -0.08,      0.00]]]], device='cuda:0')\n",
      "triton:\n",
      "q grad: tensor([[[  4.87,   1.65,  ...,  -1.53,  -2.43],\n",
      "         [  5.53,  -8.25,  ...,  -0.64,   0.64],\n",
      "         ...,\n",
      "         [ -3.31, -13.54,  ...,  -8.48,  -3.98],\n",
      "         [  4.67,   6.31,  ...,  -2.21,  -4.84]],\n",
      "\n",
      "        [[ -0.61,   3.50,  ...,   2.47,   1.22],\n",
      "         [  3.13,   1.01,  ...,  -4.44,   2.77],\n",
      "         ...,\n",
      "         [  6.54, -12.02,  ...,   4.00,  -3.65],\n",
      "         [-13.45,  -4.70,  ...,   7.30,   6.88]],\n",
      "\n",
      "        [[ -3.34,  -9.33,  ...,   8.09,  -4.37],\n",
      "         [ -6.19,  -6.89,  ...,  -2.63,  -4.40],\n",
      "         ...,\n",
      "         [ -2.95,   4.27,  ...,   6.94,   3.48],\n",
      "         [ -3.17,   1.13,  ...,   4.81,  -1.45]],\n",
      "\n",
      "        [[ -3.79,   3.77,  ...,   2.92,   5.33],\n",
      "         [  1.39,   0.26,  ...,   2.54,   0.49],\n",
      "         ...,\n",
      "         [-17.65,  -0.56,  ...,   7.50,   0.95],\n",
      "         [  8.09,  -4.75,  ...,   7.55,  -5.78]]], device='cuda:0')\n",
      "k grad: tensor([[[    -4.42,      6.85,  ...,     -5.69,     -2.79],\n",
      "         [    -2.80,      4.68,  ...,    -12.58,      6.92],\n",
      "         ...,\n",
      "         [    -1.13,      1.67,  ...,     -0.37,      3.11],\n",
      "         [     2.25,      1.39,  ...,      0.75,     -1.05]],\n",
      "\n",
      "        [[    -4.62,     -3.70,  ...,     -4.99,      1.89],\n",
      "         [     1.22,     -2.72,  ...,     -1.24,      5.86],\n",
      "         ...,\n",
      "         [     1.15,      0.23,  ...,     -0.83,      0.52],\n",
      "         [    -0.70,     -0.44,  ...,      0.50,     -0.05]],\n",
      "\n",
      "        [[     2.02,      4.98,  ...,     -3.59,      5.78],\n",
      "         [     3.37,      1.83,  ...,      1.11,      1.46],\n",
      "         ...,\n",
      "         [    -0.92,     -0.71,  ...,     -0.03,     -0.68],\n",
      "         [     0.04,      0.03,  ...,     -0.00,      0.02]],\n",
      "\n",
      "        [[     1.53,     -3.82,  ...,      2.85,      6.70],\n",
      "         [     2.91,      6.31,  ...,     -1.96,     -4.52],\n",
      "         ...,\n",
      "         [    -0.12,      2.57,  ...,     -2.70,      0.71],\n",
      "         [    -0.04,     -0.08,  ...,      0.68,     -0.03]]], device='cuda:0')\n",
      "states grad: tensor([[[[    -2.92,      3.39,  ...,      1.33,     -0.98],\n",
      "          [    -1.26,      1.46,  ...,      0.57,     -0.42],\n",
      "          ...,\n",
      "          [    -1.02,      1.18,  ...,      0.46,     -0.34],\n",
      "          [    -1.63,      1.89,  ...,      0.74,     -0.55]],\n",
      "\n",
      "         [[    -0.49,     -0.88,  ...,      1.34,      0.18],\n",
      "          [     0.21,      0.38,  ...,     -0.58,     -0.08],\n",
      "          ...,\n",
      "          [    -0.32,     -0.58,  ...,      0.88,      0.12],\n",
      "          [    -0.17,     -0.30,  ...,      0.46,      0.06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0.05,     -0.05,  ...,      0.02,     -0.11],\n",
      "          [     1.52,     -1.35,  ...,      0.50,     -3.21],\n",
      "          ...,\n",
      "          [    -2.87,      2.54,  ...,     -0.94,      6.05],\n",
      "          [     1.07,     -0.94,  ...,      0.35,     -2.25]],\n",
      "\n",
      "         [[     0.68,      0.42,  ...,      0.23,     -0.31],\n",
      "          [     0.90,      0.56,  ...,      0.30,     -0.42],\n",
      "          ...,\n",
      "          [    -0.91,     -0.57,  ...,     -0.31,      0.43],\n",
      "          [     0.35,      0.22,  ...,      0.12,     -0.16]]],\n",
      "\n",
      "\n",
      "        [[[     0.81,      0.27,  ...,      0.49,     -0.56],\n",
      "          [    -0.34,     -0.12,  ...,     -0.21,      0.24],\n",
      "          ...,\n",
      "          [    -1.72,     -0.58,  ...,     -1.04,      1.20],\n",
      "          [    -0.25,     -0.08,  ...,     -0.15,      0.17]],\n",
      "\n",
      "         [[    -2.10,     -2.20,  ...,      0.86,      0.44],\n",
      "          [     0.66,      0.70,  ...,     -0.27,     -0.14],\n",
      "          ...,\n",
      "          [    -1.24,     -1.30,  ...,      0.51,      0.26],\n",
      "          [     0.67,      0.70,  ...,     -0.27,     -0.14]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -0.02,      0.02,  ...,      0.01,     -0.03],\n",
      "          [     0.37,     -0.42,  ...,     -0.27,      0.59],\n",
      "          ...,\n",
      "          [     0.00,     -0.00,  ...,     -0.00,      0.00],\n",
      "          [     0.27,     -0.31,  ...,     -0.20,      0.44]],\n",
      "\n",
      "         [[     2.07,      1.31,  ...,     -1.48,      0.15],\n",
      "          [    -2.25,     -1.43,  ...,      1.61,     -0.17],\n",
      "          ...,\n",
      "          [    -0.75,     -0.48,  ...,      0.54,     -0.06],\n",
      "          [    -0.33,     -0.21,  ...,      0.23,     -0.02]]],\n",
      "\n",
      "\n",
      "        [[[     0.06,      0.08,  ...,     -0.37,     -0.95],\n",
      "          [     0.09,      0.14,  ...,     -0.63,     -1.60],\n",
      "          ...,\n",
      "          [    -0.06,     -0.10,  ...,      0.42,      1.07],\n",
      "          [    -0.07,     -0.11,  ...,      0.47,      1.19]],\n",
      "\n",
      "         [[    -0.51,     -0.58,  ...,     -0.42,      0.06],\n",
      "          [    -0.37,     -0.41,  ...,     -0.30,      0.04],\n",
      "          ...,\n",
      "          [    -0.54,     -0.61,  ...,     -0.44,      0.07],\n",
      "          [    -0.47,     -0.53,  ...,     -0.38,      0.06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0.50,      0.32,  ...,      0.25,      0.67],\n",
      "          [     0.07,      0.04,  ...,      0.03,      0.09],\n",
      "          ...,\n",
      "          [    -0.52,     -0.34,  ...,     -0.26,     -0.70],\n",
      "          [     0.28,      0.18,  ...,      0.14,      0.38]],\n",
      "\n",
      "         [[    -1.72,     -1.41,  ...,      0.20,     -0.96],\n",
      "          [    -0.19,     -0.16,  ...,      0.02,     -0.11],\n",
      "          ...,\n",
      "          [    -1.00,     -0.82,  ...,      0.12,     -0.55],\n",
      "          [    -0.01,     -0.01,  ...,      0.00,     -0.01]]],\n",
      "\n",
      "\n",
      "        [[[    -0.05,      0.93,  ...,     -0.59,     -0.04],\n",
      "          [    -0.01,      0.12,  ...,     -0.08,     -0.01],\n",
      "          ...,\n",
      "          [     0.09,     -1.96,  ...,      1.24,      0.08],\n",
      "          [     0.14,     -2.79,  ...,      1.77,      0.12]],\n",
      "\n",
      "         [[    -0.07,     -0.01,  ...,      0.18,      0.18],\n",
      "          [    -0.06,     -0.01,  ...,      0.16,      0.16],\n",
      "          ...,\n",
      "          [    -0.13,     -0.02,  ...,      0.36,      0.35],\n",
      "          [     0.10,      0.02,  ...,     -0.27,     -0.27]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0.38,     -3.78,  ...,      1.56,     -1.03],\n",
      "          [     0.21,     -2.09,  ...,      0.86,     -0.57],\n",
      "          ...,\n",
      "          [     0.14,     -1.45,  ...,      0.60,     -0.39],\n",
      "          [    -0.15,      1.46,  ...,     -0.60,      0.40]],\n",
      "\n",
      "         [[     0.18,      0.34,  ...,     -2.76,      0.11],\n",
      "          [    -0.17,     -0.31,  ...,      2.55,     -0.10],\n",
      "          ...,\n",
      "          [     0.04,      0.07,  ...,     -0.55,      0.02],\n",
      "          [     0.01,      0.01,  ...,     -0.08,      0.00]]]], device='cuda:0')\n",
      "max diff q: tensor(    0.00, device='cuda:0')\n",
      "max diff k: tensor(    0.00, device='cuda:0')\n",
      "max diff states: tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# try the backward pass, compare with torch autograd\n",
    "# test attend_folded_keys_torch with always the same random inputs\n",
    "grad_output = torch.randn(B, T, S+W, device=device)\n",
    "print(\"grad_output:\", grad_output)\n",
    "print(\"torch:\")\n",
    "q.requires_grad = True\n",
    "k.requires_grad = True\n",
    "states.requires_grad = True\n",
    "out = attend_folded_all_keys_torch(q, k, states, W)\n",
    "out.backward(grad_output)\n",
    "torch_q_grad = q.grad.clone()\n",
    "torch_k_grad = k.grad.clone()\n",
    "torch_states_grad = states.grad.clone()\n",
    "print('q grad:', q.grad)\n",
    "print('k grad:', k.grad)\n",
    "print('states grad:', states.grad)\n",
    "# reset gradients\n",
    "q.grad = None\n",
    "k.grad = None\n",
    "states.grad = None\n",
    "print(\"triton:\")\n",
    "out_triton = attend_folded_all_keys_triton(q, k, states, W)\n",
    "out_triton.backward(grad_output)\n",
    "triton_q_grad = q.grad.clone()\n",
    "triton_k_grad = k.grad.clone()\n",
    "triton_states_grad = states.grad.clone()\n",
    "print('q grad:', q.grad)\n",
    "print('k grad:', k.grad)\n",
    "print('states grad:', states.grad)\n",
    "print(\"max diff q:\", (torch_q_grad - triton_q_grad).abs().max())\n",
    "print(\"max diff k:\", (torch_k_grad - triton_k_grad).abs().max())\n",
    "print(\"max diff states:\", (torch_states_grad - triton_states_grad).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch forward:\n",
      "2.78 ms Â± 3.48 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n",
      "torch forward & backward:\n",
      "8.83 ms Â± 40.3 Î¼s per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "triton forward:\n",
      "151 Î¼s Â± 31.1 Î¼s per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "triton forward & backward:\n",
      "586 Î¼s Â± 39.5 Î¼s per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# measure the speedup\n",
    "B, T, W, S, C = 128, 1024, 32, 32, 64\n",
    "q = torch.randn(B, T, C, device=device)\n",
    "k = torch.randn(B, T, C, device=device)\n",
    "states = torch.randn(B, T, S, C, device=device)\n",
    "grad_output = torch.randn(B, T, S+W, device=device)\n",
    "# warmup the triton kernel\n",
    "q.requires_grad = True\n",
    "k.requires_grad = True\n",
    "out = attend_folded_all_keys_triton(q, k, states, W)\n",
    "out.backward(grad_output)\n",
    "q.grad = None\n",
    "k.grad = None\n",
    "\n",
    "B, T, W, S, C = 128, 128, 32, 32, 64\n",
    "q = torch.randn(B, T, C, device=device)\n",
    "k = torch.randn(B, T, C, device=device)\n",
    "states = torch.randn(B, T, S, C, device=device)\n",
    "grad_output = torch.randn(B, T, S+W, device=device)\n",
    "print(\"torch forward:\")\n",
    "q.requires_grad = True\n",
    "k.requires_grad = True\n",
    "%timeit attend_folded_all_keys_torch(q, k, states, W)\n",
    "print(\"torch forward & backward:\")\n",
    "def ftorch():\n",
    "    q.grad = None\n",
    "    k.grad = None\n",
    "    out_torch = attend_folded_all_keys_torch(q, k, states, W)\n",
    "    out_torch.backward(grad_output)\n",
    "%timeit ftorch()\n",
    "print(\"\\ntriton forward:\")\n",
    "q.requires_grad = True\n",
    "k.requires_grad = True\n",
    "%timeit attend_folded_all_keys_triton(q, k, states, W)\n",
    "print(\"triton forward & backward:\")\n",
    "def ftriton():\n",
    "    q.grad = None\n",
    "    k.grad = None\n",
    "    out_triton = attend_folded_all_keys_triton(q, k, states, W)\n",
    "    out_triton.backward(grad_output)\n",
    "%timeit ftriton()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulate Folded Values Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_fold(x, window_len):\n",
    "    # x is a (B, T, C) tensor\n",
    "    # output should be a (B, T, window_len, C) tensor that slides over the T dimension\n",
    "    # first window_len-1 elements will have to be padded with zeros in the beginning\n",
    "    # example: x = torch.tensor([[[1,2],[3,4],[5,6],[7,8],[9,10]]])\n",
    "    # sliding_window_fold(x, 2) -> torch.tensor([[[[0,1],[1,2]],[[1,2],[3,4]],[[3,4],[5,6]],[[5,6],[7,8]],[[7,8],[9,10]]]])\n",
    "    padded_x = F.pad(x, (0, 0, window_len - 1, 0), mode='constant', value=0)\n",
    "    output = padded_x.unfold(dimension=1, size=window_len, step=1).permute(0, 1, 3, 2)\n",
    "    return output\n",
    "\n",
    "def attend_folded_all_keys_torch(q, k, states, W):\n",
    "    k = sliding_window_fold(k, W) # (B, T, W, C)\n",
    "    all_keys = torch.cat((states, k), dim=2) # (B, T, S, C) + (B, T, W, C) -> (B, T, S+W, C)\n",
    "    scores = torch.einsum(\"btc, btxc -> btx\", q, all_keys) # (B, T, S+W)\n",
    "    return scores\n",
    "\n",
    "def accumulate_folded_all_values_torch(s, v, states, W):\n",
    "    v = sliding_window_fold(v, W) # (B, T, W, C)\n",
    "    all_values = torch.cat((states, v), dim=2) # (B, T, S, C) + (B, T, W, C) -> (B, T, S+W, C)\n",
    "    out = torch.einsum(\"btx, btxc -> btc\", s, all_values) # (B, T, C)\n",
    "    # out = torch.einsum(\"btx, btxc -> btc\", s[:, :, :S], states) # (B, T, C)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triton kernel\n",
    "@triton.jit\n",
    "def afav_fwd_kernel(\n",
    "    s_ptr, v_ptr, states_ptr, y_ptr,\n",
    "    B: tl.constexpr, T: tl.constexpr, S: tl.constexpr, C: tl.constexpr, W: tl.constexpr,\n",
    "    BLOCK_SIZE_C: tl.constexpr,\n",
    "):\n",
    "    # Use multiple program IDs for better parallelization\n",
    "    b_id = tl.program_id(axis=0)\n",
    "    t_id = tl.program_id(axis=1)\n",
    "    c_block_id = tl.program_id(axis=2)\n",
    "    c_first_id = c_block_id * BLOCK_SIZE_C\n",
    "    SW = S + W\n",
    "\n",
    "    # Compute base pointers\n",
    "    s_base = s_ptr + b_id * T * W\n",
    "    v_base = v_ptr + b_id * T * C\n",
    "    y_base = y_ptr + b_id * T * C\n",
    "\n",
    "    # First we accumulate the values\n",
    "    # Fetch the scores at [b_id, t_id, S:W]\n",
    "    sv_block_ptr = tl.make_block_ptr(\n",
    "        base=s_ptr,\n",
    "        shape=(B, T, SW),\n",
    "        strides=(T * SW, SW, 1),\n",
    "        offsets=(b_id, t_id, S),\n",
    "        block_shape=(1, 1, W),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    sv = tl.load(sv_block_ptr) # (1, 1, W)\n",
    "    # Fetch the value at [b_id, t_id-W+1:t_id, c_first_id:c_first_id+BLOCK_SIZE_C]\n",
    "    # need to load the keys manually because make_block_ptr doesn't support masks\n",
    "    tw_offs = tl.arange(0, W)\n",
    "    c_offs = tl.arange(0, BLOCK_SIZE_C)\n",
    "    v_block_ptr = v_base + (t_id - W + 1 + tw_offs[:, None]) * C + c_first_id + c_offs[None, :]\n",
    "    mask = tl.arange(0, W)[:, None] > (W - t_id - 2)\n",
    "    v = tl.load(v_block_ptr, mask=mask) # (W, BLOCK_SIZE_C) but W can vary <W\n",
    "    # Compute the dot product (but not with tl.dot because it has a minimum size of 16)\n",
    "    # y = sv.permute(0, 2, 1) * v[None, :] # (1, W, BLOCK_SIZE_C)\n",
    "    # y = tl.sum(y, axis=1, keep_dims=True) # (1, 1, BLOCK_SIZE_C)\n",
    "    # turns out keep_dims kinda messes stuff up when later adding the accumulated states\n",
    "\n",
    "    # Then we accumulate the states\n",
    "    # Fetch the scores at [b_id, t_id, :S]\n",
    "    ss_block_ptr = tl.make_block_ptr(\n",
    "        base=s_ptr,\n",
    "        shape=(B, T, SW),\n",
    "        strides=(T * SW, SW, 1),\n",
    "        offsets=(b_id, t_id, 0),\n",
    "        block_shape=(1, 1, S),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    ss = tl.load(ss_block_ptr) # (1, 1, S)\n",
    "    # Fetch the states at [b_id, t_id, c_first_id:c_first_id+BLOCK_SIZE_C]\n",
    "    states_block_ptr = tl.make_block_ptr(\n",
    "        base=states_ptr,\n",
    "        shape=(B, T, S, C),\n",
    "        strides=(T * S * C, S * C, C, 1),\n",
    "        offsets=(b_id, t_id, 0, c_first_id),\n",
    "        block_shape=(1, 1, S, BLOCK_SIZE_C),\n",
    "        order=(0, 1, 2, 3),\n",
    "    )\n",
    "    states = tl.load(states_block_ptr) # (1, 1, S, BLOCK_SIZE_C)\n",
    "    # Compute the dot product\n",
    "    y = tl.sum(sv.permute(0, 2, 1) * v[None, :], axis=1) + tl.sum(ss[:, :, :, None] * states, axis=2).reshape(1, BLOCK_SIZE_C)\n",
    "\n",
    "    # Store the result\n",
    "    y_block_ptr = tl.make_block_ptr(\n",
    "        base=y_ptr,\n",
    "        shape=(B, T, C),\n",
    "        strides=(T * C, C, 1),\n",
    "        offsets=(b_id, t_id, c_first_id),\n",
    "        block_shape=(1, 1, BLOCK_SIZE_C),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    tl.store(y_block_ptr, y[None, :]) # (1, 1, BLOCK_SIZE_C)\n",
    "\n",
    "@triton.jit\n",
    "def afav_bwd_kernel(\n",
    "    s_ptr, v_ptr, states_ptr, dy_ptr, ds_ptr, dv_ptr, dstates_ptr,\n",
    "    B: tl.constexpr, T: tl.constexpr, S:tl.constexpr, C: tl.constexpr, W: tl.constexpr,\n",
    "    BLOCK_SIZE_S: tl.constexpr,\n",
    "    BLOCK_SIZE_W: tl.constexpr,\n",
    "):\n",
    "    # Use multiple program IDs for better parallelization\n",
    "    b_id = tl.program_id(axis=0)\n",
    "    t_id = tl.program_id(axis=1)\n",
    "    sw_block_id = tl.program_id(axis=2)\n",
    "    num_s_blocks = triton.cdiv(S, BLOCK_SIZE_S)\n",
    "    num_w_blocks = triton.cdiv(W, BLOCK_SIZE_W)\n",
    "    is_state = sw_block_id < num_s_blocks\n",
    "    SW = S + W\n",
    "\n",
    "    # Compute base pointers\n",
    "    s_base = s_ptr + b_id * T * C\n",
    "    v_base = v_ptr + b_id * T * C\n",
    "    dy_base = dy_ptr + b_id * T * W\n",
    "    ds_base = ds_ptr + b_id * T * C\n",
    "    dv_base = dv_ptr + b_id * (T+W-1) * C + (W-1) * C # skip the first W-1 elements\n",
    "\n",
    "    if not is_state:\n",
    "        # Here we calculate the gradients for s [:, :, S:W] and for v\n",
    "        w_first_id = (sw_block_id - num_s_blocks) * BLOCK_SIZE_W\n",
    "        # First calculate the gradients for s\n",
    "        # Fetch original values at [b_id, t_id-W+1+(w_block_id*BLOCK_SIZE_W):t_id+(w_block_id*BLOCK_SIZE_W), :]\n",
    "        # using a block ptr also disallows the use of masks when loading, so let's just make a ptr manually\n",
    "        tw_offs = tl.arange(0, BLOCK_SIZE_W)\n",
    "        c_offs = tl.arange(0, C)\n",
    "        v_block_ptr = v_base + (t_id - W + 1 + (w_first_id + tw_offs[:, None])) * C + c_offs[None, :]\n",
    "        mask = w_first_id + tl.arange(0, BLOCK_SIZE_W)[:, None] > (W - t_id - 2)\n",
    "        v = tl.load(v_block_ptr, mask=mask) # (BLOCK_SIZE_W, C)\n",
    "        # Fetch original output gradients at [b_id, t_id, :]\n",
    "        dy_block_ptr = tl.make_block_ptr(\n",
    "            base=dy_ptr,\n",
    "            shape=(B, T, C),\n",
    "            strides=(T * C, C, 1),\n",
    "            offsets=(b_id, t_id, 0),\n",
    "            block_shape=(1, 1, C),\n",
    "            order=(0, 1, 2),\n",
    "        )\n",
    "        dy = tl.load(dy_block_ptr) # (1, 1, C)\n",
    "        # Compute the gradients for q\n",
    "        dsv = dy * v[None, :] # (1, BLOCK_SIZE_W, C)\n",
    "        dsv = tl.sum(dsv, axis=-1) # (1, BLOCK_SIZE_W)\n",
    "        # Store the result\n",
    "        dsv_block_ptr = tl.make_block_ptr(\n",
    "            base=ds_ptr,\n",
    "            shape=(B, T, SW),\n",
    "            strides=(T * SW, SW, 1),\n",
    "            offsets=(b_id, t_id, S+w_first_id),\n",
    "            block_shape=(1, 1, BLOCK_SIZE_W),\n",
    "            order=(0, 1, 2),\n",
    "        )\n",
    "        tl.store(dsv_block_ptr, dsv[None, :]) # (1, 1, BLOCK_SIZE_W)\n",
    "\n",
    "        # Then calculate the gradients for v\n",
    "        s_block_ptr = tl.make_block_ptr(\n",
    "            base=s_ptr,\n",
    "            shape=(B, T, SW),\n",
    "            strides=(T * SW, SW, 1),\n",
    "            offsets=(b_id, t_id, S+w_first_id),\n",
    "            block_shape=(1, 1, BLOCK_SIZE_W),\n",
    "            order=(0, 1, 2),\n",
    "        )\n",
    "        s = tl.load(s_block_ptr) # (1, 1, BLOCK_SIZE_W)\n",
    "        # We already fetched output gradients dy at [b_id, t_id, :] w/ size (1, 1, C)\n",
    "        # Compute the gradients for v\n",
    "        dv = dy * s.permute(0, 2, 1) # (1, BLOCK_SIZE_W, C)\n",
    "        # Store the result\n",
    "        # need to make a ptr manually because make_block_ptr doesn't support masks\n",
    "        tw_offs = tl.arange(0, BLOCK_SIZE_W)\n",
    "        c_offs = tl.arange(0, C)\n",
    "        dv_block_ptr = dv_base + (t_id - W + 1 + (w_first_id + tw_offs[:, None])) * C + c_offs[None, :]\n",
    "        mask = w_first_id + tl.arange(0, BLOCK_SIZE_W)[:, None] > (W - t_id - 2)\n",
    "        # now we have to atomically add the gradients to the original values\n",
    "        tl.atomic_add(dv_block_ptr[None, :], dv)\n",
    "    else:\n",
    "        s_first_id = sw_block_id * BLOCK_SIZE_S\n",
    "        # Here we calculate the gradients for s[:, :, :S] and for states\n",
    "        # First calculate the gradients for s\n",
    "        # Fetch states at [b_id, t_id, s_first_id:s_first_id+BLOCK_SIZE_S, :]\n",
    "        states_block_ptr = tl.make_block_ptr(\n",
    "            base=states_ptr,\n",
    "            shape=(B, T, S, C),\n",
    "            strides=(T * S * C, S * C, C, 1),\n",
    "            offsets=(b_id, t_id, s_first_id, 0),\n",
    "            block_shape=(1, 1, BLOCK_SIZE_S, C),\n",
    "            order=(0, 1, 2, 3),\n",
    "        )\n",
    "        states = tl.load(states_block_ptr) # (1, 1, BLOCK_SIZE_S, C)\n",
    "        # Fetch original output gradients at [b_id, t_id, :]\n",
    "        dy_block_ptr = tl.make_block_ptr(\n",
    "            base=dy_ptr,\n",
    "            shape=(B, T, C),\n",
    "            strides=(T * C, C, 1),\n",
    "            offsets=(b_id, t_id, 0),\n",
    "            block_shape=(1, 1, C),\n",
    "            order=(0, 1, 2),\n",
    "        )\n",
    "        dy = tl.load(dy_block_ptr) # (1, 1, C)\n",
    "        # Compute the gradients for s\n",
    "        dss = dy[:, :, None, :] * states # (1, 1, BLOCK_SIZE_S, C)\n",
    "        dss = tl.sum(dss, axis=-1) # (1, 1, BLOCK_SIZE_S)\n",
    "        # Store the result at [b_id, t_id, :S\n",
    "        dss_block_ptr = tl.make_block_ptr(\n",
    "            base=ds_ptr,\n",
    "            shape=(B, T, SW),\n",
    "            strides=(T * SW, SW, 1),\n",
    "            offsets=(b_id, t_id, s_first_id),\n",
    "            block_shape=(1, 1, BLOCK_SIZE_S),\n",
    "            order=(0, 1, 2),\n",
    "        )\n",
    "        tl.store(dss_block_ptr, dss) # (1, 1, BLOCK_SIZE_S)\n",
    "        \n",
    "        # Then calculate the gradients for states\n",
    "        # Fetch the scores at [b_id, t_id, :S]\n",
    "        ss_block_ptr = tl.make_block_ptr(\n",
    "            base=s_ptr,\n",
    "            shape=(B, T, SW),\n",
    "            strides=(T * SW, SW, 1),\n",
    "            offsets=(b_id, t_id, s_first_id),\n",
    "            block_shape=(1, 1, BLOCK_SIZE_S),\n",
    "            order=(0, 1, 2),\n",
    "        )\n",
    "        ss = tl.load(ss_block_ptr) # (1, 1, BLOCK_SIZE_S)\n",
    "        dstates = dy[:, :, None, :] * ss[:, :, :, None] # (1, 1, BLOCK_SIZE_S, C)\n",
    "        # Store the result\n",
    "        dstates_block_ptr = tl.make_block_ptr(\n",
    "            base=dstates_ptr,\n",
    "            shape=(B, T, S, C),\n",
    "            strides=(T * S * C, S * C, C, 1),\n",
    "            offsets=(b_id, t_id, s_first_id, 0),\n",
    "            block_shape=(1, 1, BLOCK_SIZE_S, C),\n",
    "            order=(0, 1, 2, 3),\n",
    "        )\n",
    "        tl.store(dstates_block_ptr, dstates) # (1, 1, BLOCK_SIZE_S, C)\n",
    "\n",
    "\n",
    "class AccumulateFoldedAllValuesTriton(torch.autograd.Function):\n",
    "    # @torch.compiler.disable\n",
    "    @staticmethod\n",
    "    def forward(ctx, s, v, states, W):\n",
    "        B, T, S, C = states.shape\n",
    "        s = s.contiguous()\n",
    "        v = v.contiguous()\n",
    "        states = states.contiguous()\n",
    "        ctx.save_for_backward(s, v, states)\n",
    "        ctx.W = W\n",
    "        \n",
    "        # Configure block sizes\n",
    "        BLOCK_SIZE_C = 16\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        num_c_blocks = triton.cdiv(C, BLOCK_SIZE_C)\n",
    "        grid = (B, T, num_c_blocks)\n",
    "\n",
    "        # Allocate output tensor\n",
    "        y = torch.zeros((B, T, C), dtype=v.dtype, device=v.device).contiguous()\n",
    "        \n",
    "        # Launch kernel\n",
    "        afav_fwd_kernel[grid](\n",
    "            s, v, states, y,\n",
    "            B, T, S, C, W,\n",
    "            BLOCK_SIZE_C=BLOCK_SIZE_C,\n",
    "        )\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # @torch.compiler.disable\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.contiguous()\n",
    "        s, v, states = ctx.saved_tensors\n",
    "        B, T, S, C = states.shape\n",
    "        W = ctx.W\n",
    "\n",
    "        # Configure block sizes\n",
    "        BLOCK_SIZE_S = 16\n",
    "        BLOCK_SIZE_W = 16\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        num_s_blocks = triton.cdiv(S, BLOCK_SIZE_S)\n",
    "        num_w_blocks = triton.cdiv(W, BLOCK_SIZE_W)\n",
    "        grid = (B, T, num_s_blocks+num_w_blocks)\n",
    "        \n",
    "        gs = torch.zeros_like(s).contiguous()\n",
    "        # for gv we want an additional W at the start of the time dimension bc we can't mask atomic add\n",
    "        gv = torch.zeros((B, T+W-1, C), dtype=v.dtype, device=v.device).contiguous()\n",
    "        gst = torch.zeros_like(states).contiguous()\n",
    "\n",
    "        # Launch kernel\n",
    "        afav_bwd_kernel[grid](\n",
    "            s, v, states, grad_output, gs, gv, gst,\n",
    "            B, T, S, C, W,\n",
    "            BLOCK_SIZE_S=BLOCK_SIZE_S,\n",
    "            BLOCK_SIZE_W=BLOCK_SIZE_W,\n",
    "        )\n",
    "\n",
    "        # No need for the additional W at the start of the time dimension for gv\n",
    "        return gs, gv[:, W-1:], gst, None\n",
    "\n",
    "# @torch.compile\n",
    "def accumulate_folded_all_values_triton(s, v, states, W):\n",
    "    return AccumulateFoldedAllValuesTriton.apply(s, v, states, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: tensor([[[-0.47,  1.60,  ..., -0.45,  0.31],\n",
      "         [ 0.31, -1.52,  ..., -0.09, -0.26],\n",
      "         ...,\n",
      "         [ 0.39,  0.61,  ..., -3.05,  1.86],\n",
      "         [ 0.30,  0.28,  ...,  1.38,  0.76]],\n",
      "\n",
      "        [[-0.10, -0.28,  ..., -1.89,  1.57],\n",
      "         [-1.65, -0.44,  ...,  0.27, -0.85],\n",
      "         ...,\n",
      "         [ 0.79,  0.28,  ..., -0.31,  0.88],\n",
      "         [ 0.15,  1.15,  ..., -0.68, -1.12]],\n",
      "\n",
      "        [[-0.53,  0.41,  ...,  0.33,  0.86],\n",
      "         [ 0.24, -0.31,  ...,  1.24, -0.24],\n",
      "         ...,\n",
      "         [-0.99, -0.42,  ..., -0.59,  0.55],\n",
      "         [ 1.26, -1.44,  ...,  0.33, -0.64]],\n",
      "\n",
      "        [[ 0.91, -0.20,  ..., -1.21,  0.17],\n",
      "         [-0.21,  1.38,  ...,  0.64, -1.56],\n",
      "         ...,\n",
      "         [-2.06, -0.09,  ..., -1.20, -0.49],\n",
      "         [-0.30,  0.64,  ..., -1.39, -0.41]]], device='cuda:0')\n",
      "v: tensor([[[ 0.19, -0.05,  ...,  0.42,  2.71],\n",
      "         [ 1.44,  1.68,  ...,  0.24,  0.78],\n",
      "         ...,\n",
      "         [-0.41,  0.01,  ...,  0.77, -1.10],\n",
      "         [-1.43, -0.77,  ..., -0.04,  0.31]],\n",
      "\n",
      "        [[-3.16, -0.80,  ...,  0.06, -2.20],\n",
      "         [ 0.42,  0.58,  ...,  1.82, -0.81],\n",
      "         ...,\n",
      "         [-0.65, -0.48,  ..., -1.31,  1.81],\n",
      "         [ 0.22,  0.15,  ...,  0.22, -0.03]],\n",
      "\n",
      "        [[ 0.99, -1.60,  ...,  0.42,  0.76],\n",
      "         [ 0.01, -1.06,  ..., -0.90,  1.47],\n",
      "         ...,\n",
      "         [ 0.89,  0.74,  ...,  0.58, -0.13],\n",
      "         [ 0.15,  1.28,  ...,  0.15, -0.63]],\n",
      "\n",
      "        [[-0.23,  0.44,  ...,  0.54,  1.04],\n",
      "         [-1.89,  0.49,  ..., -0.04, -0.04],\n",
      "         ...,\n",
      "         [ 0.49,  1.83,  ..., -0.76, -0.23],\n",
      "         [ 0.12, -0.61,  ..., -0.18, -1.17]]], device='cuda:0')\n",
      "states: tensor([[[[    -0.40,     -0.78,  ...,      1.44,     -0.06],\n",
      "          [     0.79,      0.28,  ...,      0.39,      1.81],\n",
      "          ...,\n",
      "          [    -0.66,     -1.00,  ...,     -1.53,     -0.07],\n",
      "          [     1.03,     -1.39,  ...,      1.63,     -0.98]],\n",
      "\n",
      "         [[     1.36,      0.46,  ...,     -0.60,     -0.56],\n",
      "          [     0.77,      1.07,  ...,      0.62,      0.70],\n",
      "          ...,\n",
      "          [     0.63,     -0.27,  ...,     -0.57,     -0.51],\n",
      "          [     0.04,      1.14,  ...,      0.59,      0.15]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0.44,      0.39,  ...,     -0.00,      2.53],\n",
      "          [     0.44,      0.05,  ...,     -1.29,     -1.55],\n",
      "          ...,\n",
      "          [     0.85,      1.35,  ...,     -1.22,      1.94],\n",
      "          [     0.72,     -2.59,  ...,      1.20,      2.33]],\n",
      "\n",
      "         [[    -1.44,      0.98,  ...,     -0.39,     -1.37],\n",
      "          [     0.38,      0.21,  ...,      1.05,     -0.79],\n",
      "          ...,\n",
      "          [     1.50,      0.69,  ...,      0.37,     -0.60],\n",
      "          [    -0.61,      2.12,  ...,     -0.46,      1.50]]],\n",
      "\n",
      "\n",
      "        [[[     0.90,     -0.32,  ...,      0.32,     -1.71],\n",
      "          [    -1.20,     -0.18,  ...,     -0.97,      1.69],\n",
      "          ...,\n",
      "          [     0.02,      0.09,  ...,     -0.33,      0.06],\n",
      "          [     0.08,      0.17,  ...,     -0.06,      1.49]],\n",
      "\n",
      "         [[     0.96,     -0.38,  ...,      1.43,     -0.48],\n",
      "          [     0.14,      1.73,  ...,     -1.15,     -0.66],\n",
      "          ...,\n",
      "          [     0.67,      0.32,  ...,      0.32,      0.39],\n",
      "          [     0.09,     -0.59,  ...,     -1.15,     -0.88]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -0.04,     -0.77,  ...,     -1.09,      2.12],\n",
      "          [    -0.83,      0.00,  ...,     -1.63,      0.46],\n",
      "          ...,\n",
      "          [     0.21,     -0.58,  ...,     -1.03,      0.70],\n",
      "          [     2.15,     -0.81,  ...,     -0.29,      0.20]],\n",
      "\n",
      "         [[    -0.16,     -0.55,  ...,     -0.62,     -2.50],\n",
      "          [     1.90,      0.88,  ...,      2.09,     -1.28],\n",
      "          ...,\n",
      "          [    -0.17,     -0.26,  ...,      0.44,      1.06],\n",
      "          [    -0.21,     -0.03,  ...,     -0.46,     -0.36]]],\n",
      "\n",
      "\n",
      "        [[[     0.16,      0.79,  ...,      1.05,      0.20],\n",
      "          [     1.08,      0.73,  ...,     -0.26,     -0.29],\n",
      "          ...,\n",
      "          [     0.14,     -0.91,  ...,      1.16,      0.40],\n",
      "          [     0.41,     -1.75,  ...,     -0.87,     -0.03]],\n",
      "\n",
      "         [[     1.23,     -0.12,  ...,      0.82,      0.97],\n",
      "          [    -0.96,     -3.19,  ...,      0.72,      1.05],\n",
      "          ...,\n",
      "          [    -0.90,     -1.31,  ...,     -0.40,      0.26],\n",
      "          [     0.12,     -0.01,  ...,     -0.85,      1.76]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     1.41,      0.09,  ...,     -0.00,      2.08],\n",
      "          [     0.75,     -0.30,  ...,      0.56,     -0.75],\n",
      "          ...,\n",
      "          [    -0.43,      0.38,  ...,      1.14,      0.30],\n",
      "          [     1.17,      0.03,  ...,     -0.35,     -0.19]],\n",
      "\n",
      "         [[    -1.18,     -1.02,  ...,     -0.06,     -0.37],\n",
      "          [     0.36,     -0.31,  ...,     -1.07,      1.20],\n",
      "          ...,\n",
      "          [    -2.26,     -0.65,  ...,     -0.25,     -0.03],\n",
      "          [     0.43,      0.80,  ...,      0.21,     -1.08]]],\n",
      "\n",
      "\n",
      "        [[[    -0.35,     -0.36,  ...,      0.05,      0.56],\n",
      "          [     0.44,      0.43,  ...,      0.55,      0.03],\n",
      "          ...,\n",
      "          [    -0.74,      0.79,  ...,      1.42,     -0.52],\n",
      "          [    -0.36,     -1.32,  ...,     -0.38,     -1.72]],\n",
      "\n",
      "         [[     0.16,     -0.55,  ...,      0.09,     -0.09],\n",
      "          [     0.73,     -0.85,  ...,      1.23,     -0.46],\n",
      "          ...,\n",
      "          [     0.17,      0.49,  ...,     -0.05,      0.38],\n",
      "          [     0.12,      0.36,  ...,      2.44,      0.17]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -1.88,     -0.90,  ...,      0.69,     -0.10],\n",
      "          [     1.24,     -1.53,  ...,     -1.11,      1.96],\n",
      "          ...,\n",
      "          [    -0.12,      1.46,  ...,     -0.25,      0.35],\n",
      "          [     1.27,      0.60,  ...,     -0.26,      0.29]],\n",
      "\n",
      "         [[     0.21,     -1.37,  ...,     -2.03,     -0.38],\n",
      "          [    -0.82,      0.68,  ...,     -0.10,     -0.50],\n",
      "          ...,\n",
      "          [    -0.66,     -1.08,  ...,      0.16,      0.28],\n",
      "          [     0.38,      0.42,  ...,     -1.55,      0.36]]]], device='cuda:0')\n",
      "torch:\n",
      "tensor([[[ -1.32,   2.06,  ...,   0.85,   1.29],\n",
      "         [ -0.79,   0.33,  ...,  -2.17,  -6.73],\n",
      "         ...,\n",
      "         [ -1.08,   1.54,  ...,   0.36, -14.13],\n",
      "         [  0.12,   3.74,  ..., -12.51,   8.40]],\n",
      "\n",
      "        [[-13.11,   5.71,  ...,   4.71,  -0.66],\n",
      "         [ -6.96,  -3.38,  ...,  -6.27,  -4.18],\n",
      "         ...,\n",
      "         [-11.62,  -2.10,  ...,   2.66,   3.50],\n",
      "         [  8.84,  -7.94,  ...,   0.16,  -1.20]],\n",
      "\n",
      "        [[  1.57,  -8.41,  ...,  -8.57,   3.14],\n",
      "         [  5.76,  -0.63,  ...,  -5.72,  -8.32],\n",
      "         ...,\n",
      "         [-10.04,   0.68,  ...,  -2.51,  -3.59],\n",
      "         [-18.35,   7.59,  ...,   8.40,   7.38]],\n",
      "\n",
      "        [[ -1.59,  -2.38,  ...,  -4.24,   0.44],\n",
      "         [ -2.01,  -2.70,  ...,  -4.29,  -0.25],\n",
      "         ...,\n",
      "         [  8.82,   3.50,  ...,   2.73,  -1.65],\n",
      "         [  0.79, -10.07,  ...,  12.87,  -3.48]]], device='cuda:0')\n",
      "triton:\n",
      "tensor([[[ -1.32,   2.06,  ...,   0.85,   1.29],\n",
      "         [ -0.79,   0.33,  ...,  -2.17,  -6.73],\n",
      "         ...,\n",
      "         [ -1.08,   1.54,  ...,   0.36, -14.13],\n",
      "         [  0.12,   3.74,  ..., -12.51,   8.40]],\n",
      "\n",
      "        [[-13.11,   5.71,  ...,   4.71,  -0.66],\n",
      "         [ -6.96,  -3.38,  ...,  -6.27,  -4.18],\n",
      "         ...,\n",
      "         [-11.62,  -2.10,  ...,   2.66,   3.50],\n",
      "         [  8.84,  -7.94,  ...,   0.16,  -1.20]],\n",
      "\n",
      "        [[  1.57,  -8.41,  ...,  -8.57,   3.14],\n",
      "         [  5.76,  -0.63,  ...,  -5.72,  -8.32],\n",
      "         ...,\n",
      "         [-10.04,   0.68,  ...,  -2.51,  -3.59],\n",
      "         [-18.35,   7.59,  ...,   8.40,   7.38]],\n",
      "\n",
      "        [[ -1.59,  -2.38,  ...,  -4.24,   0.44],\n",
      "         [ -2.01,  -2.70,  ...,  -4.29,  -0.25],\n",
      "         ...,\n",
      "         [  8.82,   3.50,  ...,   2.73,  -1.65],\n",
      "         [  0.79, -10.07,  ...,  12.87,  -3.48]]], device='cuda:0')\n",
      "max diff: tensor(    0.00, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# try the forward pass\n",
    "# test accumulate_folded_values_torch with always the same random inputs\n",
    "B, T, S, W, C = 4, 128, 32, 32, 32\n",
    "s = torch.randn(B, T, S+W, device=device)\n",
    "print(\"s:\", s)\n",
    "v = torch.randn(B, T, C, device=device)\n",
    "print(\"v:\", v)\n",
    "states = torch.randn(B, T, S, C, device=device)\n",
    "print(\"states:\", states)\n",
    "print(\"torch:\")\n",
    "out = accumulate_folded_all_values_torch(s, v, states, W)\n",
    "print(out)\n",
    "print(\"triton:\")\n",
    "out_triton = accumulate_folded_all_values_triton(s, v, states, W)\n",
    "print(out_triton)\n",
    "print(\"max diff:\", (out - out_triton).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_output: tensor([[[    -0.17,      1.59,  ...,      0.26,     -0.01],\n",
      "         [    -0.26,     -1.20,  ...,     -0.70,      0.20],\n",
      "         ...,\n",
      "         [    -0.08,     -2.85,  ...,     -1.01,     -0.24],\n",
      "         [     0.55,     -1.62,  ...,     -1.47,     -1.87]],\n",
      "\n",
      "        [[    -1.78,      1.97,  ...,     -0.06,      1.41],\n",
      "         [     1.07,      2.87,  ...,     -0.93,      1.17],\n",
      "         ...,\n",
      "         [    -1.82,      0.32,  ...,      1.08,      0.65],\n",
      "         [     0.69,     -0.58,  ...,     -0.15,      0.60]],\n",
      "\n",
      "        [[    -1.09,      0.56,  ...,     -2.19,      0.70],\n",
      "         [     0.20,     -0.96,  ...,     -0.67,      2.11],\n",
      "         ...,\n",
      "         [    -1.34,     -0.03,  ...,      0.28,      1.64],\n",
      "         [     0.01,      0.20,  ...,      0.21,      1.24]],\n",
      "\n",
      "        [[     0.19,      1.39,  ...,      0.25,     -0.36],\n",
      "         [    -0.19,     -0.98,  ...,      0.00,      0.54],\n",
      "         ...,\n",
      "         [    -0.22,     -0.48,  ...,      0.55,      0.82],\n",
      "         [    -1.27,     -0.15,  ...,      0.10,     -0.76]]], device='cuda:0')\n",
      "torch:\n",
      "s grad: tensor([[[  4.67,   3.24,  ...,   0.00,  -1.79],\n",
      "         [ -4.18,   0.74,  ...,   1.20,   0.30],\n",
      "         ...,\n",
      "         [ -9.35,   3.38,  ...,  -1.40,  -6.46],\n",
      "         [ -0.67,  -1.96,  ...,   0.96,  -0.21]],\n",
      "\n",
      "        [[  4.61,   5.82,  ...,   0.00,   5.82],\n",
      "         [  3.33,  -2.02,  ..., -15.12,  -8.15],\n",
      "         ...,\n",
      "         [  1.39,   0.96,  ...,  -5.92,   4.77],\n",
      "         [  8.01,   5.94,  ..., -11.28,  -5.42]],\n",
      "\n",
      "        [[ -4.05,  -3.10,  ...,   0.00, -15.59],\n",
      "         [ -2.45,   8.56,  ...,   9.44,   5.99],\n",
      "         ...,\n",
      "         [ 12.31,  -0.06,  ...,   4.66,  -1.10],\n",
      "         [ -3.94,   6.17,  ...,   1.28, -14.61]],\n",
      "\n",
      "        [[  6.77,   8.24,  ...,   0.00,   1.26],\n",
      "         [ -0.60,  -2.69,  ...,   3.52,   2.13],\n",
      "         ...,\n",
      "         [  2.05,   8.49,  ...,   1.26,  -2.53],\n",
      "         [  2.48,  -6.55,  ...,   1.81,  -5.96]]], device='cuda:0')\n",
      "v grad: tensor([[[    -8.12,     11.03,  ...,     -6.18,      3.86],\n",
      "         [     1.02,      4.19,  ...,      6.95,      0.69],\n",
      "         ...,\n",
      "         [     0.61,     -7.54,  ...,     -3.92,     -3.02],\n",
      "         [     0.42,     -1.24,  ...,     -1.13,     -1.42]],\n",
      "\n",
      "        [[   -10.09,      2.02,  ...,     -1.92,      5.45],\n",
      "         [     4.54,      2.31,  ...,      0.93,     -2.80],\n",
      "         ...,\n",
      "         [    -2.07,      0.67,  ...,      1.06,      0.16],\n",
      "         [    -0.77,      0.65,  ...,      0.17,     -0.67]],\n",
      "\n",
      "        [[     6.70,     -4.73,  ...,     -0.70,     16.60],\n",
      "         [    -0.13,      2.94,  ...,     -0.81,     -7.56],\n",
      "         ...,\n",
      "         [    -0.74,      0.05,  ...,      0.22,      1.32],\n",
      "         [    -0.00,     -0.13,  ...,     -0.13,     -0.79]],\n",
      "\n",
      "        [[    -2.89,     -7.53,  ...,     -4.00,      5.22],\n",
      "         [    -7.34,      8.66,  ...,     -2.28,     -4.29],\n",
      "         ...,\n",
      "         [     1.87,      0.44,  ...,     -0.41,      0.65],\n",
      "         [     0.52,      0.06,  ...,     -0.04,      0.31]]], device='cuda:0')\n",
      "states grad: tensor([[[[     0.08,     -0.75,  ...,     -0.13,      0.01],\n",
      "          [    -0.28,      2.54,  ...,      0.42,     -0.02],\n",
      "          ...,\n",
      "          [     0.05,     -0.50,  ...,     -0.08,      0.00],\n",
      "          [     0.13,     -1.17,  ...,     -0.19,      0.01]],\n",
      "\n",
      "         [[    -0.08,     -0.37,  ...,     -0.22,      0.06],\n",
      "          [     0.39,      1.83,  ...,      1.07,     -0.31],\n",
      "          ...,\n",
      "          [     0.36,      1.69,  ...,      0.98,     -0.28],\n",
      "          [     0.47,      2.21,  ...,      1.28,     -0.37]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -0.03,     -1.11,  ...,     -0.39,     -0.09],\n",
      "          [    -0.05,     -1.73,  ...,     -0.61,     -0.15],\n",
      "          ...,\n",
      "          [    -0.04,     -1.52,  ...,     -0.54,     -0.13],\n",
      "          [     0.09,      3.39,  ...,      1.20,      0.29]],\n",
      "\n",
      "         [[     0.16,     -0.49,  ...,     -0.44,     -0.56],\n",
      "          [     0.15,     -0.45,  ...,     -0.41,     -0.52],\n",
      "          ...,\n",
      "          [    -0.17,      0.51,  ...,      0.46,      0.58],\n",
      "          [     0.98,     -2.91,  ...,     -2.64,     -3.34]]],\n",
      "\n",
      "\n",
      "        [[[     0.18,     -0.20,  ...,      0.01,     -0.14],\n",
      "          [     0.49,     -0.55,  ...,      0.02,     -0.39],\n",
      "          ...,\n",
      "          [     1.78,     -1.97,  ...,      0.06,     -1.40],\n",
      "          [     0.79,     -0.87,  ...,      0.03,     -0.62]],\n",
      "\n",
      "         [[    -1.77,     -4.72,  ...,      1.53,     -1.92],\n",
      "          [    -0.47,     -1.27,  ...,      0.41,     -0.52],\n",
      "          ...,\n",
      "          [    -1.27,     -3.40,  ...,      1.10,     -1.39],\n",
      "          [    -1.01,     -2.70,  ...,      0.87,     -1.10]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -1.43,      0.25,  ...,      0.85,      0.51],\n",
      "          [    -0.50,      0.09,  ...,      0.30,      0.18],\n",
      "          ...,\n",
      "          [     0.92,     -0.16,  ...,     -0.54,     -0.33],\n",
      "          [     1.30,     -0.23,  ...,     -0.77,     -0.47]],\n",
      "\n",
      "         [[     0.10,     -0.09,  ...,     -0.02,      0.09],\n",
      "          [     0.79,     -0.66,  ...,     -0.18,      0.69],\n",
      "          ...,\n",
      "          [    -0.29,      0.25,  ...,      0.06,     -0.25],\n",
      "          [    -0.07,      0.06,  ...,      0.02,     -0.06]]],\n",
      "\n",
      "\n",
      "        [[[     0.58,     -0.30,  ...,      1.15,     -0.37],\n",
      "          [    -0.44,      0.23,  ...,     -0.89,      0.28],\n",
      "          ...,\n",
      "          [    -1.16,      0.59,  ...,     -2.32,      0.74],\n",
      "          [    -0.58,      0.30,  ...,     -1.17,      0.37]],\n",
      "\n",
      "         [[     0.05,     -0.23,  ...,     -0.16,      0.50],\n",
      "          [    -0.06,      0.30,  ...,      0.21,     -0.65],\n",
      "          ...,\n",
      "          [    -0.18,      0.88,  ...,      0.61,     -1.92],\n",
      "          [    -0.17,      0.82,  ...,      0.57,     -1.80]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     1.33,      0.03,  ...,     -0.27,     -1.63],\n",
      "          [     0.57,      0.01,  ...,     -0.12,     -0.69],\n",
      "          ...,\n",
      "          [    -0.56,     -0.01,  ...,      0.12,      0.69],\n",
      "          [     2.11,      0.04,  ...,     -0.43,     -2.58]],\n",
      "\n",
      "         [[     0.01,      0.26,  ...,      0.26,      1.56],\n",
      "          [    -0.01,     -0.29,  ...,     -0.30,     -1.78],\n",
      "          ...,\n",
      "          [     0.01,      0.28,  ...,      0.28,      1.68],\n",
      "          [    -0.00,     -0.08,  ...,     -0.09,     -0.51]]],\n",
      "\n",
      "\n",
      "        [[[     0.17,      1.26,  ...,      0.23,     -0.33],\n",
      "          [    -0.04,     -0.28,  ...,     -0.05,      0.07],\n",
      "          ...,\n",
      "          [    -0.15,     -1.13,  ...,     -0.21,      0.29],\n",
      "          [     0.14,      1.06,  ...,      0.19,     -0.27]],\n",
      "\n",
      "         [[     0.04,      0.21,  ...,     -0.00,     -0.12],\n",
      "          [    -0.27,     -1.35,  ...,      0.00,      0.74],\n",
      "          ...,\n",
      "          [    -0.25,     -1.29,  ...,      0.00,      0.71],\n",
      "          [    -0.16,     -0.82,  ...,      0.00,      0.45]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0.46,      0.98,  ...,     -1.13,     -1.69],\n",
      "          [     0.02,      0.04,  ...,     -0.05,     -0.07],\n",
      "          ...,\n",
      "          [     0.15,      0.32,  ...,     -0.37,     -0.55],\n",
      "          [     0.00,      0.01,  ...,     -0.01,     -0.02]],\n",
      "\n",
      "         [[     0.38,      0.04,  ...,     -0.03,      0.23],\n",
      "          [    -0.81,     -0.09,  ...,      0.06,     -0.49],\n",
      "          ...,\n",
      "          [     1.48,      0.17,  ...,     -0.12,      0.88],\n",
      "          [     0.32,      0.04,  ...,     -0.02,      0.19]]]], device='cuda:0')\n",
      "triton:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s grad: tensor([[[  4.67,   3.24,  ...,   0.00,  -1.79],\n",
      "         [ -4.18,   0.74,  ...,   1.20,   0.30],\n",
      "         ...,\n",
      "         [ -9.35,   3.38,  ...,  -1.40,  -6.46],\n",
      "         [ -0.67,  -1.96,  ...,   0.96,  -0.21]],\n",
      "\n",
      "        [[  4.61,   5.82,  ...,   0.00,   5.82],\n",
      "         [  3.33,  -2.02,  ..., -15.12,  -8.15],\n",
      "         ...,\n",
      "         [  1.39,   0.96,  ...,  -5.92,   4.77],\n",
      "         [  8.01,   5.94,  ..., -11.28,  -5.42]],\n",
      "\n",
      "        [[ -4.05,  -3.10,  ...,   0.00, -15.59],\n",
      "         [ -2.45,   8.56,  ...,   9.44,   5.99],\n",
      "         ...,\n",
      "         [ 12.31,  -0.06,  ...,   4.66,  -1.10],\n",
      "         [ -3.94,   6.17,  ...,   1.28, -14.61]],\n",
      "\n",
      "        [[  6.77,   8.24,  ...,   0.00,   1.26],\n",
      "         [ -0.60,  -2.69,  ...,   3.52,   2.13],\n",
      "         ...,\n",
      "         [  2.05,   8.49,  ...,   1.26,  -2.53],\n",
      "         [  2.48,  -6.55,  ...,   1.81,  -5.96]]], device='cuda:0')\n",
      "v grad: tensor([[[    -8.12,     11.03,  ...,     -6.18,      3.86],\n",
      "         [     1.02,      4.19,  ...,      6.95,      0.69],\n",
      "         ...,\n",
      "         [     0.61,     -7.54,  ...,     -3.92,     -3.02],\n",
      "         [     0.42,     -1.24,  ...,     -1.13,     -1.42]],\n",
      "\n",
      "        [[   -10.09,      2.02,  ...,     -1.92,      5.45],\n",
      "         [     4.54,      2.31,  ...,      0.93,     -2.80],\n",
      "         ...,\n",
      "         [    -2.07,      0.67,  ...,      1.06,      0.16],\n",
      "         [    -0.77,      0.65,  ...,      0.17,     -0.67]],\n",
      "\n",
      "        [[     6.70,     -4.73,  ...,     -0.70,     16.60],\n",
      "         [    -0.13,      2.94,  ...,     -0.81,     -7.56],\n",
      "         ...,\n",
      "         [    -0.74,      0.05,  ...,      0.22,      1.32],\n",
      "         [    -0.00,     -0.13,  ...,     -0.13,     -0.79]],\n",
      "\n",
      "        [[    -2.89,     -7.53,  ...,     -4.00,      5.22],\n",
      "         [    -7.34,      8.66,  ...,     -2.28,     -4.29],\n",
      "         ...,\n",
      "         [     1.87,      0.44,  ...,     -0.41,      0.65],\n",
      "         [     0.52,      0.06,  ...,     -0.04,      0.31]]], device='cuda:0')\n",
      "states grad: tensor([[[[     0.08,     -0.75,  ...,     -0.13,      0.01],\n",
      "          [    -0.28,      2.54,  ...,      0.42,     -0.02],\n",
      "          ...,\n",
      "          [     0.05,     -0.50,  ...,     -0.08,      0.00],\n",
      "          [     0.13,     -1.17,  ...,     -0.19,      0.01]],\n",
      "\n",
      "         [[    -0.08,     -0.37,  ...,     -0.22,      0.06],\n",
      "          [     0.39,      1.83,  ...,      1.07,     -0.31],\n",
      "          ...,\n",
      "          [     0.36,      1.69,  ...,      0.98,     -0.28],\n",
      "          [     0.47,      2.21,  ...,      1.28,     -0.37]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -0.03,     -1.11,  ...,     -0.39,     -0.09],\n",
      "          [    -0.05,     -1.73,  ...,     -0.61,     -0.15],\n",
      "          ...,\n",
      "          [    -0.04,     -1.52,  ...,     -0.54,     -0.13],\n",
      "          [     0.09,      3.39,  ...,      1.20,      0.29]],\n",
      "\n",
      "         [[     0.16,     -0.49,  ...,     -0.44,     -0.56],\n",
      "          [     0.15,     -0.45,  ...,     -0.41,     -0.52],\n",
      "          ...,\n",
      "          [    -0.17,      0.51,  ...,      0.46,      0.58],\n",
      "          [     0.98,     -2.91,  ...,     -2.64,     -3.34]]],\n",
      "\n",
      "\n",
      "        [[[     0.18,     -0.20,  ...,      0.01,     -0.14],\n",
      "          [     0.49,     -0.55,  ...,      0.02,     -0.39],\n",
      "          ...,\n",
      "          [     1.78,     -1.97,  ...,      0.06,     -1.40],\n",
      "          [     0.79,     -0.87,  ...,      0.03,     -0.62]],\n",
      "\n",
      "         [[    -1.77,     -4.72,  ...,      1.53,     -1.92],\n",
      "          [    -0.47,     -1.27,  ...,      0.41,     -0.52],\n",
      "          ...,\n",
      "          [    -1.27,     -3.40,  ...,      1.10,     -1.39],\n",
      "          [    -1.01,     -2.70,  ...,      0.87,     -1.10]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -1.43,      0.25,  ...,      0.85,      0.51],\n",
      "          [    -0.50,      0.09,  ...,      0.30,      0.18],\n",
      "          ...,\n",
      "          [     0.92,     -0.16,  ...,     -0.54,     -0.33],\n",
      "          [     1.30,     -0.23,  ...,     -0.77,     -0.47]],\n",
      "\n",
      "         [[     0.10,     -0.09,  ...,     -0.02,      0.09],\n",
      "          [     0.79,     -0.66,  ...,     -0.18,      0.69],\n",
      "          ...,\n",
      "          [    -0.29,      0.25,  ...,      0.06,     -0.25],\n",
      "          [    -0.07,      0.06,  ...,      0.02,     -0.06]]],\n",
      "\n",
      "\n",
      "        [[[     0.58,     -0.30,  ...,      1.15,     -0.37],\n",
      "          [    -0.44,      0.23,  ...,     -0.89,      0.28],\n",
      "          ...,\n",
      "          [    -1.16,      0.59,  ...,     -2.32,      0.74],\n",
      "          [    -0.58,      0.30,  ...,     -1.17,      0.37]],\n",
      "\n",
      "         [[     0.05,     -0.23,  ...,     -0.16,      0.50],\n",
      "          [    -0.06,      0.30,  ...,      0.21,     -0.65],\n",
      "          ...,\n",
      "          [    -0.18,      0.88,  ...,      0.61,     -1.92],\n",
      "          [    -0.17,      0.82,  ...,      0.57,     -1.80]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     1.33,      0.03,  ...,     -0.27,     -1.63],\n",
      "          [     0.57,      0.01,  ...,     -0.12,     -0.69],\n",
      "          ...,\n",
      "          [    -0.56,     -0.01,  ...,      0.12,      0.69],\n",
      "          [     2.11,      0.04,  ...,     -0.43,     -2.58]],\n",
      "\n",
      "         [[     0.01,      0.26,  ...,      0.26,      1.56],\n",
      "          [    -0.01,     -0.29,  ...,     -0.30,     -1.78],\n",
      "          ...,\n",
      "          [     0.01,      0.28,  ...,      0.28,      1.68],\n",
      "          [    -0.00,     -0.08,  ...,     -0.09,     -0.51]]],\n",
      "\n",
      "\n",
      "        [[[     0.17,      1.26,  ...,      0.23,     -0.33],\n",
      "          [    -0.04,     -0.28,  ...,     -0.05,      0.07],\n",
      "          ...,\n",
      "          [    -0.15,     -1.13,  ...,     -0.21,      0.29],\n",
      "          [     0.14,      1.06,  ...,      0.19,     -0.27]],\n",
      "\n",
      "         [[     0.04,      0.21,  ...,     -0.00,     -0.12],\n",
      "          [    -0.27,     -1.35,  ...,      0.00,      0.74],\n",
      "          ...,\n",
      "          [    -0.25,     -1.29,  ...,      0.00,      0.71],\n",
      "          [    -0.16,     -0.82,  ...,      0.00,      0.45]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0.46,      0.98,  ...,     -1.13,     -1.69],\n",
      "          [     0.02,      0.04,  ...,     -0.05,     -0.07],\n",
      "          ...,\n",
      "          [     0.15,      0.32,  ...,     -0.37,     -0.55],\n",
      "          [     0.00,      0.01,  ...,     -0.01,     -0.02]],\n",
      "\n",
      "         [[     0.38,      0.04,  ...,     -0.03,      0.23],\n",
      "          [    -0.81,     -0.09,  ...,      0.06,     -0.49],\n",
      "          ...,\n",
      "          [     1.48,      0.17,  ...,     -0.12,      0.88],\n",
      "          [     0.32,      0.04,  ...,     -0.02,      0.19]]]], device='cuda:0')\n",
      "max diff s: tensor(    0.00, device='cuda:0')\n",
      "max diff v: tensor(    0.00, device='cuda:0')\n",
      "max diff states: tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# try the backward pass, compare with torch autograd\n",
    "# test accumulate_folded_values_torch with always the same random inputs\n",
    "grad_output = torch.randn(B, T, C, device=device)\n",
    "print(\"grad_output:\", grad_output)\n",
    "print(\"torch:\")\n",
    "s.requires_grad = True\n",
    "v.requires_grad = True\n",
    "states.requires_grad = True\n",
    "s.grad = None\n",
    "v.grad = None\n",
    "states.grad = None\n",
    "out = accumulate_folded_all_values_torch(s, v, states, W)\n",
    "out.backward(grad_output)\n",
    "torch_s_grad = s.grad.clone()\n",
    "torch_v_grad = v.grad.clone()\n",
    "torch_states_grad = states.grad.clone()\n",
    "print('s grad:', torch_s_grad)\n",
    "print('v grad:', torch_v_grad)\n",
    "print('states grad:', torch_states_grad)\n",
    "# reset gradients\n",
    "s.grad = None\n",
    "v.grad = None\n",
    "states.grad = None\n",
    "print(\"triton:\")\n",
    "out_triton = accumulate_folded_all_values_triton(s, v, states, W)\n",
    "out_triton.backward(grad_output)\n",
    "triton_s_grad = s.grad.clone()\n",
    "triton_v_grad = v.grad.clone()\n",
    "triton_states_grad = states.grad.clone()\n",
    "print('s grad:', triton_s_grad)\n",
    "print('v grad:', triton_v_grad)\n",
    "print('states grad:', triton_states_grad)\n",
    "print(\"max diff s:\", (torch_s_grad - triton_s_grad).abs().max())\n",
    "print(\"max diff v:\", (torch_v_grad - triton_v_grad).abs().max())\n",
    "print(\"max diff states:\", (torch_states_grad - triton_states_grad).abs().max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch forward:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.1 ms Â± 17 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n",
      "torch forward & backward:\n",
      "58.6 ms Â± 6.04 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "triton forward:\n",
      "5.54 ms Â± 262 Î¼s per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n",
      "triton forward & backward:\n",
      "21 ms Â± 736 Î¼s per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# measure the speedup\n",
    "B, T, W, S, C = 128, 1024, 32, 32, 64\n",
    "s = torch.randn(B, T, S+W, device=device)\n",
    "v = torch.randn(B, T, C, device=device)\n",
    "states = torch.randn(B, T, S, C, device=device)\n",
    "grad_output = torch.randn(B, T, C, device=device)\n",
    "# warmup the triton kernel\n",
    "s.requires_grad = True\n",
    "v.requires_grad = True\n",
    "states.requires_grad = True\n",
    "out = accumulate_folded_all_values_triton(s, v, states, W)\n",
    "out.backward(grad_output)\n",
    "s.grad = None\n",
    "v.grad = None\n",
    "\n",
    "print(\"torch forward:\")\n",
    "s.requires_grad = True\n",
    "v.requires_grad = True\n",
    "states.requires_grad = True\n",
    "%timeit accumulate_folded_all_values_torch(s, v, states, W)\n",
    "print(\"torch forward & backward:\")\n",
    "def ftorch():\n",
    "    s.grad = None\n",
    "    v.grad = None\n",
    "    out_torch = accumulate_folded_all_values_torch(s, v, states, W)\n",
    "    out_torch.backward(grad_output)\n",
    "%timeit ftorch()\n",
    "print(\"\\ntriton forward:\")\n",
    "s.requires_grad = True\n",
    "v.requires_grad = True\n",
    "states.requires_grad = True\n",
    "%timeit accumulate_folded_all_values_triton(s, v, states, W)\n",
    "print(\"triton forward & backward:\")\n",
    "def ftriton():\n",
    "    s.grad = None\n",
    "    v.grad = None\n",
    "    out_triton = accumulate_folded_all_values_triton(s, v, states, W)\n",
    "    out_triton.backward(grad_output)\n",
    "%timeit ftriton()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Gating Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no reverse cumprod in pytorch :(\n",
    "def reverse_cumprod(x, dim=-1):\n",
    "    # cp = torch.cumprod(x, dim=dim)\n",
    "    # i = [slice(None)] * x.dim()\n",
    "    # i[dim] = -1\n",
    "    # return (x / (cp + 1e-8)) * cp[i].unsqueeze(dim)\n",
    "    return torch.flip(torch.cumprod(torch.flip(x, [dim]), dim), [dim])\n",
    "\n",
    "# this is for stator and integrator in SCAN\n",
    "# @torch.compile\n",
    "def cum_gating_2d_torch(x, g):\n",
    "    # x is (B, T, C) and g is (B, T, S), return (B, T, S, C)\n",
    "    # the parallel cumulative version of the recursion o[i] = o[i-1](1-g[i])^T + x[i]g[i]^T\n",
    "    inv_g = 1 - g\n",
    "    r_inv_g = reverse_cumprod(inv_g, dim=1)\n",
    "    r_inv_g = torch.cat((r_inv_g[:, 1:], torch.ones_like(r_inv_g[:, -1:])), dim=1)\n",
    "    g_r_inv_g = g * r_inv_g\n",
    "    x_g_r_inv_g = torch.einsum('bts,btc->btsc', g_r_inv_g, x)\n",
    "    c_x_g_r_inv_g = torch.cumsum(x_g_r_inv_g, dim=1)\n",
    "    return c_x_g_r_inv_g / (r_inv_g.unsqueeze(-1) + 1e-8) # avoid division by zero\n",
    "\n",
    "def g_sliding_window_fold(x, window_len):\n",
    "    # x is a (B, T, C) tensor\n",
    "    # output should be a (B, T, window_len, C) tensor that slides over the T dimension\n",
    "    padded_x = F.pad(x, (0, 0, 0, window_len - 1), mode='constant', value=1)\n",
    "    output = padded_x.unfold(dimension=1, size=window_len, step=1).permute(0, 1, 3, 2)\n",
    "    return output\n",
    "\n",
    "# for debugging, now a naive version\n",
    "def cum_gating_2d_naive(x, g, gi=None):\n",
    "    # x is (B, T, C) and g is (B, T, S), return (B, T, S, C)\n",
    "    # the naive iterative version of the recursion o[i] = o[i-1](1-g[i])^T + x[i]g[i]^T\n",
    "    B, T, C = x.shape\n",
    "    S = g.shape[-1]\n",
    "    x_g = torch.einsum('bts,btc->btsc', g, x)\n",
    "    \n",
    "    out_list = [x_g[:, 0].unsqueeze(1)]  # Start with the first time step\n",
    "    complement_g = 1 - g if gi is None else gi\n",
    "    \n",
    "    for i in range(1, T):\n",
    "        prev_out = out_list[-1][:, -1]  # Get the last time step from previous output\n",
    "        current_out = torch.einsum('bsc,bs->bsc', prev_out, complement_g[:, i]) + x_g[:, i]\n",
    "        out_list.append(current_out.unsqueeze(1))\n",
    "    \n",
    "    return torch.cat(out_list, dim=1)\n",
    "\n",
    "# wait a second... we can do all that CUDA/triton multistage accumulation stuff in pytorch.. right??\n",
    "# @torch.compile\n",
    "# def cum_gating_2d_torch_multistage(x, g):\n",
    "#     xg = torch.einsum('bts,btc->btsc', g, x)\n",
    "#     gi = 1 - g\n",
    "#     B, T, S, C = xg.shape\n",
    "#     halfT = T // 2\n",
    "#     nstages = math.ceil(math.log2(T))\n",
    "#     for stage in range(nstages):\n",
    "#         group_stride = 1 << stage\n",
    "#         initial_indices = group_stride + ((torch.arange(halfT) // group_stride) * group_stride * 2)\n",
    "#         t_targets = initial_indices + (torch.arange(halfT) % group_stride)\n",
    "#         t_adders = initial_indices - 1\n",
    "#         inverse_gates = torch.cumprod(gi[:, t_targets, :].reshape(B, halfT//group_stride, group_stride, S), dim=-2).reshape(B, halfT, S)\n",
    "#         xg[:, t_targets] += xg[:, t_adders] * inverse_gates.unsqueeze(-1)\n",
    "#     return xg\n",
    "\n",
    "# this should be faster but somehow it's slower???\n",
    "# @torch.compile\n",
    "def cum_gating_2d_torch_multistage(x, g):\n",
    "    xg = torch.einsum('bts,btc->btsc', g, x)\n",
    "    gi = 1 - g\n",
    "    B, T, S, C = xg.shape\n",
    "    halfT = T // 2\n",
    "    nstages = math.ceil(math.log2(T))\n",
    "    t_arange = torch.arange(halfT)\n",
    "    for stage in range(nstages):\n",
    "        group_stride = 1 << stage\n",
    "        initial_indices = group_stride + ((t_arange // group_stride) * group_stride * 2)\n",
    "        t_targets = initial_indices + (t_arange % group_stride)\n",
    "        t_adders = initial_indices - 1\n",
    "        xg[:, t_targets] += xg[:, t_adders] * gi[:, t_targets].unsqueeze(-1)\n",
    "        gi[:, t_targets] *= gi[:, t_adders]\n",
    "    return xg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def cg2d_fwd_kernel(\n",
    "    xg_ptr, gi_ptr, \n",
    "    B, S, C,\n",
    "    T: tl.constexpr,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "    # Add more constants for tiling\n",
    "    BLOCK_SIZE_S: tl.constexpr,\n",
    "    BLOCK_SIZE_C: tl.constexpr,\n",
    "):\n",
    "    # Use multiple program IDs for better parallelization\n",
    "    pid = tl.program_id(axis=0)\n",
    "    # Compute batch, spatial, and channel indices\n",
    "    num_s_blocks = tl.cdiv(S, BLOCK_SIZE_S)\n",
    "    num_c_blocks = tl.cdiv(C, BLOCK_SIZE_C)\n",
    "    b = pid // (num_s_blocks * num_c_blocks)\n",
    "    rem = pid % (num_s_blocks * num_c_blocks)\n",
    "    s_block = rem // num_c_blocks\n",
    "    c_block = rem % num_c_blocks\n",
    "\n",
    "    # Compute actual indices\n",
    "    s_offs = tl.arange(0, BLOCK_SIZE_S)\n",
    "    c_offs = tl.arange(0, BLOCK_SIZE_C)\n",
    "    s_mask = s_offs < (S - s_block * BLOCK_SIZE_S)\n",
    "    c_mask = c_offs < (C - c_block * BLOCK_SIZE_C)\n",
    "    s_offs = s_block * BLOCK_SIZE_S + s_offs\n",
    "    c_offs = c_block * BLOCK_SIZE_C + c_offs\n",
    "\n",
    "    # Compute base pointers\n",
    "    xg_base = xg_ptr + b * T * S * C\n",
    "    gi_base = gi_ptr + b * T * S\n",
    "\n",
    "    # Precompute stages for better efficiency\n",
    "    nstages = tl.ceil(tl.log2(float(T))).to(tl.int32)\n",
    "    \n",
    "    for stage in range(nstages):\n",
    "        group_stride = 1 << stage\n",
    "        # Process multiple elements per thread using BLOCK_SIZE\n",
    "        for block_start in range(0, T//2, BLOCK_SIZE):\n",
    "            offs = tl.arange(0, BLOCK_SIZE)\n",
    "            block_mask = offs < (T//2 - block_start)\n",
    "            block_s_mask = block_mask[:, None] & s_mask[None, :]\n",
    "            block_s_c_mask = block_mask[:, None, None] & s_mask[None, :, None] & c_mask[None, None, :]\n",
    "            \n",
    "            # Compute indices with vectorization\n",
    "            initial_indices = group_stride + ((offs + block_start) // group_stride) * group_stride * 2\n",
    "            t_targets = initial_indices + ((offs + block_start) % group_stride)\n",
    "            t_adders = initial_indices - 1\n",
    "\n",
    "            xg_targets_ptr = xg_base + t_targets[:, None, None] * S * C + s_offs[None, :, None] * C + c_offs[None, None, :]\n",
    "            xg_adders_ptr = xg_base + t_adders[:, None, None] * S * C + s_offs[None, :, None] * C + c_offs[None, None, :]\n",
    "            gi_targets_ptr = gi_base + t_targets[:, None] * S + s_offs[None, :]\n",
    "            gi_adders_ptr = gi_base + t_adders[:, None] * S + s_offs[None, :]\n",
    "            \n",
    "            # Load data with block masking\n",
    "            gi_targets = tl.load(gi_targets_ptr, mask=block_s_mask)\n",
    "            \n",
    "            # Use block-level operations for better memory access patterns\n",
    "            xg_targets = tl.load(xg_targets_ptr, mask=block_s_c_mask)\n",
    "            \n",
    "            xg_adders = tl.load(xg_adders_ptr, mask=block_s_c_mask)\n",
    "            \n",
    "            # Compute and store results\n",
    "            xg_targets += xg_adders * gi_targets[:, :, None]\n",
    "            tl.store(xg_targets_ptr, xg_targets, mask=block_s_c_mask)\n",
    "            \n",
    "            # Update gates\n",
    "            gi_adders = tl.load(gi_adders_ptr, mask=block_s_mask)\n",
    "            gi_targets *= gi_adders\n",
    "            tl.store(gi_targets_ptr, gi_targets, mask=block_s_mask)\n",
    "\n",
    "@triton.jit\n",
    "def cg2d_bwd_kernel(\n",
    "    gi_ptr, go_ptr, y_ptr, grad_gi_ptr,\n",
    "    B, S, C,\n",
    "    T: tl.constexpr,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "    BLOCK_SIZE_S: tl.constexpr,\n",
    "    BLOCK_SIZE_C: tl.constexpr,\n",
    "):\n",
    "    # Similar structure to forward kernel with reversed indices\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_s_blocks = tl.cdiv(S, BLOCK_SIZE_S)\n",
    "    num_c_blocks = tl.cdiv(C, BLOCK_SIZE_C)\n",
    "    b = pid // (num_s_blocks * num_c_blocks)\n",
    "    rem = pid % (num_s_blocks * num_c_blocks)\n",
    "    s_block = rem // num_c_blocks\n",
    "    c_block = rem % num_c_blocks\n",
    "\n",
    "    s_offs = tl.arange(0, BLOCK_SIZE_S)\n",
    "    c_offs = tl.arange(0, BLOCK_SIZE_C)\n",
    "    s_mask = s_offs < (S - s_block * BLOCK_SIZE_S)\n",
    "    c_mask = c_offs < (C - c_block * BLOCK_SIZE_C)\n",
    "    s_offs = s_block * BLOCK_SIZE_S + s_offs\n",
    "    c_offs = c_block * BLOCK_SIZE_C + c_offs\n",
    "\n",
    "    gi_base = gi_ptr + b * T * S\n",
    "    go_base = go_ptr + b * T * S * C\n",
    "\n",
    "    nstages = tl.ceil(tl.log2(float(T))).to(tl.int32)\n",
    "    \n",
    "    for stage in range(nstages):\n",
    "        group_stride = 1 << stage\n",
    "        for block_start in range(0, T//2, BLOCK_SIZE):\n",
    "            offs = tl.arange(0, BLOCK_SIZE)\n",
    "            block_mask = offs < (T//2 - block_start)\n",
    "            block_s_mask = block_mask[:, None] & s_mask[None, :]\n",
    "            block_s_c_mask = block_mask[:, None, None] & s_mask[None, :, None] & c_mask[None, None, :]\n",
    "            \n",
    "            initial_indices = T - 1 - group_stride - ((offs + block_start) // group_stride) * group_stride * 2\n",
    "            t_targets = initial_indices - ((offs + block_start) % group_stride)\n",
    "            t_adders = initial_indices + 1\n",
    "\n",
    "            go_targets_ptr = go_base + t_targets[:, None, None] * S * C + s_offs[None, :, None] * C + c_offs[None, None, :]\n",
    "            go_adders_ptr = go_base + t_adders[:, None, None] * S * C + s_offs[None, :, None] * C + c_offs[None, None, :]\n",
    "            gi_targets_ptr = gi_base + t_targets[:, None] * S + s_offs[None, :]\n",
    "            gi_adders_ptr = gi_base + t_adders[:, None] * S + s_offs[None, :]\n",
    "            \n",
    "            # Load with block masking\n",
    "            gi_targets = tl.load(gi_targets_ptr, mask=block_s_mask)\n",
    "            \n",
    "            go_targets = tl.load(go_targets_ptr, mask=block_s_c_mask)\n",
    "            \n",
    "            go_adders = tl.load(go_adders_ptr, mask=block_s_c_mask)\n",
    "            \n",
    "            # Compute and store results\n",
    "            go_targets += go_adders * gi_targets[:, :, None]\n",
    "            tl.store(go_targets_ptr, go_targets, mask=block_s_c_mask)\n",
    "            \n",
    "            gi_adders = tl.load(gi_adders_ptr,\n",
    "                              mask=block_s_mask)\n",
    "            gi_targets *= gi_adders\n",
    "            tl.store(gi_targets_ptr, gi_targets, mask=block_s_mask)\n",
    "\n",
    "    # Compute grad_gi\n",
    "    # torch:\n",
    "    # grad_gi = grad_output * y\n",
    "    # grad_gi = grad_gi.sum(-1)\n",
    "    grad_gi_base = grad_gi_ptr + b * T * S\n",
    "    s_first_id = s_block * BLOCK_SIZE_S\n",
    "    c_first_id = c_block * BLOCK_SIZE_C\n",
    "    t_offs = tl.arange(0, BLOCK_SIZE)\n",
    "    for block_start in range(0, T, BLOCK_SIZE):\n",
    "        # We can use make_block_ptr since the blocks we need are contiguous\n",
    "        go_block_ptr = tl.make_block_ptr(\n",
    "            base=go_ptr,\n",
    "            shape=(B, T, S, C),\n",
    "            strides=(T * S * C, S * C, C, 1),\n",
    "            offsets=(b, block_start, s_first_id, c_first_id),\n",
    "            block_shape=(1, BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C),\n",
    "            order=(0, 1, 2, 3)\n",
    "        )\n",
    "        go_block = tl.load(go_block_ptr) # (1, BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C)\n",
    "        y_block_ptr = tl.make_block_ptr(\n",
    "            base=y_ptr,\n",
    "            shape=(B, T, S, C),\n",
    "            strides=(T * S * C, S * C, C, 1),\n",
    "            offsets=(b, block_start, s_first_id, c_first_id), # y is already shifted to the right by 1\n",
    "            block_shape=(1, BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C),\n",
    "            order=(0, 1, 2, 3)\n",
    "        )\n",
    "        y_block = tl.load(y_block_ptr) # (1, BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C)\n",
    "\n",
    "        grad_gi = go_block * y_block  # (1, BLOCK_SIZE, BLOCK_SIZE_S, BLOCK_SIZE_C)\n",
    "        grad_gi = tl.sum(grad_gi, axis=-1)  # (1, BLOCK_SIZE, BLOCK_SIZE_S)\n",
    "\n",
    "        # Need to use atomic add for accumulation between S blocks, so we also need to use manual pointer bc it's what atomic add accepts\n",
    "        t_mask = t_offs < (T - block_start)\n",
    "        t_offs = block_start + t_offs\n",
    "        grad_gi_block_ptr = grad_gi_base + t_offs[:, None] * S + s_offs[None, :]\n",
    "        grad_gi_mask = t_mask[:, None] & s_mask[None, :]\n",
    "        tl.atomic_add(grad_gi_block_ptr[None, :], grad_gi, mask=grad_gi_mask[None, :])\n",
    "\n",
    "class CumulativeGating2DTriton(torch.autograd.Function):\n",
    "    # @torch.compiler.disable\n",
    "    @staticmethod\n",
    "    def forward(ctx, xg, gi):\n",
    "        xg = xg.contiguous()\n",
    "        gi = gi.contiguous()\n",
    "        orig_gi = gi.clone()\n",
    "        B, T, S, C = xg.shape\n",
    "        \n",
    "        # Configure block sizes\n",
    "        BLOCK_SIZE = 32\n",
    "        BLOCK_SIZE_S = 16\n",
    "        BLOCK_SIZE_C = 16\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        num_s_blocks = triton.cdiv(S, BLOCK_SIZE_S)\n",
    "        num_c_blocks = triton.cdiv(C, BLOCK_SIZE_C)\n",
    "        grid = (B * num_s_blocks * num_c_blocks,)\n",
    "        \n",
    "        # Launch kernel\n",
    "        cg2d_fwd_kernel[grid](\n",
    "            xg, gi,\n",
    "            B, S, C, T,\n",
    "            BLOCK_SIZE=BLOCK_SIZE,\n",
    "            BLOCK_SIZE_S=BLOCK_SIZE_S,\n",
    "            BLOCK_SIZE_C=BLOCK_SIZE_C,\n",
    "        )\n",
    "        \n",
    "        ctx.save_for_backward(xg, orig_gi)\n",
    "        return xg\n",
    "    \n",
    "    # @torch.compiler.disable\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.contiguous()\n",
    "        y, gi = ctx.saved_tensors\n",
    "        B, T, S, C = y.shape\n",
    "        \n",
    "        # Configure block sizes\n",
    "        BLOCK_SIZE = 32\n",
    "        BLOCK_SIZE_S = 16\n",
    "        BLOCK_SIZE_C = 16\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        num_s_blocks = triton.cdiv(S, BLOCK_SIZE_S)\n",
    "        num_c_blocks = triton.cdiv(C, BLOCK_SIZE_C)\n",
    "        grid = (B * num_s_blocks * num_c_blocks,)\n",
    "        \n",
    "        gi = torch.cat((gi[:, 1:], torch.ones_like(gi[:, -1:])), dim=1).contiguous()\n",
    "        grad_xg = grad_output.clone()\n",
    "        y = torch.cat((torch.zeros_like(y[:, :1]), y[:, :-1]), dim=1).contiguous()\n",
    "        grad_gi = torch.zeros_like(gi)\n",
    "\n",
    "        # Launch kernel\n",
    "        cg2d_bwd_kernel[grid](\n",
    "            gi, grad_xg, y, grad_gi,\n",
    "            B, S, C, T,\n",
    "            BLOCK_SIZE=BLOCK_SIZE,\n",
    "            BLOCK_SIZE_S=BLOCK_SIZE_S,\n",
    "            BLOCK_SIZE_C=BLOCK_SIZE_C,\n",
    "        )\n",
    "\n",
    "        return grad_xg, grad_gi\n",
    "\n",
    "# @torch.compile\n",
    "def cum_gating_2d_triton(x, g):\n",
    "    x_g = torch.einsum('bts,btc->btsc', g, x)\n",
    "    inv_g = 1 - g\n",
    "    return CumulativeGating2DTriton.apply(x_g, inv_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[[-2.31, -1.31,  ..., -0.20,  0.20],\n",
      "         [ 1.35, -0.10,  ...,  0.29,  1.35],\n",
      "         ...,\n",
      "         [ 1.62,  0.82,  ..., -0.08, -0.32],\n",
      "         [ 1.24,  0.61,  ...,  1.52, -3.05]],\n",
      "\n",
      "        [[-0.18,  0.15,  ...,  0.05,  1.57],\n",
      "         [ 2.32, -0.08,  ..., -0.59,  0.50],\n",
      "         ...,\n",
      "         [-0.70, -1.66,  ...,  0.26,  0.77],\n",
      "         [-1.83,  1.61,  ...,  0.77, -0.19]]], device='cuda:0')\n",
      "g: tensor([[[0.02, 0.01,  ..., 0.03, 0.06],\n",
      "         [0.01, 0.03,  ..., 0.03, 0.01],\n",
      "         ...,\n",
      "         [0.01, 0.02,  ..., 0.00, 0.12],\n",
      "         [0.00, 0.01,  ..., 0.01, 0.20]],\n",
      "\n",
      "        [[0.01, 0.00,  ..., 0.02, 0.03],\n",
      "         [0.02, 0.04,  ..., 0.18, 0.02],\n",
      "         ...,\n",
      "         [0.01, 0.02,  ..., 0.01, 0.01],\n",
      "         [0.04, 0.01,  ..., 0.03, 0.04]]], device='cuda:0')\n",
      "torch:\n",
      "tensor([[[[    -0.04,     -0.02,  ...,     -0.00,      0.00],\n",
      "          [    -0.03,     -0.02,  ...,     -0.00,      0.00],\n",
      "          ...,\n",
      "          [    -0.07,     -0.04,  ...,     -0.01,      0.01],\n",
      "          [    -0.14,     -0.08,  ...,     -0.01,      0.01]],\n",
      "\n",
      "         [[    -0.03,     -0.03,  ...,     -0.00,      0.02],\n",
      "          [     0.00,     -0.02,  ...,      0.00,      0.04],\n",
      "          ...,\n",
      "          [    -0.03,     -0.04,  ...,      0.00,      0.04],\n",
      "          [    -0.14,     -0.08,  ...,     -0.01,      0.02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0.07,      0.04,  ...,     -0.00,     -0.00],\n",
      "          [     0.20,      0.14,  ...,     -0.23,     -0.16],\n",
      "          ...,\n",
      "          [    -0.06,     -0.05,  ...,      0.12,     -0.33],\n",
      "          [     0.37,      0.05,  ...,     -0.03,     -0.33]],\n",
      "\n",
      "         [[     0.08,      0.04,  ...,      0.00,     -0.01],\n",
      "          [     0.21,      0.15,  ...,     -0.21,     -0.19],\n",
      "          ...,\n",
      "          [    -0.05,     -0.05,  ...,      0.13,     -0.35],\n",
      "          [     0.54,      0.16,  ...,      0.28,     -0.87]]],\n",
      "\n",
      "\n",
      "        [[[    -0.00,      0.00,  ...,      0.00,      0.02],\n",
      "          [    -0.00,      0.00,  ...,      0.00,      0.00],\n",
      "          ...,\n",
      "          [    -0.00,      0.00,  ...,      0.00,      0.03],\n",
      "          [    -0.00,      0.00,  ...,      0.00,      0.04]],\n",
      "\n",
      "         [[     0.03,      0.00,  ...,     -0.01,      0.02],\n",
      "          [     0.10,     -0.00,  ...,     -0.03,      0.02],\n",
      "          ...,\n",
      "          [     0.42,     -0.01,  ...,     -0.11,      0.11],\n",
      "          [     0.04,      0.00,  ...,     -0.01,      0.05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -0.43,     -0.01,  ...,      0.43,      0.08],\n",
      "          [    -0.09,     -0.03,  ...,      0.12,     -0.34],\n",
      "          ...,\n",
      "          [     0.08,     -0.03,  ...,      0.14,     -0.04],\n",
      "          [    -0.34,     -0.03,  ...,      0.18,      0.03]],\n",
      "\n",
      "         [[    -0.48,      0.05,  ...,      0.44,      0.07],\n",
      "          [    -0.11,     -0.01,  ...,      0.12,     -0.33],\n",
      "          ...,\n",
      "          [     0.03,      0.02,  ...,      0.16,     -0.05],\n",
      "          [    -0.40,      0.04,  ...,      0.20,      0.02]]]], device='cuda:0')\n",
      "naive:\n",
      "tensor([[[[    -0.04,     -0.02,  ...,     -0.00,      0.00],\n",
      "          [    -0.03,     -0.02,  ...,     -0.00,      0.00],\n",
      "          ...,\n",
      "          [    -0.07,     -0.04,  ...,     -0.01,      0.01],\n",
      "          [    -0.14,     -0.08,  ...,     -0.01,      0.01]],\n",
      "\n",
      "         [[    -0.03,     -0.03,  ...,     -0.00,      0.02],\n",
      "          [     0.00,     -0.02,  ...,      0.00,      0.04],\n",
      "          ...,\n",
      "          [    -0.03,     -0.04,  ...,      0.00,      0.04],\n",
      "          [    -0.14,     -0.08,  ...,     -0.01,      0.02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0.07,      0.04,  ...,     -0.00,     -0.00],\n",
      "          [     0.20,      0.14,  ...,     -0.23,     -0.16],\n",
      "          ...,\n",
      "          [    -0.06,     -0.05,  ...,      0.12,     -0.33],\n",
      "          [     0.37,      0.05,  ...,     -0.03,     -0.33]],\n",
      "\n",
      "         [[     0.08,      0.04,  ...,      0.00,     -0.01],\n",
      "          [     0.21,      0.15,  ...,     -0.21,     -0.19],\n",
      "          ...,\n",
      "          [    -0.05,     -0.05,  ...,      0.13,     -0.35],\n",
      "          [     0.54,      0.16,  ...,      0.28,     -0.87]]],\n",
      "\n",
      "\n",
      "        [[[    -0.00,      0.00,  ...,      0.00,      0.02],\n",
      "          [    -0.00,      0.00,  ...,      0.00,      0.00],\n",
      "          ...,\n",
      "          [    -0.00,      0.00,  ...,      0.00,      0.03],\n",
      "          [    -0.00,      0.00,  ...,      0.00,      0.04]],\n",
      "\n",
      "         [[     0.03,      0.00,  ...,     -0.01,      0.02],\n",
      "          [     0.10,     -0.00,  ...,     -0.03,      0.02],\n",
      "          ...,\n",
      "          [     0.42,     -0.01,  ...,     -0.11,      0.11],\n",
      "          [     0.04,      0.00,  ...,     -0.01,      0.05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -0.43,     -0.01,  ...,      0.43,      0.08],\n",
      "          [    -0.09,     -0.03,  ...,      0.12,     -0.34],\n",
      "          ...,\n",
      "          [     0.08,     -0.03,  ...,      0.14,     -0.04],\n",
      "          [    -0.34,     -0.03,  ...,      0.18,      0.03]],\n",
      "\n",
      "         [[    -0.48,      0.05,  ...,      0.44,      0.07],\n",
      "          [    -0.11,     -0.01,  ...,      0.12,     -0.33],\n",
      "          ...,\n",
      "          [     0.03,      0.02,  ...,      0.16,     -0.05],\n",
      "          [    -0.40,      0.04,  ...,      0.20,      0.02]]]], device='cuda:0')\n",
      "torch multistage:\n",
      "tensor([[[[    -0.04,     -0.02,  ...,     -0.00,      0.00],\n",
      "          [    -0.03,     -0.02,  ...,     -0.00,      0.00],\n",
      "          ...,\n",
      "          [    -0.07,     -0.04,  ...,     -0.01,      0.01],\n",
      "          [    -0.14,     -0.08,  ...,     -0.01,      0.01]],\n",
      "\n",
      "         [[    -0.03,     -0.03,  ...,     -0.00,      0.02],\n",
      "          [     0.00,     -0.02,  ...,      0.00,      0.04],\n",
      "          ...,\n",
      "          [    -0.03,     -0.04,  ...,      0.00,      0.04],\n",
      "          [    -0.14,     -0.08,  ...,     -0.01,      0.02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0.07,      0.04,  ...,     -0.00,     -0.00],\n",
      "          [     0.20,      0.14,  ...,     -0.23,     -0.16],\n",
      "          ...,\n",
      "          [    -0.06,     -0.05,  ...,      0.12,     -0.33],\n",
      "          [     0.37,      0.05,  ...,     -0.03,     -0.33]],\n",
      "\n",
      "         [[     0.08,      0.04,  ...,      0.00,     -0.01],\n",
      "          [     0.21,      0.15,  ...,     -0.21,     -0.19],\n",
      "          ...,\n",
      "          [    -0.05,     -0.05,  ...,      0.13,     -0.35],\n",
      "          [     0.54,      0.16,  ...,      0.28,     -0.87]]],\n",
      "\n",
      "\n",
      "        [[[    -0.00,      0.00,  ...,      0.00,      0.02],\n",
      "          [    -0.00,      0.00,  ...,      0.00,      0.00],\n",
      "          ...,\n",
      "          [    -0.00,      0.00,  ...,      0.00,      0.03],\n",
      "          [    -0.00,      0.00,  ...,      0.00,      0.04]],\n",
      "\n",
      "         [[     0.03,      0.00,  ...,     -0.01,      0.02],\n",
      "          [     0.10,     -0.00,  ...,     -0.03,      0.02],\n",
      "          ...,\n",
      "          [     0.42,     -0.01,  ...,     -0.11,      0.11],\n",
      "          [     0.04,      0.00,  ...,     -0.01,      0.05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -0.43,     -0.01,  ...,      0.43,      0.08],\n",
      "          [    -0.09,     -0.03,  ...,      0.12,     -0.34],\n",
      "          ...,\n",
      "          [     0.08,     -0.03,  ...,      0.14,     -0.04],\n",
      "          [    -0.34,     -0.03,  ...,      0.18,      0.03]],\n",
      "\n",
      "         [[    -0.48,      0.05,  ...,      0.44,      0.07],\n",
      "          [    -0.11,     -0.01,  ...,      0.12,     -0.33],\n",
      "          ...,\n",
      "          [     0.03,      0.02,  ...,      0.16,     -0.05],\n",
      "          [    -0.40,      0.04,  ...,      0.20,      0.02]]]], device='cuda:0')\n",
      "triton:\n",
      "tensor([[[[    -0.04,     -0.02,  ...,     -0.00,      0.00],\n",
      "          [    -0.03,     -0.02,  ...,     -0.00,      0.00],\n",
      "          ...,\n",
      "          [    -0.07,     -0.04,  ...,     -0.01,      0.01],\n",
      "          [    -0.14,     -0.08,  ...,     -0.01,      0.01]],\n",
      "\n",
      "         [[    -0.03,     -0.03,  ...,     -0.00,      0.02],\n",
      "          [     0.00,     -0.02,  ...,      0.00,      0.04],\n",
      "          ...,\n",
      "          [    -0.03,     -0.04,  ...,      0.00,      0.04],\n",
      "          [    -0.14,     -0.08,  ...,     -0.01,      0.02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[     0.07,      0.04,  ...,     -0.00,     -0.00],\n",
      "          [     0.20,      0.14,  ...,     -0.23,     -0.16],\n",
      "          ...,\n",
      "          [    -0.06,     -0.05,  ...,      0.12,     -0.33],\n",
      "          [     0.37,      0.05,  ...,     -0.03,     -0.33]],\n",
      "\n",
      "         [[     0.08,      0.04,  ...,      0.00,     -0.01],\n",
      "          [     0.21,      0.15,  ...,     -0.21,     -0.19],\n",
      "          ...,\n",
      "          [    -0.05,     -0.05,  ...,      0.13,     -0.35],\n",
      "          [     0.54,      0.16,  ...,      0.28,     -0.87]]],\n",
      "\n",
      "\n",
      "        [[[    -0.00,      0.00,  ...,      0.00,      0.02],\n",
      "          [    -0.00,      0.00,  ...,      0.00,      0.00],\n",
      "          ...,\n",
      "          [    -0.00,      0.00,  ...,      0.00,      0.03],\n",
      "          [    -0.00,      0.00,  ...,      0.00,      0.04]],\n",
      "\n",
      "         [[     0.03,      0.00,  ...,     -0.01,      0.02],\n",
      "          [     0.10,     -0.00,  ...,     -0.03,      0.02],\n",
      "          ...,\n",
      "          [     0.42,     -0.01,  ...,     -0.11,      0.11],\n",
      "          [     0.04,      0.00,  ...,     -0.01,      0.05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[    -0.43,     -0.01,  ...,      0.43,      0.08],\n",
      "          [    -0.09,     -0.03,  ...,      0.12,     -0.34],\n",
      "          ...,\n",
      "          [     0.08,     -0.03,  ...,      0.14,     -0.04],\n",
      "          [    -0.34,     -0.03,  ...,      0.18,      0.03]],\n",
      "\n",
      "         [[    -0.48,      0.05,  ...,      0.44,      0.07],\n",
      "          [    -0.11,     -0.01,  ...,      0.12,     -0.33],\n",
      "          ...,\n",
      "          [     0.03,      0.02,  ...,      0.16,     -0.05],\n",
      "          [    -0.40,      0.04,  ...,      0.20,      0.02]]]], device='cuda:0')\n",
      "max diff: tensor(    0.00, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# try the forward pass\n",
    "# test cumulative_gating_2d_torch with always the same random inputs\n",
    "B, T, S, W, C = 2, 64, 32, 4, 32\n",
    "x = torch.randn(B, T, C, device=device)\n",
    "g = F.softmax(torch.randn(B, T, S, device=device), dim=-1)\n",
    "gi = 1 - g\n",
    "print(\"x:\", x)\n",
    "print(\"g:\", g)\n",
    "print(\"torch:\")\n",
    "print(cum_gating_2d_torch(x, g))\n",
    "print(\"naive:\")\n",
    "out_naive = cum_gating_2d_naive(x, g)\n",
    "print(out_naive)\n",
    "print(\"torch multistage:\")\n",
    "out = cum_gating_2d_torch_multistage(x, g)\n",
    "print(out)\n",
    "print(\"triton:\")\n",
    "out = cum_gating_2d_triton(x, g)\n",
    "print(out)\n",
    "print(\"max diff:\", (out - out_naive).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_output: tensor([[[[-0.66, -1.60,  ..., -1.05,  1.05],\n",
      "          [ 0.50,  1.34,  ...,  0.82,  0.47],\n",
      "          ...,\n",
      "          [-1.18, -0.78,  ...,  0.43, -0.75],\n",
      "          [-0.52, -1.36,  ...,  1.93, -0.38]],\n",
      "\n",
      "         [[-0.60,  0.89,  ...,  0.60, -0.06],\n",
      "          [ 1.65, -0.39,  ..., -0.37, -1.88],\n",
      "          ...,\n",
      "          [-0.98,  0.45,  ..., -0.17,  0.74],\n",
      "          [-0.11, -0.55,  ..., -0.50, -0.08]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.14, -0.89,  ..., -0.56,  1.46],\n",
      "          [ 1.94,  0.24,  ..., -0.25, -2.59],\n",
      "          ...,\n",
      "          [ 1.14, -1.70,  ..., -0.02,  0.38],\n",
      "          [-1.66,  0.05,  ..., -0.90,  0.12]],\n",
      "\n",
      "         [[ 0.97, -0.03,  ...,  0.30,  1.32],\n",
      "          [-1.76, -0.51,  ..., -0.64,  0.21],\n",
      "          ...,\n",
      "          [-1.11, -0.58,  ..., -1.34, -0.48],\n",
      "          [ 1.93, -1.22,  ...,  0.08,  0.51]]],\n",
      "\n",
      "\n",
      "        [[[-0.74, -0.01,  ...,  0.52, -0.18],\n",
      "          [ 0.09, -0.65,  ...,  0.12, -0.13],\n",
      "          ...,\n",
      "          [ 1.83,  0.77,  ...,  0.32,  1.33],\n",
      "          [ 1.82, -0.07,  ...,  0.05,  1.36]],\n",
      "\n",
      "         [[ 1.29, -0.20,  ..., -0.61,  0.37],\n",
      "          [-0.10, -0.96,  ...,  1.59, -0.05],\n",
      "          ...,\n",
      "          [ 1.07,  1.76,  ..., -1.27, -0.85],\n",
      "          [-0.02, -1.26,  ...,  1.63,  2.56]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.19, -1.04,  ..., -0.28, -1.15],\n",
      "          [-0.14, -0.14,  ...,  0.75,  0.82],\n",
      "          ...,\n",
      "          [ 0.87,  0.45,  ...,  0.32,  1.09],\n",
      "          [-1.82,  0.04,  ...,  1.78, -1.49]],\n",
      "\n",
      "         [[ 1.10,  0.82,  ...,  0.35, -0.19],\n",
      "          [-0.86, -0.37,  ..., -1.36, -0.20],\n",
      "          ...,\n",
      "          [-0.41,  1.42,  ...,  0.43, -0.28],\n",
      "          [-0.46, -1.20,  ...,  0.09,  2.22]]]], device='cuda:0')\n",
      "torch:\n",
      "x grad: tensor([[[    -0.41,      0.00,  ...,      1.43,     -2.48],\n",
      "         [    -1.69,     -0.78,  ...,      0.77,     -0.34],\n",
      "         ...,\n",
      "         [     0.13,     -0.44,  ...,      0.16,     -0.34],\n",
      "         [     0.84,     -0.40,  ...,      0.00,     -0.12]],\n",
      "\n",
      "        [[    -0.58,      1.96,  ...,      0.53,     -0.64],\n",
      "         [     0.80,      0.24,  ...,      0.66,      1.54],\n",
      "         ...,\n",
      "         [     0.91,      0.50,  ...,      0.02,     -0.60],\n",
      "         [     0.38,      0.08,  ...,     -0.16,      0.26]]], device='cuda:0')\n",
      "g grad: tensor([[[    16.14,     19.23,  ...,    -17.37,     38.00],\n",
      "         [     5.13,      5.68,  ...,    -21.45,      8.50],\n",
      "         ...,\n",
      "         [     6.12,     -8.50,  ...,     -5.48,     -4.28],\n",
      "         [    -2.44,     -0.81,  ...,      0.72,     -3.37]],\n",
      "\n",
      "        [[     1.19,      0.29,  ...,     25.77,    -17.01],\n",
      "         [    40.39,    -30.70,  ...,     15.47,     -5.42],\n",
      "         ...,\n",
      "         [    -4.31,     -5.67,  ...,      7.79,     -8.99],\n",
      "         [    -1.57,     -9.72,  ...,      5.73,     -0.02]]], device='cuda:0')\n",
      "naive:\n",
      "x grad: tensor([[[    -0.41,      0.00,  ...,      1.43,     -2.48],\n",
      "         [    -1.69,     -0.78,  ...,      0.77,     -0.34],\n",
      "         ...,\n",
      "         [     0.13,     -0.44,  ...,      0.16,     -0.34],\n",
      "         [     0.84,     -0.40,  ...,      0.00,     -0.12]],\n",
      "\n",
      "        [[    -0.58,      1.96,  ...,      0.53,     -0.64],\n",
      "         [     0.80,      0.24,  ...,      0.66,      1.54],\n",
      "         ...,\n",
      "         [     0.91,      0.50,  ...,      0.02,     -0.60],\n",
      "         [     0.38,      0.08,  ...,     -0.16,      0.26]]], device='cuda:0')\n",
      "g grad: tensor([[[    16.14,     19.23,  ...,    -17.37,     38.00],\n",
      "         [     5.13,      5.68,  ...,    -21.45,      8.50],\n",
      "         ...,\n",
      "         [     6.12,     -8.50,  ...,     -5.48,     -4.28],\n",
      "         [    -2.44,     -0.81,  ...,      0.72,     -3.37]],\n",
      "\n",
      "        [[     1.19,      0.29,  ...,     25.77,    -17.01],\n",
      "         [    40.39,    -30.70,  ...,     15.47,     -5.42],\n",
      "         ...,\n",
      "         [    -4.31,     -5.67,  ...,      7.79,     -8.99],\n",
      "         [    -1.57,     -9.72,  ...,      5.73,     -0.02]]], device='cuda:0')\n",
      "gi grad: None\n",
      "triton:\n",
      "x grad: tensor([[[    -0.41,      0.00,  ...,      1.43,     -2.48],\n",
      "         [    -1.69,     -0.78,  ...,      0.77,     -0.34],\n",
      "         ...,\n",
      "         [     0.13,     -0.44,  ...,      0.16,     -0.34],\n",
      "         [     0.84,     -0.40,  ...,      0.00,     -0.12]],\n",
      "\n",
      "        [[    -0.58,      1.96,  ...,      0.53,     -0.64],\n",
      "         [     0.80,      0.24,  ...,      0.66,      1.54],\n",
      "         ...,\n",
      "         [     0.91,      0.50,  ...,      0.02,     -0.60],\n",
      "         [     0.38,      0.08,  ...,     -0.16,      0.26]]], device='cuda:0')\n",
      "g grad: tensor([[[    16.14,     19.23,  ...,    -17.37,     38.00],\n",
      "         [     5.13,      5.68,  ...,    -21.45,      8.50],\n",
      "         ...,\n",
      "         [     6.12,     -8.50,  ...,     -5.48,     -4.28],\n",
      "         [    -2.44,     -0.81,  ...,      0.72,     -3.37]],\n",
      "\n",
      "        [[     1.19,      0.29,  ...,     25.77,    -17.01],\n",
      "         [    40.39,    -30.70,  ...,     15.47,     -5.42],\n",
      "         ...,\n",
      "         [    -4.31,     -5.67,  ...,      7.79,     -8.99],\n",
      "         [    -1.57,     -9.72,  ...,      5.73,     -0.02]]], device='cuda:0')\n",
      "max diff x grad: tensor(    0.00, device='cuda:0')\n",
      "max diff g grad: tensor(    0.00, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# try the backward pass, compare with torch autograd\n",
    "# test cumulative_gating_2d_torch with always the same random inputs\n",
    "grad_output = torch.randn(B, T, S, C, device=device)\n",
    "print(\"grad_output:\", grad_output)\n",
    "print(\"torch:\")\n",
    "x.requires_grad = True\n",
    "g.requires_grad = True\n",
    "gi.requires_grad = True\n",
    "out = cum_gating_2d_torch(x, g)\n",
    "out.backward(grad_output)\n",
    "print('x grad:', x.grad)\n",
    "print('g grad:', g.grad)\n",
    "x.grad = None\n",
    "g.grad = None\n",
    "gi.grad = None\n",
    "print(\"naive:\")\n",
    "x.requires_grad = True\n",
    "g.requires_grad = True\n",
    "gi.requires_grad = True\n",
    "out = cum_gating_2d_naive(x, g)\n",
    "out.backward(grad_output)\n",
    "naive_x_grad = x.grad.clone()\n",
    "naive_g_grad = g.grad.clone()\n",
    "naive_gi_grad = gi.grad.clone() if gi.grad is not None else None\n",
    "print('x grad:', x.grad)\n",
    "print('g grad:', g.grad)\n",
    "print('gi grad:', gi.grad)\n",
    "# reset gradients\n",
    "x.grad = None\n",
    "g.grad = None\n",
    "gi.grad = None\n",
    "print(\"triton:\")\n",
    "x.requires_grad = True\n",
    "g.requires_grad = True\n",
    "out = cum_gating_2d_triton(x, g)\n",
    "out.backward(grad_output)\n",
    "print('x grad:', x.grad)\n",
    "print('g grad:', g.grad)\n",
    "print(\"max diff x grad:\", (x.grad - naive_x_grad).abs().max())\n",
    "print(\"max diff g grad:\", (g.grad - naive_g_grad).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch approximation (cheating) forward:\n",
      "2.31 ms Â± 5.3 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n",
      "torch approximation (cheating) forward & backward:\n",
      "13.9 ms Â± 76.7 Î¼s per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "triton forward:\n",
      "4.23 ms Â± 6.02 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n",
      "triton forward & backward:\n",
      "13.2 ms Â± 53.7 Î¼s per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# measure the speedup\n",
    "B, T, S, C = 128, 128, 32, 64\n",
    "x = torch.randn(B, T, C, device=device)\n",
    "g = F.softmax(torch.randn(B, T, S, device=device), dim=-1)\n",
    "grad_output = torch.randn(B, T, S, C, device=device)\n",
    "# warmup all torch.compile functions first\n",
    "x.requires_grad = True\n",
    "g.requires_grad = True\n",
    "out = cum_gating_2d_torch(x, g)\n",
    "out.backward(grad_output)\n",
    "x.grad = None\n",
    "g.grad = None\n",
    "out = cum_gating_2d_triton(x, g)\n",
    "out.backward(grad_output)\n",
    "x.grad = None\n",
    "g.grad = None\n",
    "\n",
    "print(\"torch approximation (cheating) forward:\")\n",
    "x.requires_grad = True\n",
    "g.requires_grad = True\n",
    "%timeit cum_gating_2d_torch(x, g)\n",
    "print(\"torch approximation (cheating) forward & backward:\")\n",
    "def ftorch():\n",
    "    x.grad = None\n",
    "    g.grad = None\n",
    "    out_torch = cum_gating_2d_torch(x, g)\n",
    "    out_torch.backward(grad_output)\n",
    "%timeit ftorch()\n",
    "# print(\"\\ntorch naive forward:\")\n",
    "# x.requires_grad = True\n",
    "# g.requires_grad = True\n",
    "# %timeit cum_gating_2d_naive(x, g)\n",
    "# print(\"torch naive forward & backward:\")\n",
    "# def fnaive():\n",
    "#     x.grad = None\n",
    "#     g.grad = None\n",
    "#     out_naive = cum_gating_2d_naive(x, g)\n",
    "#     out_naive.backward(grad_output)\n",
    "# %timeit fnaive()\n",
    "# print(\"\\ntorch multistage forward:\")\n",
    "# x.requires_grad = True\n",
    "# g.requires_grad = True\n",
    "# %timeit cum_gating_2d_torch_multistage(x, g)\n",
    "# print(\"torch multistage forward & backward:\")\n",
    "# def ftorch():\n",
    "#     x.grad = None\n",
    "#     g.grad = None\n",
    "#     out_torch = cum_gating_2d_torch_multistage(x, g)\n",
    "#     out_torch.backward(grad_output)\n",
    "# %timeit ftorch()\n",
    "print(\"\\ntriton forward:\")\n",
    "x.requires_grad = True\n",
    "g.requires_grad = True\n",
    "%timeit cum_gating_2d_triton(x, g)\n",
    "print(\"triton forward & backward:\")\n",
    "def ftriton():\n",
    "    x.grad = None\n",
    "    g.grad = None\n",
    "    out_triton = cum_gating_2d_triton(x, g)\n",
    "    out_triton.backward(grad_output)\n",
    "%timeit ftriton()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCANTransformerConfig:\n",
    "    def __init__(self, vocab_size, seq_len, window_len, embed_size, head_num, layer_num, state_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.window_len = window_len\n",
    "        self.embed_size = embed_size\n",
    "        self.head_num = head_num\n",
    "        self.layer_num = layer_num\n",
    "        self.state_size = state_size\n",
    "\n",
    "# no reverse cumprod in pytorch :(\n",
    "def reverse_cumprod(x, dim=-1):\n",
    "    # cp = torch.cumprod(x, dim=dim)\n",
    "    # i = [slice(None)] * x.dim()\n",
    "    # i[dim] = -1\n",
    "    # return (x / (cp + 1e-8)) * cp[i].unsqueeze(dim)\n",
    "    return torch.flip(torch.cumprod(torch.flip(x, [dim]), dim), [dim])\n",
    "\n",
    "# this is for stator and integrator in SCAN\n",
    "def cum_gating_2d_torch(x, g):\n",
    "    # x is (B, T, C) and g is (B, T, S), return (B, T, S, C)\n",
    "    # the parallel cumulative version of the recursion o[i] = o[i-1](1-g[i])^T + x[i]g[i]^T\n",
    "    inv_g = 1 - g\n",
    "    r_inv_g = reverse_cumprod(inv_g, dim=1)\n",
    "    r_inv_g = torch.cat((r_inv_g[:, 1:], torch.ones_like(r_inv_g[:, -1:])), dim=1)\n",
    "    g_r_inv_g = g * r_inv_g\n",
    "    x_g_r_inv_g = torch.einsum('bts,btc->btsc', g_r_inv_g, x)\n",
    "    c_x_g_r_inv_g = torch.cumsum(x_g_r_inv_g, dim=1)\n",
    "    return c_x_g_r_inv_g / (r_inv_g.unsqueeze(-1) + 1e-8) # avoid division by zero\n",
    "\n",
    "# for debugging, now a naive version\n",
    "def cum_gating_2d_naive(x, g):\n",
    "    # x is (B, T, C) and g is (B, T, S), return (B, T, S, C)\n",
    "    # the naive iterative version of the recursion o[i] = o[i-1](1-g[i])^T + x[i]g[i]^T\n",
    "    B, T, C = x.shape\n",
    "    S = g.shape[-1]\n",
    "    x_g = torch.einsum('bts,btc->btsc', g, x)\n",
    "    \n",
    "    out_list = [x_g[:, 0].unsqueeze(1)]  # Start with the first time step\n",
    "    \n",
    "    for i in range(1, T):\n",
    "        prev_out = out_list[-1][:, -1]  # Get the last time step from previous output\n",
    "        complement_g = 1 - g[:, i]\n",
    "        current_out = torch.einsum('bsc,bs->bsc', prev_out, complement_g) + x_g[:, i]\n",
    "        out_list.append(current_out.unsqueeze(1))\n",
    "    \n",
    "    return torch.cat(out_list, dim=1)\n",
    "\n",
    "# sliding window fold\n",
    "def sliding_window_fold(x, window_len):\n",
    "    # x is a (B, T, C) tensor\n",
    "    # output should be a (B, T, window_len, C) tensor that slides over the T dimension\n",
    "    # first window_len-1 elements will have to be padded with zeros in the beginning\n",
    "    # example: x = torch.tensor([[[1,2],[3,4],[5,6],[7,8],[9,10]]])\n",
    "    # sliding_window_fold(x, 2) -> torch.tensor([[[[0,0],[1,2]],[[1,2],[3,4]],[[3,4],[5,6]],[[5,6],[7,8]],[[7,8],[9,10]]]])\n",
    "    padded_x = F.pad(x, (0, 0, window_len - 1, 0), mode='constant', value=0)\n",
    "    output = padded_x.unfold(dimension=1, size=window_len, step=1).permute(0, 1, 3, 2)\n",
    "    return output\n",
    "\n",
    "def attend_folded_all_keys_torch(q, k, states, W):\n",
    "    k = sliding_window_fold(k, W) # (B, T, W, C)\n",
    "    all_keys = torch.cat((states, k), dim=2) # (B, T, S, C) + (B, T, W, C) -> (B, T, S+W, C)\n",
    "    scores = torch.einsum(\"btc, btxc -> btx\", q, all_keys) # (B, T, S+W)\n",
    "    return scores\n",
    "\n",
    "def accumulate_folded_all_values_torch(s, v, states, W):\n",
    "    v = sliding_window_fold(v, W) # (B, T, W, C)\n",
    "    all_values = torch.cat((states, v), dim=2) # (B, T, S, C) + (B, T, W, C) -> (B, T, S+W, C)\n",
    "    out = torch.einsum(\"btx, btxc -> btc\", s, all_values) # (B, T, C)\n",
    "    return out\n",
    "\n",
    "# repeat last element in dim n_repeat times\n",
    "def repeat_last(x, n_repeat, dim):\n",
    "    last = x.select(dim, -1).unsqueeze(dim)\n",
    "    last_repeated = last.expand(*x.shape[:dim], n_repeat, *x.shape[dim+1:])\n",
    "    return torch.cat([x, last_repeated], dim=dim)\n",
    "\n",
    "def scores_mask(T, W, S):\n",
    "    # create lower right triangle mask (W, W)\n",
    "    mask = torch.tril(torch.ones(W, W)).flip(1)\n",
    "    # concat ones with size (T-W, W) in 0th dim\n",
    "    mask = torch.cat((mask, torch.ones(T-W, W)), dim=0)\n",
    "    # concat ones with size (T, S) in 1st dim\n",
    "    mask = torch.cat((torch.ones(T, S), mask), dim=1)\n",
    "    return mask\n",
    "\n",
    "def build_alibi_tensor_SCAN(head_num, seq_len, window_len, state_size):\n",
    "    slopes = torch.tensor([2 ** (-8.0 * i / head_num) for i in range(head_num)])\n",
    "    alibi = torch.zeros((head_num, seq_len, window_len))\n",
    "    for i in range(seq_len):\n",
    "        for j in range(window_len):\n",
    "            if i < window_len:\n",
    "                alibi[:, i, j] = slopes * (j - window_len + 1) if i > (window_len - j - 2) else 0\n",
    "            else:\n",
    "                alibi[:, i, j] = alibi[:, window_len-1, j]\n",
    "    # Now concat a zeros tensor of size (head_num, seq_len, state_size) to the left of the above square tensor\n",
    "    alibi = torch.cat((torch.zeros(head_num, seq_len, state_size), alibi), dim=2)\n",
    "    return alibi.to(device)\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "    \n",
    "class SCAttentionHead(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seq_len = config.seq_len\n",
    "        self.window_len = config.window_len\n",
    "        self.state_size = config.state_size\n",
    "        self.head_size = config.embed_size // config.head_num\n",
    "        self.stator = nn.Linear(config.embed_size, self.head_size, bias=False)\n",
    "        self.integrator = nn.Linear(config.embed_size, self.state_size, bias=False)\n",
    "        self.key = nn.Linear(config.embed_size, self.head_size, bias=False)\n",
    "        self.query = nn.Linear(config.embed_size, self.head_size, bias=False)\n",
    "        self.value = nn.Linear(config.embed_size, self.head_size, bias=False)\n",
    "        self.register_buffer('mask', scores_mask(config.seq_len, config.window_len, config.state_size))\n",
    "\n",
    "    def forward(self, x, alibi, kv_cache=None, state=None):\n",
    "        B, T, E = x.shape\n",
    "        train = T == self.seq_len\n",
    "        W = self.window_len\n",
    "        S = self.state_size\n",
    "        C = self.head_size\n",
    "        _, T_past, _ = kv_cache[0].shape if kv_cache is not None and kv_cache[0] is not None else (0, 0, 0)\n",
    "\n",
    "        stator = self.stator(x) # (B, T, C)\n",
    "        integrator = self.integrator(x) # (B, T, S)\n",
    "        integrator = F.softmax(integrator, dim=-1) # (B, T, S)\n",
    "        # integrator = F.sigmoid(integrator) # (B, T, S)\n",
    "        if T > 1:\n",
    "            # assuming there is never an inference step with a given state AND prompt for now...\n",
    "            states = cum_gating_2d_triton(stator, integrator) # (B, T, S, C)\\\n",
    "            state = states[:, -1] if state is not None else None\n",
    "        else:\n",
    "            # update rule: o[i] = o[i-1](1-g[i])^T + x[i]g[i]^T\n",
    "            states = torch.einsum(\"bts, btc -> bsc\", integrator, stator) # (B, S, C)\n",
    "            prev_state = torch.einsum(\"bts, bsc -> bsc\", 1 - integrator, state) if state is not None else 0 # (B, S, C)\n",
    "            state = prev_state + states # (B, S, C)\n",
    "\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        k = self.key(x)   # (B, T, C)\n",
    "        v = self.value(x) # (B, T, C)\n",
    "\n",
    "        if kv_cache is not None: # never in training mode\n",
    "            k_past, v_past = kv_cache\n",
    "            if k_past is not None:\n",
    "                k = torch.cat((k_past, k), dim=1)\n",
    "                v = torch.cat((v_past, v), dim=1)\n",
    "            if k.shape[1] > self.window_len - 1:\n",
    "                k = k[:, -self.window_len:]\n",
    "                v = v[:, -self.window_len:]\n",
    "            kv_cache = (k, v) \n",
    "        T_k = k.shape[1]\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        if T > 1:\n",
    "            scores = attend_folded_all_keys_triton(q, k, states, W) * C ** -0.5 # (B, T, S+W)\n",
    "            scores += alibi[:T_k]\n",
    "            scores = scores.masked_fill(self.mask[:T] == 0, float('-inf')) \n",
    "            scores = F.softmax(scores, dim=-1) # (B, T, S+W)\n",
    "            out = accumulate_folded_all_values_triton(scores, v, states, W) # (B, T, C)\n",
    "        else:\n",
    "            # q is (B, 1, C), k/v are (B, T_past+1, C), state is (B, S, C) \n",
    "            all_keys = torch.cat((state, k), dim=1) # (B, S+T_past+1, C)\n",
    "            all_values = torch.cat((state, v), dim=1) # (B, S+T_past+1, C)\n",
    "            scores = torch.einsum(\"boc, bwc -> bw\", q, all_keys) * C ** -0.5 # (B, S+T_past+1)\n",
    "            scores += alibi[-1] # (B, S+T_past+1) TODO: indexing for T_k < window_len\n",
    "            scores = F.softmax(scores, dim=-1)\n",
    "            out = torch.einsum(\"bw, bwc -> bc\", scores, all_values).reshape(B, 1, C)\n",
    "        \n",
    "        return out, state, kv_cache\n",
    "\n",
    "class MultiHeadSCAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SCAttentionHead(config) for _ in range(config.head_num)])\n",
    "        self.o = nn.Linear(config.head_num*config.embed_size//config.head_num, config.embed_size)\n",
    "\n",
    "    def forward(self, x, alibi, kv_cache=None, state=None):\n",
    "        head_outs = [h(x, alibi[i], None if kv_cache is None else kv_cache[i], None if state is None else state[:, i]) for i, h in enumerate(self.heads)]\n",
    "        kv_cache = [h[2] for h in head_outs]\n",
    "        state = torch.stack([h[1] for h in head_outs], dim=1) if state is not None else None\n",
    "        out = torch.cat([h[0] for h in head_outs], dim=-1) # concat single-head results\n",
    "        out = self.o(out)\n",
    "        return out, state, kv_cache\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(config.embed_size, config.embed_size*4)\n",
    "        self.lin_2 = nn.Linear(config.embed_size*4, config.embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin_1(x) # (B, T, C*multiplier)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin_2(x) # (B, T, C)\n",
    "        return x\n",
    "\n",
    "class SCANBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.sa_heads = MultiHeadSCAttention(config)\n",
    "        self.ff_layer = FeedForward(config)\n",
    "        self.sa_norm = RMSNorm(config.embed_size)\n",
    "        self.ff_norm = RMSNorm(config.embed_size)\n",
    "    \n",
    "    def forward(self, x, alibi, kv_cache=None, state=None):\n",
    "        a, state, kv_cache = self.sa_heads(x, alibi, kv_cache, state)\n",
    "        h = x + self.sa_norm(a)\n",
    "        o = h + self.ff_norm(self.ff_layer(h))\n",
    "        return o, state, kv_cache\n",
    "    \n",
    "class SCANTransformerLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_num = config.layer_num\n",
    "        self.head_num = config.head_num\n",
    "        self.seq_len = config.seq_len\n",
    "        self.window_len = config.window_len\n",
    "        self.head_size = config.embed_size // config.head_num\n",
    "        self.state_size = config.state_size\n",
    "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.embed_size)\n",
    "        # Language Modelling (?) Head is a standard linear layer to go from \n",
    "        # embeddings back to logits of vocab_size\n",
    "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size)\n",
    "        # transformer blocks\n",
    "        self.blocks = nn.ModuleList([SCANBlock(config) for _ in range(config.layer_num)])\n",
    "        # precompute AliBi tensor\n",
    "        self.alibi = build_alibi_tensor_SCAN(config.head_num, config.seq_len, config.window_len, config.state_size) \n",
    "\n",
    "    def forward(self, idx, targets=None, kv_cache=None, states=None):\n",
    "        train = targets is not None\n",
    "        B, T = idx.shape\n",
    "        if train: \n",
    "            assert T == self.seq_len\n",
    "            assert T % self.window_len == 0\n",
    "            x = self.token_embedding_table(idx)\n",
    "            # go through blocks\n",
    "            alibi = self.alibi\n",
    "            for i, block in enumerate(self.blocks):\n",
    "                x, _, _ = block(x, alibi, None, None)\n",
    "            logits = self.lm_head(x)\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            _, T_past, _ = kv_cache[0][0][0].shape if kv_cache is not None and kv_cache[0][0][0] is not None else (0, 0, 0)\n",
    "            x = self.token_embedding_table(idx)\n",
    "            _, T_now, _ = x.shape\n",
    "            # go through blocks\n",
    "            alibi = self.alibi\n",
    "            for i, block in enumerate(self.blocks):\n",
    "                x, state, cache = block(x, alibi, None if kv_cache is None else kv_cache[i], states[:, i])\n",
    "                if kv_cache is not None:\n",
    "                    kv_cache[i] = cache\n",
    "                states[:, i] = state\n",
    "            logits = self.lm_head(x) # this will only be the last window_len tokens\n",
    "            loss = None\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1, use_cache=True):\n",
    "        if use_cache:\n",
    "            # initialize key-value cache\n",
    "            kv_cache = [[(None, None) for _ in range(self.head_num)] for _ in range(self.layer_num)]\n",
    "            # initialize states\n",
    "            states = torch.zeros(1, self.layer_num, self.head_num, self.state_size, self.head_size).to(idx.device)\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            B, T = idx.shape\n",
    "            # now if T > window_len, we need to do one window_len forward pass first, then pass the rest of the tokens one by one to prefill\n",
    "            # I'll come up with a better way to do this later\n",
    "            if T > self.window_len:\n",
    "                idx_context = idx[:, :self.window_len] # just the first pass\n",
    "                n_passes = T-self.window_len\n",
    "                for i in range(n_passes):\n",
    "                    _, _ = self(idx_context, kv_cache=kv_cache, states=states)\n",
    "                    idx_context = idx[:, self.window_len+i:self.window_len+i+1]\n",
    "            else:\n",
    "                idx_context = idx\n",
    "\n",
    "            for _ in range(max_new_tokens):\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context, kv_cache=kv_cache, states=states)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "                # since we have kv cache, only need to pass new token\n",
    "                idx_context = idx_next\n",
    "            return idx\n",
    "        else:\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            idx_context = idx[:, -self.seq_len:]\n",
    "            for _ in range(max_new_tokens):\n",
    "                # get the predictions\n",
    "                logits, _ = self(idx_context)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "                # crop idx to the last window_len tokens\n",
    "                idx_context = idx[:, -self.window_len:]\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "688983"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test forward pass\n",
    "config = SCANTransformerConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    seq_len=seq_len,\n",
    "    window_len=64,\n",
    "    embed_size=128,\n",
    "    head_num=2,\n",
    "    layer_num=3,\n",
    "    state_size=32\n",
    ")\n",
    "# config = SCANTransformerConfig(\n",
    "#     vocab_size=vocab_size,\n",
    "#     seq_len=seq_len,\n",
    "#     window_len=128,\n",
    "#     embed_size=256,\n",
    "#     head_num=2,\n",
    "#     layer_num=6,\n",
    "#     state_size=32\n",
    "# )tate_size=32\n",
    "m = SCANTransformerLM(config)\n",
    "m.to(device)\n",
    "xb, yb = get_batch('train', seq_len, 1)\n",
    "logits, loss = m(xb, yb)\n",
    "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664377d65c104925b939c3226e3ad93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss: 1.3442387580871582 final val loss: 1.3860230565071106\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training!\n",
    "model = SCANTransformerLM(config)\n",
    "# model = torch.compile(model)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "train(model, optimizer, seq_len, batch_size, total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), 'models/TritonSCANTransformerLM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khalid Zuhri\\AppData\\Local\\Temp\\ipykernel_18548\\994828852.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('models/TritonSCANTransformerLM.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SCANTransformerLM(\n",
       "  (token_embedding_table): Embedding(87, 128)\n",
       "  (lm_head): Linear(in_features=128, out_features=87, bias=True)\n",
       "  (blocks): ModuleList(\n",
       "    (0-2): 3 x SCANBlock(\n",
       "      (sa_heads): MultiHeadSCAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-1): 2 x SCAttentionHead(\n",
       "            (stator): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (integrator): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (key): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=64, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff_layer): FeedForward(\n",
       "        (lin_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (lin_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (sa_norm): RMSNorm()\n",
       "      (ff_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = SCANTransformerLM(config)\n",
    "# model = torch.compile(model)\n",
    "model.load_state_dict(torch.load('models/TritonSCANTransformerLM.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75,  1, 59, 62,  1, 58, 76,  1, 64, 72, 72, 61,  1, 58, 76,  1, 70, 62, 10,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0, 37,  1, 65, 58, 79, 62,  1, 77, 75, 58, 66, 71, 62, 61,  1, 63, 72, 75,  1, 77, 65, 72, 78, 76, 58, 71]])\n",
      "You will never be as good as me.\n",
      "             \n",
      "I have trained for thousands, so I can't see it to sure again.\n",
      "\n",
      "I'm on the probably sure that you want to play a real to hear the top the way to be soul about the raining.\n",
      "\n",
      "She was confident a lot of the lick of the power of the power of the truth.\n",
      "\n",
      "Hey, what is he said the first time to the manga does.\n",
      "\n",
      "I see, I can't have the one who well with my mission attacks.\n",
      "\n",
      "Can't you tell me your problem?\n",
      "\n",
      "It says that means that battle was hopeless anything straight.\n",
      "\n",
      "Now that I was the same as a past as your power?\n",
      "\n",
      "And that I didn't have a day this one who are happened.\n",
      "\n",
      "And then why did you think you're all right now.\n",
      "\n",
      "And that the more is the country to continue soul me any cheater the train of them.\n",
      "\n",
      "That's why I have to go stuck for you to speak the bunch the battle profession.\n",
      "\n",
      "It's the strain they all the country of his me.\n",
      "\n",
      "So you must be best the same things.\n",
      "\n",
      "I was a counting out the only one who would never left to stay her aside the truth as a medicifical from the season as well.\n",
      "\n",
      "If you want to live in t\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "idx = encode(\"\"\"You will never be as good as me.\n",
    "             \n",
    "I have trained for thousan\"\"\")\n",
    "print(torch.tensor([idx]))\n",
    "print(decode(model.generate(idx=torch.tensor([idx], dtype=torch.long).to(device), max_new_tokens=1000, temperature=0.5, use_cache=True)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerConfig:\n",
    "    def __init__(self, vocab_size, seq_len, embed_size, head_num, layer_num):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = embed_size\n",
    "        self.head_num = head_num\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "def build_alibi_tensor(head_num: int, seq_len: int) -> torch.Tensor:\n",
    "    # Create position indices\n",
    "    pos = torch.arange(seq_len, device=device)\n",
    "    # Create slopes for each head: 2^(-8i/h) for i in [0,h)\n",
    "    slopes = torch.pow(2, -8.0 * torch.arange(head_num, device=device) / head_num)\n",
    "    # Compute relative positions using broadcasting\n",
    "    # This creates a seq_len x seq_len matrix of position differences\n",
    "    relative_pos = pos[None, :] - pos[:, None]  # shape: (seq_len, seq_len)\n",
    "    # Multiply slopes by relative positions using broadcasting\n",
    "    # slopes[:, None, None] creates a (head_num, 1, 1) tensor\n",
    "    # relative_pos[None, :, :] creates a (1, seq_len, seq_len) tensor\n",
    "    alibi = slopes[:, None, None] * relative_pos[None, :, :]\n",
    "    return alibi\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "class SelfAttentionHead(nn.Module):\n",
    "    \"\"\" one head of self-attention with AliBi \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seq_len = config.seq_len\n",
    "        self.key = nn.Linear(config.embed_size, config.embed_size//config.head_num, bias=False)\n",
    "        self.query = nn.Linear(config.embed_size, config.embed_size//config.head_num, bias=False)\n",
    "        self.value = nn.Linear(config.embed_size, config.embed_size//config.head_num, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(config.seq_len, config.seq_len)))\n",
    "\n",
    "    def forward(self, x, alibi, kv_cache=None):\n",
    "        B, T, C = x.shape\n",
    "        _, T_past, _ = kv_cache[0].shape if kv_cache is not None and kv_cache[0] is not None else (0, 0, 0)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            k_past, v_past = kv_cache\n",
    "            if k_past is not None:\n",
    "                k = torch.cat((k_past, k), dim=1)\n",
    "                v = torch.cat((v_past, v), dim=1)\n",
    "            if k.shape[1] > self.seq_len:\n",
    "                k = k[:, -self.seq_len:]\n",
    "                v = v[:, -self.seq_len:]\n",
    "                alibi = alibi[-self.seq_len:, -self.seq_len:]\n",
    "            kv_cache = (k, v)\n",
    "\n",
    "        T_k = k.shape[1]\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei * C**-0.5 # scaled attention\n",
    "        wei = wei + alibi[-T:, -T_k:] # add AliBi\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "\n",
    "        # perform the weighted aggregation of the values\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out, kv_cache\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(config.embed_size, config.embed_size*4)\n",
    "        self.lin_2 = nn.Linear(config.embed_size*4, config.embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin_2(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention with AliBi in parallel \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(config) for _ in range(config.head_num)])\n",
    "        self.o = nn.Linear(config.embed_size, config.embed_size)\n",
    "\n",
    "    def forward(self, x, alibi, kv_cache=None):\n",
    "        head_outs = [h(x, alibi[i], None if kv_cache is None else kv_cache[i]) for i, h in enumerate(self.heads)]\n",
    "        kv_cache = [h[1] for h in head_outs]\n",
    "        out = torch.cat([h[0] for h in head_outs], dim=-1) # concat single-head results\n",
    "        out = self.o(out)\n",
    "        return out, kv_cache\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.sa_heads = MultiHeadAttention(config)\n",
    "        self.ff_layer = FeedForward(config)\n",
    "        self.sa_norm = RMSNorm(config.embed_size)\n",
    "        self.ff_norm = RMSNorm(config.embed_size)\n",
    "    \n",
    "    def forward(self, x, alibi, kv_cache=None):\n",
    "        a, kv_cache = self.sa_heads(self.sa_norm(x), alibi, kv_cache)\n",
    "        h = x + a\n",
    "        o = h + self.ff_layer(self.ff_norm(h))\n",
    "        return o, kv_cache\n",
    "    \n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_num = config.layer_num\n",
    "        self.head_num = config.head_num\n",
    "        self.seq_len = config.seq_len\n",
    "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.embed_size)\n",
    "        # Language Modelling (?) Head is a standard linear layer to go from \n",
    "        # embeddings back to logits of vocab_size\n",
    "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size)\n",
    "        # transformer blocks\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.layer_num)])\n",
    "        # precompute AliBi tensor\n",
    "        self.alibi = build_alibi_tensor(config.head_num, config.seq_len)\n",
    "\n",
    "    def forward(self, idx, targets=None, kv_cache=None):\n",
    "        B, T = idx.shape\n",
    "        _, T_past, _ = kv_cache[0][0][0].shape if kv_cache is not None and kv_cache[0][0][0] is not None else (0, 0, 0)\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        x = tok_embd\n",
    "        # go through blocks\n",
    "        alibi = self.alibi[:, :T_past+T, :T_past+T]\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, cache = block(x, alibi, None if kv_cache is None else kv_cache[i])\n",
    "            if kv_cache is not None:\n",
    "                kv_cache[i] = cache\n",
    "        # get logits with linear layer\n",
    "        logits = self.lm_head(x) # (B,T,C)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1, use_cache=True):\n",
    "        if use_cache:\n",
    "            # initialize key-value cache\n",
    "            kv_cache = [[(None, None) for _ in range(self.head_num)] for _ in range(self.layer_num)]\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            # crop idx to the last seq_len tokens\n",
    "            idx_context = idx[:, -self.seq_len:]\n",
    "            for _ in range(max_new_tokens):\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context, kv_cache=kv_cache)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "                # since we have kv cache, only need to pass new token\n",
    "                idx_context = idx_next\n",
    "            return idx\n",
    "        else:\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            for _ in range(max_new_tokens):\n",
    "                #crop idx to the last seq_len tokens\n",
    "                idx_context = idx[:, -self.seq_len:]\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_context)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply temperature\n",
    "                logits = logits / temperature if temperature > 0 else logits\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) if temperature > 0 else torch.argmax(probs, dim=-1, keepdim=True) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615255"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test forward pass\n",
    "config = TransformerConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    seq_len=seq_len,\n",
    "    embed_size=128,\n",
    "    head_num=2,\n",
    "    layer_num=3\n",
    ")\n",
    "m = TransformerLM(config)\n",
    "m.to(device)\n",
    "xb, yb = get_batch('train', 5, 1)\n",
    "logits, loss = m(xb, yb)\n",
    "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6230792884d44c59641f67434adf7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss: 1.2193772792816162 final val loss: 1.3547022700309754\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVAAAAD+CAYAAABsiV3zAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAexAAAHsQEGxWGGAAAum0lEQVR4nO3dd3hUVfrA8e/UkIQ0WiihKggSFJSoKGqElYgUUbMgRAR+FAEpS2yIyqIgCqsoRCCuNAmgshgwiO4IK6jgIhYWmNCkE0JLCJOQRsr9/XFIzwAZErgT38/z8CRzbjsnM7zz3nPOvdegaZqGEEKICjPe6AoIIYS7kgAqhBAukgAqhBAukgAqhBAuchpA09PT6dixI1999VVh2caNGxk0aBAREREkJiZelwoKIYReOQ2gM2bMoG/fviXKoqOjWbx4Ma+88goLFy6s8soJIYSemcsrXL9+PbfeeitZWVklyjVNw2g00rRpUxISEspsZ7PZsNlsfP/997Rt27ZCFTl3zsyBAybuuiu7Qtu5m+zsbDw8PG50NaqUtLF6kDaWdOHCBWJjY0uUlRtAN23aRHp6Ort378bT05NHH30Uo9GI0WgkPz+fY8eOERQUVGa7sLAwwsLCiIyMZNasWRVqyPbtMGfOWRYvrluh7dyN3W4nODj4RlejSkkbqwdpY0mRkZFlysoNoG+99RYAS5YsoU6dOgwaNIiYmBhGjBjBsGHDyMnJYcaMGddQ7bIMhkrdnRBCVLlyA2iBwYMHA9CzZ08AunTpQpcuXaqsMpomUVQI4T4uG0CFEMIZh8OBw+HA4ManjyaTiePHj5e7zGAwUKtWLby8vJxuX+kB9JdffqnsXQohdMjhcNC4cWO3DqCZmZl4enqWuywvL48TJ07QpEkTp9vrZiK9G78HQvwpGQwGtw6eV2Iyma7YvkoPoCEhIS5vK/eFEkIsWbKkxAU8APn5+WXWi46O5uDBg5fdV3h4eKXWrTTd9IFW4y8yIaotTYO8PNe3N5nK/t/fvHkzGRkZAKxatYpmzZrRrl07MjMz2b59O2lpacydO5dTp06RmZnJlClTSEtLw2w207p1a4YMGVLmOB999BE7d+4kNTWVDz74gCVLlnD06FG8vLx48803GTRoEEFBQdx333306dPnquuvmwAKkoEK4W7y8uDxx13ffvVqMJeKQp07d6ZOnTr07NmTVatWMXz4cBo1asSyZcuwWCycOHGC7du3l9imb9++3H333fTv37/cAGqz2YiNjeX777/n008/5ciRI4SEhBAaGkp2djbp6el0796dBx54oEL111UAFUK4F5NJBcFr2b40o7Fkz6Kfnx8AK1euJC4ujjfeeKMwQy3g7e0NqKslL8dgMKBpGrNnz+aXX37h2Wef5fPPPycmJoZvv/2WMWPGEB0dfdX1l1F4IYTLDIayGeS1uv3223nrrbfIzc0tUd6gQQNmzpzJtm3bePDBByu0z7/85S+MGzeOlJQU3n//fWbOnElSUhK1atXC4XAwc+ZMTCZThS9BR6tk27Zt0yZMmFDh7Xbu1LSBA89WdnV0Z9euXTe6ClVO2lg9XKmNx44du041qToZGRmXXV68jeXFNRmFF0IIF8k8UCGEcJFuAqgQQribSg+g1zKIJKfwQoirUXqCfFVPmHdGN9OY5BReCDdUBTPpR44cyVtvvUVAQAADBgxg1qxZzJ07l+TkZB555JHLTnR3NmHez8+P119/3eUJ885UegANCQnh008/rfB2huwsvLNTgDqVXSUhRFWpgpn0ffv2ZeXKlbRs2ZIuXbpgNpvJzs4mMDCQ5cuXXzbwOZsw371792uaMO+MbjJQzz2/033vF8B7N7oqQoirVQUz6UNDQ/nnP//Jzp07mT59OosWLaJ3797cfffdPPbYY1e129IT5ocMGcKKFStcnjDvjG4CqGb1wJSXc6OrIYSoiCqYSV/w3LXExEQCAgK49957iY6OZsuWLVit1stuW2UT5p3QTQDFYsGULwFUCEGJRwZ16tSJTp06lVi+atWqcl+PHj26RPnEiRNLvI6KiqrMaupnFF6zemDOv1jJtRFCiKqjm3mgmtmCWTJQIYQb0c2lnJrVA3OeZKBCuAuDwUDetUxh0rkLFy5gvkL/rr76QDXJQIVwF7Vq1eLEiRNu/ViPCxcuULNmzXKXmc1mAgMDL7u9bgKoZKBCuBcvL6/LPnDNHdjtdho3buzy9k5P4ffs2cPIkSMJDw9n/vz5heVTpkyhX79+jBw5ksTERJcPXEbBKLxczymEcBNOA2ibNm2Ijo5m5cqVbNmypbDcbDZjtVqxWCz4+/tXWkU0ixUDGpS6iaoQQujVZU/h4+LimD9/PgMHDiwsmzRpEkajkbi4OBYsWMC4ceMKl9lsNmw2G/Hx8djt9gpV5PAhC4F5GvHbt6Nd5kH27i4pKanCfxt3I22sHqSNV3bZANq7d2969+5Njx49GDBgAFD0vJJ69eqVOXBYWBhhYWFERkYSHBxcoYpYrXDO4knbli0hIKBC27oTu91e4b+Nu5E2Vg/SxitzGkA3bdpEbGws2dnZPProowwcOJCYmBimT5/O8ePHSUpKYs6cOS4fuDx5RjPkyEi8EMI9OA2goaGhhIaGFr5+7rnnAHUKXxUMBsgxekB2dpXsXwghKpturkQCyJUMVAjhRnRzLTxArtEDLspcUCGEe9BXBmqwSAAVQrgN3VwLbzBAjskqAVQI4TZ0lYHmSQYqhHAjugmgBgPkGiWACiHch24CKECOUU7hhRDuQzej8AYD5BokgAoh3IeuMlA5hRdCuBPdjMID5MopvBDCjegmA5VBJCGEu9FNAAXJQIUQ7kVfg0iSgQoh3IiuMtAcGYUXQrgRGUQSQggXSQYqhBAu0k0AlT5QIYS70U0AhUun8HJDZSGEm9DZKLxVHukhhHAbuspAcwwWyUCFEG5DN6PwBsOlQSTJQIUQbkJXGWiuUTJQIYT70FkAlQxUCOE+nAbQPXv2MHLkSMLDw5k/f35hud1uJyIigoiICOx2e6VVpHAQSTJQIYSbcBpA27RpQ3R0NCtXrmTLli2F5bNnz2bu3LnMmzePqKioSq1MDmaVgWpape5XCCGqgvlyC+Pi4pg/fz4DBw4sLHM4HPj7+wOQlpZWYn2bzYbNZiM+Pr7C2enp02Yu5NQkNT2VYzt2gPmyVXNbSUlJlZq565G0sXqQNl7ZZaNU79696d27Nz169GDAgAEA+Pn54XA4MBgM+Pj4lFg/LCyMsLAwIiMjCQ4OrlBFAgLAUuMcvjV8CW7VCry8KtgU92C32yv8t3E30sbqQdp4ZU4D6KZNm4iNjSU7O5tHH32UgQMHEhMTw/jx4xk7diwAL730kssHLs1ohHzNCFYZiRdCuAenATQ0NJTQ0NDC18899xwAwcHBLF26tNIrYjJBfj7g4SEj8UIIt6CbaUxG46UAapEMVAjhHnRzLbwKoAaVgcodmYQQbkBXGaimoTJQCaBCCDegm2vhC0/hJQMVQrgJ3WSgJhPk5RkkAxVCuA3dBNDCU3irPNZDCOEedDWIBKBZJIAKIdyDrjJQgHwJoEIIN6GbQSSTSf3UzBJAhRDuQTcZqMGgfsopvBDCXegqgBoMkC8ZqBDCTegmgMKluaASQIUQbkI3o/AARqMmp/BCCLehqwzUYIA8kwRQIYR70M0oPIDJJBmoEMJ96C4DXf4vK1mpEkCFEPqnqwBqNMLx01aST0oAFULon64CqMmkkWu0YsiVGyoLIfRPV6PwBc+GN1yUR3oIIfRPVxmo0Qg5koEKIdyE7kbhcw2SgQoh3IOuMlCLRfpAhRDuQ58BNEcyUCGE/jl9LvyaNWtYt24dqampDB06lG7dugEwePBgzGYzZrOZ2bNn4+HhUWmVsVg0siUDFUK4CacBtE+fPvTp04eUlBReeOGFwgDq6elJbm4u/v7+WCyWSq2MxaKRbrBIH6gQwi04DaAFpk2bxnPPPVf4eu7cuRiNRubMmcNXX31F7969C5fZbDZsNhvx8fHY7fYKVyYnx4uUDE8yMs9z0oXt3UFSUpJLfxt3Im2sHqSNV+Y0gGqaxsSJE+nevTt33HFHYbnx0rM36tWrx4ULF0psExYWRlhYGJGRkQQHB1e4Mn5+p0j2rUVNrQaN27Qpuk19NWK3213627gTaWP1IG28MqcBNCoqig0bNuBwODhw4ABbtmwhJiaG559/nszMTFJSUliwYIHLBy6PxaKRa7jULZCTUy0DqBCi+nAaQMeNG8e4ceMKX48cORKA9957r8oqk58PGAxFD5arUaPKjiWEENdKV9OYtm3zBuS5SEII96Cra+ELyGM9hBDuQFcZaAF5tLEQwh3o6lp4q1UDQDNbJIAKIXRPVxmop2c+APkWDwmgQgjd01UAbd8+A5AMVAjhHnQ1iNSwoboGXjJQIYQ70FUG2qSJugbe7CkZqBBC/3Q1iNS4cQ433QTUkAxUCKF/uspAAQ4ehK2/SgYqhNA/3QVQgNMpMg9UCKF/ugyguUYJoEII/dPVKHyBXKOVrFQJoEIIfdNdBnrzzSqArl8nAVQIoW+6GoUHePFFuGD2J2lvUiXVSAghqobuMlCrFY75tKVpmp0LadqNro4QQjiluwBqNMJZzyYYtTxWz0280dURQgindBdA/f0Bg4GjvsGkb911o6sjhBBO6W4U/tIz6zjq0w7rfgmgQgj90l0GCjB0KBzxaUez1F3k5kg/qBBCn3Q3Cg/Qo0dRP+iLEYloEkOFEDqkywzUYqGwH9T36C5OnrzRNRJCiLKcBtA1a9YwfPhw+vXrx7fffltYvnHjRgYNGkRERASJiVU7Sn7E5zaape4kL69KDyOEEC5xGkD79OnDxx9/THR0NJ9//nlheXR0NIsXL+aVV15h4cKFVVq5o77taJa2Cy1fzuGFEPpzxVP4adOm8dxzzxW+1jQNo9FI06ZNSUhIqLKK3XMPnK3RGIOWjyHxRJUdRwghXGV2tkDTNCZOnEj37t254447CsuNRiP5+fkcO3aMoKCgEtvYbDZsNhvx8fHY7fYKVyYpKalwuy5djKxfH8Q+j1b4rf0Ch0/XCu9Pj4q3sbqSNlYP0sYrcxpAo6Ki2LBhAw6HgwMHDrBlyxZiYmIYMWIEw4YNIycnhxkzZpTYJiwsjLCwMCIjIwkODq5wZex2e4ntfHzgl2YDafPJJO4YOZCaTWpVeJ96U7qN1ZG0sXqQNl6Z0wA6btw4xo0bV/h65MiRAHTp0oUuXbq4fMCKOu3dgp21u1DjhUXctfKF63ZcIYS4El1OYyowdar6ualRBOk/7YBdcmWSEEI/dHcpZ3HNm6uf2WZvNjQewvFJ8yE3t9L2L4QQ10LXGaifX9HvO2s/xLbdNZna6esbVyEhhChGl5dyFjds2KVfDAZsTYbzQOJnvDUxrVKPIYQQrtB1BgrQu3fR7ydrtuQP/474fv3ZjauQEEJcovsAajCUfL0x6BnaJ21g0F9OyE1GhBA3lO4DKEDPnkW/p1rr8HNgb/6SsJjTp29cnYQQQtej8AVGjCj5+qcGT9Iw/Q9+W7Sj0o8lhBBXyy0y0NKn8TmmGnwX9AzN1n+Mllt0qyZNQ07rhRDXje5H4Z3ZWbsLiUlW3rjXVlg2YAAsWHBdDi+EEO6RgQK88w5ERBQrMBiIrTOC0BPL0VLVtKYLFyA+vmiVn36CXr2ubz2FEH8ebhNA27aFfv1UllkgoWZrDvjfyeG3Py0sO3gQLl5Uvx85cn3rKIT4c3GbAAqqL7R//5Jl3wUN4vCyLewYNAuvHAcA27erZUa3ap0Qwt24xSh8aXXqFP2eZq1NdPBcfvrdg9G7RnFv4iosP2+GHTvIOC1XLAkhqo5b5milnySSZa7J182e4/OWr1E36xinl29AW7CQuh+8Cpp21SPzR46AzXbF1YQQAnDTUXijEb78smz5cZ9b+bJFJPPqTeGxwx8AGu2SN111UFyyBD78sBIrKoSo1twyAwUVREtPsC9OMxj5T9BgHjoRw0dRF1m58vrVTQjx5+C2ARSuPEXpoN8dpHg0oOOZr4mJgcPbz6sUc/Pm61I/IUT15tYBFErNDS3NYGBD0GA6J66k4+l1HOs5mq9W55ATFc3+mWuuVxWFENWU02ciuep6jMIX99RTavBny5byl5+s2ZKDfh0IObOOz1q+ToJXG1I7nqB21BTMKWcIGvww1to+4O2NUbMCJgAOHYJvvoFiT3QWQogSKj2A3gg9eqgAOmQILF5cdvmXLSLJx1h4Uf2nPzTC69Z36R07m9or3ibIN42Gfun0O6vRzWFEG9WIxR4z+N9Bn6IAmpmpHifi43P9GiaE0LVKD6AhISF8+umnV16xErVrB2vXQkJC+QE032AqU5Zh8eOzVpOLCjQNk18u5vyLdPthATfnf8j/bprIL78YOBCfTdsVr3FbvdNkjH+FcR+1lWvuhRDu3wdaXFCQCqSNGrmwscFAntFCttmbfzd9lsCMI7RP2sDUN/LJfecf7DhWC0aOJOu1aQTt/JrEE2py6e7dkJV15d0fOaLWFUJUH04D6KFDhxg6dCjh4eElyqdMmUK/fv0YOXIkiYmJVV5BV7z8ctHvo0dXfPscUw1iW7zAw8cX0ufQe9TMOU9sixegc2eOjnqHe06voeabL/JWyBreHn+KL6btgU8/hWnTYO9ep3UqXq8rKbie/6odPw7nzlVwIyHEtXAaQFu0aMHC0pf8AGazGavVisViwd/fvyrr5rLmzeG999QVS/fd59o+TtZsyZYGf6Vh+h981vJ1ck0eDBkCkxc25aO2UUy1P0GD9D94Nn4sLdd9QEL8ed5e3Zrc16eQt/ZrevXUSj6BWdOol3GEGnv3QkZGmeMdOgRnj2ZATAwRYUk8+WTJ5enpRZlufn6pjXNzYepUmDgRzp93rcFCiAqrcB/opEmTMBqNxMXFsWDBAsaNG1cV9bpmrVoV/f755zBvHnTrBq++evX7+KnBk/y3/uNoBvU9k5SkynNNHuytdS97a92r7uBsMMAOoCG8V6cjXadMJzxjBx+ENGHCCyZwOBi6dRuW/Iv4fwksXw5Nm/Lu9yE89uHDtLyvHu8M/YOIxH/wYFczTxzYxdLW03E4zBiNatzqmWfg5ptVFjtokOqqADh1Cmr/sgGLry+0bg1TpsD06eDlBVD42JPAwKJ2HToEdevKeJgQ16rCAdR46RZH9erVw263l1hms9mw2WzEx8eXWXY1kpKSXNruajzyiPrZpYsvX37pXyXHAPhmTy02eU2hU+a/sWam8t6bedRqaOL7+qM57nkzkX/dxU0Na6DtPozHV4cwjR/M2dsb8FT8eeLqD+Rb3ztpmzOLuw8uJDz8KXJyDMyZc4zk5Cakp2v88dkGQg46WLQwlNjVtchKzeUfqYuo+Xo/slu1ovbevZj+9jeSBg8m39eXsWObABAVdaywjmPGNKF9+wyGDUsqU/+MDANeXtd2W/+qfB/1QtpYPVxrG50G0OTkZF599VW2b9/O22+/ze7du4mJiWH69OkcP36cpKQk5syZU2KbsLAwwsLCiIyMJDg4uMKVsdvtLm1XEcHB8N136vcFC4o9d75S+fKL//8VvrJYIKcm+ABRUe3o1MmP7ds7QhvI6jiWv92zlaVHW5HsGQTxcKTNqwy3j+d09l0c8uvA8ePB1PLK4pHERQRv+C9nMv0xfubA5DGO0MyvyKzdislLniQ2FizvvANz5lDvvfegbl36n72VE96taGVqydyvmjJ8lBkfHwgI8CU4uH6ZmvfqBZ98ArVqlSz/z3+gS5eyj1cpz/V4H50peKxL8VsZPv44LFsG3t6Vd5wb2cbrRdp4ZU4DaO3atYmOji5TPmnSJJcPphcffQR2uzqtnTwZ3nyzao+Xk1P8d0Ph/UoBfvjVix9+7QKeRWWp1jrENR/PkwdncMazGakTPBiZdYKzNZuTMvVDPnnBTP/9b/CEdSZN0uLZ2fl1OKAGnlavM9Pj2Ui8x43j/K8HSP7FTovU/3Fg6Co6HUoi48c6DNwbSOqh2qR5eeET6MWBwyb8vS5iyc8mNKEm/NgU2jcBsxkuXGDLN6n8GnOITg/vwyvlBHTtCr17Q40aJdqpabBpk+oeuBZnzqgZC6GhFd/2n/+EDRvgX/8qKsvNVeNrlRlAxbX5/Xc13lr8BunuqFpMpK+ohg3VP4Dbb1f9i6tXQ5qObh/6R8BdLLVOp2ZOCpb8bC6aPDnk257PJhrADMtveZN+f0zjeM1bWXfgFkDdjT8mRv0DM/XqteZMw9YArAI8/NLxTz+DX+AZfC8msf9cJncGZrAuNpscowc5Rh+88lM5vmQDvj7HMBvyoWZNTtp88PFsSm6n+6F5bVizBuLioHt3MJkKLzI4l2bhtxgLT4UfUSmrwwHe3qT4NyfgjuZsPViXDg/64hHgBYmJqsIpKdC1K+lmPzw8VMxetgw2bnQtgBZMKzt8WA0mFtA0dUhNc22aW26Oxq/fpXLPA9YyXxwAv/yi+t39/Cq+b91LTibX2w+DxYyp7JTqCvnjD/U3Wr4c9u+XAFrG9b6U81pZrfDXv6rBmG+/Vaeww4bBY4/d6JrBae8WnHayLMdUg2W3TMWoFT2VtPQA2ZkzJV9nm705bW7OaS8VWTp2BnrAhn+VXG9DLtzZAv7+d8jLg08eV+UP3KSCkvfk22DPHpVuWixoNTzZfdiLgJo5mLWL6vy5RQtOZfrxz3dTCcw4TMR9m0n/+hyOpg7qBeSqPoKbbgIPD87M/RcLHeEEPNOLkeOsZdp68qTKWLp1g7SUXPYs+i/3Zf0H6tWDDh3gttuK0ktNo1nqLv4bFkvz7ilqZgINSEpS7YFLA3DZ2SotrVtXRe3Lycvj1Auz0D7/CULU1IrAunXViN4t6svrzTdV/caOVXVg2zbw9FSTkwMCrq7v40bIzVUzN4rfpby4c+dg7Fg+29uRE30n8PJEA5mZcOKEGtSsqMhI9QVWXc4G/pQZaHmGDoWePYuyli++UFnFO++oN3vcOLjnHn0E1kIGA/kG19/C6OjC//9l/PabOksv7pVX1LSw55+HGV+0YfjwNmX7kBtD1552gtoFM7wXcGn0P+wV+CAR7rxDY8qrOeqb65JvMg7RYsUiWi6Ig9rd8Ex7GHOeH/y4DTZtYtviZNLN/nxi8eWhgB0kJdZCmxvGkR3nabxqNeYZM6BmTahTh/BNOWSez+LnwMfg/ovw4osEpb3K3ye3plnaLu45tRptwD7yzl/A5FcTg5YPd92l3lxvbxVQLBZo00b9zMuDWbMwn0/i3TtW8EVcDcjOJmPhQlKff4P01nfSYNIQ4FKnsaax7slFPOz9k7rHQkKC2u+oUeoYwL59sG6dCiZOnTxJ9tbtvBsTyNPTWtP0VhVx0tJUAmyxoL4EMjJUgHZFbq6asWG3q3l/jRuXXK5p8MEHcN991N6yi3P/WQsTexMTo76ECmaCOPPHH+rM7qWXSpaXmYZXSTZvVt9Zd95ZNfsvT7W4lLMyeHmVPOWzWlWwaNBAfa7uvVeVf/QR/OMf8MYb6gMyZYoKQtOmqf7U7Gw1TchdTJhQsfX37YMnnlC///xz+etkZBhZtapk2cGD6udvvxv493dWwsLghx/g/vtVpv3jLVMJSt/H03Hf0nXvc7Q7C7mrmzNtcyhJjRvjnevAO+c89t49WPBDKz6PMVzqcnmKtV9chORkSE7mv6dz2JJ2G/kGE3NPQ8SAhvQf/gbnrYF45qWxNbAPv0aMYtq8WtzazMzf+p0k8MAW8tasZcPXFwlub8bbkEFWQhJ1n3wAg+M8Bsd5EkdM4eI7nvz2O9x5Zw0OtulK9FfP8MC3nzImcQztkoaTcDyU9I+WYfztF55p8x7p5/1ZsVxj0/u/0+ujD2HLFuYcf5zApHjqb/uNvd9n0rrvbaofqUYN1ceQkKC+uRMT+eZIe9qePYvl/w7D3U3YV6sT03+8n7u6+dMt72vqb/0SH3MmdOyovtlvvhnOnlXz7Vq0KHee2uOPqyy5Q7tcvKJm4KHlqKc1Tp0Ks2apL6JLfL7/XnWxvPYaK1efZui+l0je2IysrNuu/EHJzeXntefY/e1FGOqpIlteHv5Z6dRKzSLXuzEFN+65KpqmRn+zsqBz53L7SmbMUCcTq1df/W6vlUHTrvaBF1cvMjKSWbNmVXg7PY765eWpsy9nD6iz2dRntWVL9fqNN+DXX+Hhh2H9elX2xRfqNqQbN0JaWio+Pr7Xp/I3yNW00c9PdZGWxyMvA4+8DOatrMPTT5dcFh5OmeA8ebLKPgpmVxTXrx9sWnSI2tmJ7PW/h3yjmaFDSz4WZv58lSCC+r+5fz/kHz3OM42/41R8MnseHEViimfhnNq1ayE0tKiNIzrvxjLvA3KMNfDIy+CTNu+QalWnxGFh6jOy9rN0WLiQjTO3capOO+yeHblo9GRoyE5aX9xJ4vFcNu1ryMU6DRk063YMd3Sg15MqS386PIt+wfH8c9AWWqf8RK2aOWw3dGDfbX2ZtrCBOsBXX6nTbX9/lZEmJ6s7jt9/v/oAnzsHO3bw+iu55BtMtE75L7U9M3h062SVLcyZo7b5+99VsNq3j/3PTCT5hSg69W9Gr17QKuVneh6JIrNrL+KOtSf6Py15aaKRqW9qeGSkqNOWX3+F/fvJOpHM8fM1OXbSyr0dMklPziT5vIlEhzceXibScjz5ruHTzNh8Hxw7puq/bRv4+LDjRB0COzTE2vkuvO8OJuPIGQKWRannlgcGwv/+p7pt7r9fnT1c6g94KiyFJpn7mDnykLp2+uJFNcDRogUA55Pz2Pfheu5+yAseeACoWMwpL65JAK1kK1eqQZyVK4s+z97eKhDn5UG3bhJAr9YLL8C7717bPm67DXbuvOaqlDB1KvztbyXbaMnL4q7Ta7HXfhCHR70y2yxbps5MJk8us4j27VVMKDB4MDz5ZMkbhhcEfWN+LvVrnCfxYh0CAym8qY2Wm8ffX8vjlnZWfvwRLHt2MqfVhxjqB6ouB/shzgYG891WL4zkkWH25fubhvLZmksDYjk5MGmSymDPnwdfX1743+Mcbvk4X3xRVJcWju20S/uJJsnb+cudKWzdCjc1uUi9RlYyb7mdmf/pyGuftqXf2HpcNBVNLQluq2GPV/3AgfU0/PdtpUtCDKF3Z+Bluqg6kB98ELKyeHlYMoEZh7klZSs3BSRz5gw0/1sfGk8IVylmWpr6xty8GfbuRbulNcd+O8uJnckk1mxF+Ms3QbNmar3PP+fk7Y8Q/WNbQnZ/Qq7RSp+vR6iLTpAAqjvlzUMs7uefdzNt2q2A6hIbOhSGD7+OFbwOJMsu6667VIJ1tWJiYODAy6/j46PuPhYersbjCrpJCjzyUDZN4r9h8946/OHXkRxTydkDHh4qmz96VCVrE8dnMmXocd5Y0Ih3orwZNky1ce3a8p/+4JXjQMNArtHKy69bmTZdfegnTID333de78BAdYWcQcuncdpuekW2xMPXg4ceUsuLH+uOBic5dMTIeY/AEn2ua9eqZ5iZ0x00T93BeY9ATnrdRL7RTIMGajobwKThZ2n7w3zqZh5lU6On2VU7lLVfFQ3oXWsA/dOPwlc2g+HyA67e3vmsXas+sEaj+kIt+GBs3qz6XJs0uTRIgArGR4+qD9zXX6uzmP371bI771RnTVarCzcfEddVRYInXDl4gkqwCu71Uzp4Avx7owfQp3B8qzSDQXUpjhlzqcDkyaQlrcBcsm+89GBigQxLUT/ktOlF5ZcLnlB0ebFmMHLMN5i5l7Lohx5SYwjF/X6yAXio3/PyVDuffx7q11ef+YsWP+JrP1Bim5Mn1fhE48aw61RddrUqJ+2vJDIKf4NYy87WoXPnsmUGgzobadYM7r67qPziRRV8H3tMnXlNmQIhIerDlZ+vspOCb3JPTzVV86WXoG1bdS19gUaN1JQU8eeTlXV1dyur/HNU50rd/K2EPn2Kfj916vL7uewMh0oko/BuqiAAF2Svn3+uyopPaRw4UM3GKRjgKmf+N/Pnq1kFf/xR8kMZFaUC8fjxKvD6+sJrr1VNW8SNc/bsja5BkSs9JLKyFNz/pzJIBlpNXLr5Ugl9+5a/7q23Ft3c2WAoOU9v0iQ1tbBZM/X6X/9SfWXFP3B9+6rpMMePqyt+Hn1UlffqpeaKlr7at3ZtNcBbWocO0KmTulPWgAGwYsVVNVWIa7J4Mfzf/115vatRre5IL67OjBkQG6tGk0ubPl3NnS5Qo0ZR8OzRQ03PGjhQTRds06YoeIKarnXvvfD886d4/33Vkd+ihersj4tT6yxbdulqHdTVOwW3lO3fv2g/l2adXFZ0dPlXs9yILLlr1+t/TOG6ypwnKgH0T8piUdNnKmLkSHVFljMF3QrNm1/k5pvVgNjs2arMYFDdDX5+avpet26q/JZbiq6GmjZN9YG9+64aUJg6VU1PXLhQBeA1a9RTWCdPVn23n31W8vi33KL6ideuVfsClQ137araWhC4QQX/Tz4puX2vXgX3ESjy7LNFv8fFqX0XH7Rt0EDNpAA12DJvXtGyIUOu/rp7V/rs7rij4tuIyiWj8OK68/QsCma1ahXN9bz9dvUPyg8oJhNERJQsW75cZaLLlqkugQJt2qg51J06qX8F2rZVfbuBgSrgf/ABzJ2r+oBHjFDrFGTLBoMa+f3oIxWsCzLx6dNhyZIk+vf3LexXLn1Z4z33qCu2nnhCfQnce6+qk5cXHDigMvvGjWHXLhXk77tPXQgE6ktm/vzyb7TRtq0K9IcOqVkYo0er/u+gIPU3KLjz1xdfUPhUg4ULVZ3LG6lfvVod69tvyy67WsXn2r70Esyc6fq+rodLV9RWCukDFW7N99JUzOIzC6DoJjGllc4Ib7pJBa7iDwYs3t9rMpUNjgYDhIRklDsoB6rrwrfYFNHSWXvBoB4UPVEWiqaljR6tZlG88opaZrerL5T331dnAc2alXxUTcH+n3hCtaVWLdX+YcNUYK1XT31R9OungvPJkypwHjigBh1HjVI3a0lKUn+fESPUBUkdOmRw++2+NGqkvgQef1wdt1EjNQuk4G8+erSqV8Hf96GH1FV3ffoUZednzkB8vDrzOXsWFi1SZyczZ6r9hISobL/4DXCaNlVT+AqEhKirXAvUr6+y/LffLirr0IESt4ssLjBQ3Vum4E5slUKrAhMmTHBpu127dlVyTfRH2lg9VFUb9+7VtLy8Ktm1lpurafn5mnbuXPnLV67UtFOn1O85OZq2c2dRG3v21LRFi0quf/KkpmVlqd/371frnD2rXi9frmnZ2c7rkpNTtiwtTdOOHtW0+HhNu3BBlR05ovZb4MgRtd7p0+p1Sopa3rOnpv3jH0V1Lf1v2zZNO3y47DEr8j6WF9ckAxVCR5zdHasyFNzL09nNm4pn7GZzyUx84kSVLRdXv9gDDVq2VF0fBdtc6T6f5d1BsGbNEvcyAVQWWvwMoGnTonVBDUIGBqo+9YKb3Lzzjro09rPPVH+13a6y16ogAVQIcUVX83TbG3XL04L7ARRo21b9i4hQU+2OHKm6Y0sAFUJUW40bl73NaWWq9GlMMgovhPizkHmgQgjhokoPoCFV1VsrhBA6IxmoEEK4yGkAPXToEEOHDiW81P2l7HY7ERERREREYLfbq7yCQgihV04DaIsWLVhY/MExl8yePZu5c+cyb948oqKiqrRyQgihZxWexuRwOPC/dAudNPVYxEI2mw2bzUZ8fLxL2WlSUlK1z2qljdWDtLF6uNY2VjiA+vn54XA4MBgM+JR6bGpYWBhhYWFERka69Gyj6vBMpCuRNlYP0sbq4Vrb6DSAJicn8+qrr7J9+3befvttdu/eTUxMDOPHj2fspVvpvFT8TrxCCPEn4zSA1q5dm+jo6DLlwcHBLF26tEorJYQQ7kCmMQkhhIvkUk4hhHCRZKBCCOEiuZRTCCFcJBmoEEK4SAKoEEK4SAKoEEK4SEbhhRDCRZKBCiGEi2QUXgghXCQZqBBCuEgCqBBCuEgCqBBCuEhG4YUQwkWSgQohhItkFF4IIVwkGagQQrhIAqgQQrhIBpGEEMJFkoEKIYSLZBBJCCFcJBmoEEK4SAKoEEK4yOlz4dPT0xk9ejRWq5XQ0FAiIiIAmDJlCnv27CEgIIDJkyfTsGHD61ZZIYTQE6cZaGxsLOHh4Xz88cfExcUVlpvNZqxWKxaLBX9//+tRRyGE0CWnGWhCQgLt2rUDwGQyFZZPmjQJo9FIXFwcCxYsYNy4cYXLbDYbNpuN+Ph47HZ7hSuTlJTk0nbuRNpYPUgbq4drbaPTABoUFERCQgLt27cnPz+/sNxoVElrvXr1yhw4LCyMsLAwIiMjCQ4OrnBl7Ha7S9u5E2lj9SBtrB6utY1OA+gTTzzBmDFjWLduHb169WLgwIHExMQwffp0jh8/TlJSEnPmzHH5wEII4e6cBlBvb28WL15c+LpgEGnSpElVXyshhHADMo1JCCFcJNfCCyGEiyQDFUIIF8m18EII4SLJQIUQwkUSQIUQwkUSQIUQwkUyCi+EEC6SDFQIIVwko/BCCOEiyUCFEMJFEkCFEMJFEkCFEMJFMgovhBAukgxUCCFcJKPwQgjhIslAhRDCRRJAhRDCRTKIJIQQLpIMVAghXCSDSEII4SLJQIUQwkUSQIUQwkVOA2h6ejqDBg1i+PDhLF++vLDcbrcTERFBREQEdrv9ulRSCCH0yGkAjY2NJTw8nI8//pi4uLjC8tmzZzN37lzmzZtHVFTUdamkEELokdnZgoSEBNq1aweAyWQqLHc4HPj7+wOQlpZWYhubzYbNZuPnn38mMjKSU6dOAVC/fv2rqsyRI0do1qzZVa1bkX1X1bqurC9tvD71kDZe+/ru1saK1gMq9v/xyJEjZQs1J5YuXaqtXbtW0zRN69evX2H5sGHDtPPnz2sOh0MbMWKEs81dMmHChErdnx5JG6sHaWP1cK1tdJqBPvHEE4wZM4Z169bRq1cvBg4cSExMDOPHj2fs2LEAvPTSS1cd6a9GWFhYpe5Pj6SN1YO0sXq41jYaNE3TKqkuQgjxpyLTmIQQwkVOT+Gvp/T0dEaPHo3VaiU0NJSIiIgbXSWXHTp0iLfeeguHw8GqVatYsWIFGzduJDs7m/nz5wOUaWvpdby9vW9wKy5vzZo1rFu3jtTUVIYOHcquXbs4fPgwOTk5REdHc/LkSV588UVMJhNDhgzhoYce4r333iuxjsFguNHNuKw9e/Ywe/ZskpKS6Nq1K35+ftXufUxPT+fBBx9kypQp7Nu3r9q9h5s2beL111+nbdu2PPXUU/z222+V38ZK6Ym9RkuXLtXi4uI0TdO0vn373uDaVI4nn3xS0zRNCw8P1zRN09auXastXbq03LaWXsddnDt3Ths8eLA2YMAATdM0LSoqSvvhhx+0N998U9u5c6eWl5en9e/fX8vOzi6zjrvIy8vTIiIiquX7+Prrr2szZszQvvzyy2r5Hm7atEl75JFHtEGDBmn79u2rkjbq4hQ+ISGBxo0bAyWnTFUHBd9gTZs2JSEhody2ll7HXUybNo1hw4ZRt25doGwbjUb18UpOTi6zjjuIi4ujR48ePProo9XufVy/fj233nor9erVw+FwVMv38P777+ebb75hxowZjBo1qkraqIsAGhQUVFjZ/Pz8G1ybqnHs2DGCgoIu29aCdfRO0zRefvllunfvTkhICElJSUDZNha0r3bt2mXWcQe9e/fmm2++KXElXnV5Hzdt2sTWrVtZsWIFK1as4MyZM0D1eg8LAmNAQAB+fn5V8jnVxSh8eno6Y8aMoUaNGnTu3Nmt+0CTk5N59dVXWb9+PcOGDaNp06b8+OOPZGZmMnfuXIAybV2xYkWJdfTedzZnzhw++eQTQkJCaN++PRkZGRw9erSw7+/kyZNMnDgRs9nM008/TZcuXZg1a1aJddyh/yw2Npbs7Gxuu+02AgICqt37CLBkyRLq1KnD/v37q917GBsbi81m4/z584waNYrff/+90tuoiwAqhBDuSBen8EII4Y4kgAohhIskgAohhIskgAohhIskgAohhIv+Hzeg+xpfmrmOAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training!\n",
    "model = TransformerLM(config)\n",
    "# model = torch.compile(model)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "train(model, optimizer, seq_len, batch_size, total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 72, 78,  1, 80, 66, 69, 69,  1, 71, 62, 79, 62, 75,  1, 59]])\n",
      "You will never be able to see the fire.\n",
      "\n",
      "Are you all right now?\n",
      "\n",
      "She wanted to take the way to the start of the lords of the same.\n",
      "\n",
      "I'm sorry. I wouldn't see her from the way to the propose of the last strand.\n",
      "\n",
      "I was so easily doing.\n",
      "\n",
      "I knew it, but I couldn't be happy.\n",
      "\n",
      "But I can't do it.\n",
      "\n",
      "I'm sorry. I want to see.\n",
      "\n",
      "What are you to do with that?\n",
      "\n",
      "I was so so sure that man.\n",
      "\n",
      "I was so far broken to come here, and that to me.\n",
      "\n",
      "What do you mean?\n",
      "\n",
      "I can't tell you that you were to work to work.\n",
      "\n",
      "You still hard to so any program.\n",
      "\n",
      "I wasn't that the true, though.\n",
      "\n",
      "I know where I can see he like this?\n",
      "\n",
      "I see...\n",
      "\n",
      "I love it was a thing in the same room into a handing and not be so too much.\n",
      "\n",
      "The problem to be fine.\n",
      "\n",
      "I can take that all the way to the director of the souls.\n",
      "\n",
      "I'm sorry. I'm sorry.\n",
      "\n",
      "I won't say that someone like you can go down the way.\n",
      "\n",
      "I can't be so sorry.\n",
      "\n",
      "I want to go and the book the planet with right their society.\n",
      "\n",
      "What are you to do with your best food?\n",
      "\n",
      "I'm a couple with you like this wo\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "idx = encode(\"You will never b\")\n",
    "print(torch.tensor([idx]))\n",
    "print(decode(model.generate(idx=torch.tensor([idx], dtype=torch.long).to(device), max_new_tokens=1000, temperature=0.5, use_cache=True)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
